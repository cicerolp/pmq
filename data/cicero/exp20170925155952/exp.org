# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Removal benchmark
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 tags:nil author:nil
#+PROPERTY: header-args :cache no :eval no-export 


* Description 
Benchmark of the remove operation ;

- PMQ / GEOHASH
- BTREE -
- RTREE -  Quadratic algorithm 

Parameters:
- T = 17616 
- B = 1000 
- elements = 17616000
- variate tsize for Btree and Rtree only

** DEFERRED Standalone script 
:LOGBOOK:
- State "DEFERRED"   from "TODO"       [2017-09-14 Qui 10:07]
:END:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  
  
* DONE Design of Experiment                                          :export:

PMA cannot be parameterized by the remove frequency, this is set "automatically" by the size of the time window T. 

For same time window size, we want to find the best frequency to trigger removals on the Btree and Rtree.
 
The previous experiment showed that PMA is "optimal" at T 17616, case where it triggers the removal when the T has overflowed by 33.33 %.

For Btree and Rtree, test the overflow limit (tSize parameter) ranging from:
#+begin_src python :results output :exports both :session execParam
maxVal = 0.5
overFlow = [ (maxVal / 2**i) for i in range(0,9)]
print(overFlow)

T = 17616000
tSize = [round(T + T * e) for e in overFlow]
print(tSize)
#+end_src

#+RESULTS:
: 
: >>> [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125]
: >>> >>> >>> [26424000, 22020000, 19818000, 18717000, 18166500, 17891250, 17753625, 17684812, 17650406]


* TODO Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20170925155952

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: M	benchmarks/bench_insert_remove_scan.cpp
: [master dad1c47] LBK: new entry for exp20170925155952
:  1 file changed, 49 insertions(+), 15 deletions(-)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:
: M	benchmarks/bench_insert_remove_scan.cpp

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20170925155952
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org
	parse.sh
	plotResults.R
	run.sh

nothing added to commit but untracked files present (use "git add" to track)
[exp20170925155952 5cf867d] Initial commit for exp20170925155952
 1 file changed, 1043 insertions(+)
 create mode 100644 data/cicero/exp20170925155952/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 5cf867d (HEAD -> exp20170925155952) Initial commit for exp20170925155952
: * dad1c47 (master) LBK: new entry for exp20170925155952
: * 2e6e2ce (origin/master) upd: config file for benchmarks

** DONE Export run script 

#+begin_src python :results output :exports both :session execParam
rate=1000
T=17616
n=T*rate*2
for ts in tSize:
    print("stdbuf -oL ./benchmarks/bench_insert_remove_count -rate ",rate," -n ",n," -T ",T," -tSize ",ts," >> ${TMPDIR}/bench_ins_rm_",T,"_${EXECID}.log",sep="")
#+end_src

#+RESULTS:
#+begin_example

>>> >>> ... ... 
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 26424000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 22020000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 19818000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 18717000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 18166500 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17891250 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17753625 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17684812 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17650406 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
#+end_example

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=ON -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=ON -DBENCH_BTREE=ON -DBENCH_RTREE=ON -DBENCH_DENSE=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 
rm ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
touch ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log

# Queries insert remove count

# PMQ
cmake -DBENCH_PMQ=ON -DBENCH_BTREE=OFF -DBENCH_RTREE=OFF -DBENCH_DENSE=OFF . ; make
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 26424000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log

# BTREE and RTREE
cmake -DBENCH_PMQ=OFF -DBENCH_BTREE=ON -DBENCH_RTREE=ON -DBENCH_DENSE=OFF . ; make

stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 26424000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 22020000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 19818000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 18717000 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 18166500 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17891250 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17753625 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17684812 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log
stdbuf -oL ./benchmarks/bench_insert_remove_count -rate 1000 -n 35232000 -T 17616 -tSize 17650406 >> ${TMPDIR}/bench_ins_rm_17616_${EXECID}.log

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 

** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20170925155952
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	parse.sh
	plotResults.R
	run.sh

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20170925155952 bd70644] UPD: run.sh script
:  2 files changed, 102 insertions(+), 13 deletions(-)
:  create mode 100755 data/cicero/exp20170925155952/run.sh

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** Local Execution                                                   :local:ARCHIVE:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** INPROGRESS Remote Execution                                      :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

53 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Mon Sep 25 18:16:45 2017 from 143.54.13.218
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 38, done.
(1/35)           remote: Compressing objects:   5% (2/35)           remote: Compressing objects:   8% (3/35)           remote: Compressing objects:  11% (4/35)           remote: Compressing objects:  14% (5/35)           remote: Compressing objects:  17% (6/35)           remote: Compressing objects:  20% (7/35)           remote: Compressing objects:  22% (8/35)           remote: Compressing objects:  25% (9/35)           remote: Compressing objects:  28% (10/35)           remote: Compressing objects:  31% (11/35)           remote: Compressing objects:  34% (12/35)           remote: Compressing objects:  37% (13/35)           remote: Compressing objects:  40% (14/35)           remote: Compressing objects:  42% (15/35)           remote: Compressing objects:  45% (16/35)           remote: Compressing objects:  48% (17/35)           remote: Compressing objects:  51% (18/35)           remote: Compressing objects:  54% (19/35)           remote: Compressing objects:  57% (20/35)           remote: Compressing objects:  60% (21/35)           remote: Compressing objects:  62% (22/35)           remote: Compressing objects:  65% (23/35)           remote: Compressing objects:  68% (24/35)           remote: Compressing objects:  71% (25/35)           remote: Compressing objects:  74% (26/35)           remote: Compressing objects:  77% (27/35)           remote: Compressing objects:  80% (28/35)           remote: Compressing objects:  82% (29/35)           remote: Compressing objects:  85% (30/35)           remote: Compressing objects:  88% (31/35)           remote: Compressing objects:  91% (32/35)           remote: Compressing objects:  94% (33/35)           remote: Compressing objects:  97% (34/35)           remote: Compressing objects: 100% (35/35)           remote: Compressing objects: 100% (35/35), done.
(1/38)   Unpacking objects:   5% (2/38)   Unpacking objects:   7% (3/38)   Unpacking objects:  10% (4/38)   Unpacking objects:  13% (5/38)   Unpacking objects:  15% (6/38)   Unpacking objects:  18% (7/38)   Unpacking objects:  21% (8/38)   Unpacking objects:  23% (9/38)   Unpacking objects:  26% (10/38)   Unpacking objects:  28% (11/38)   Unpacking objects:  31% (12/38)   remote: Total 38 (delta 23), reused 0 (delta 0)
(13/38)   Unpacking objects:  36% (14/38)   Unpacking objects:  39% (15/38)   Unpacking objects:  42% (16/38)   Unpacking objects:  44% (17/38)   Unpacking objects:  47% (18/38)   Unpacking objects:  50% (19/38)   Unpacking objects:  52% (20/38)   Unpacking objects:  55% (21/38)   Unpacking objects:  57% (22/38)   Unpacking objects:  60% (23/38)   Unpacking objects:  63% (24/38)   Unpacking objects:  65% (25/38)   Unpacking objects:  68% (26/38)   Unpacking objects:  71% (27/38)   Unpacking objects:  73% (28/38)   Unpacking objects:  76% (29/38)   Unpacking objects:  78% (30/38)   Unpacking objects:  81% (31/38)   Unpacking objects:  84% (32/38)   Unpacking objects:  86% (33/38)   Unpacking objects:  89% (34/38)   Unpacking objects:  92% (35/38)   Unpacking objects:  94% (36/38)   Unpacking objects:  97% (37/38)   Unpacking objects: 100% (38/38)   Unpacking objects: 100% (38/38), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20170925155952
Branch exp20170925155952 set up to track remote branch exp20170925155952 from origin.
Switched to a new branch 'exp20170925155952'
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Already up-to-date.
commit bd706445fe6e15a98a2f8f516e2571db868f5461
Date:   Mon Sep 25 18:16:59 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** Execute Remotely                                               :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20170925155952$ julio@cicero:~/Projects/pmq/data/cicero/exp20170925155952$ julio@cicero:~/Projects/pmq/data/cicero/exp20170925155952$ julio@cicero:~/Projects/pmq/data/cicero/exp20170925155952$ 1506374304

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
#+begin_example
runExp: 1 windows (created Mon Sep 25 18:18:24 2017) [80x23]
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
julio    19285  0.0  0.0  45248  4484 ?        Ss   18:16   0:00 /lib/systemd/sy
julio    19287  0.0  0.0 145364  2112 ?        S    18:16   0:00 (sd-pam)
julio    19434  0.0  0.0  97464  3316 ?        S    18:17   0:00 sshd: julio@pts
julio    19435  0.0  0.0  22764  5316 pts/9    Ss   18:17   0:00 -bash
julio    19488  0.0  0.0  29420  2860 ?        Ss   18:18   0:00 tmux new -d -s 
julio    19489  0.0  0.0  12532  2956 pts/8    Ss+  18:18   0:00 bash -c cd ~/Pr
julio    19491  0.0  0.0  12540  3092 pts/8    S+   18:18   0:00 /bin/bash ./run
julio    19865  0.0  0.0   9676  2276 pts/8    S+   18:18   0:00 make
julio    19868  0.0  0.0   9676  2372 pts/8    S+   18:18   0:00 make -f CMakeFi
julio    19967  0.2  0.0  11816  4404 pts/8    S+   18:19   0:00 make -f benchma
julio    19970  0.0  0.0   4508   708 pts/8    S+   18:19   0:00 /bin/sh -c cd /
julio    19971  0.0  0.0   8352   752 pts/8    S+   18:19   0:00 /usr/bin/c++ -I
julio    19972  100  2.3 864760 761880 pts/8   R+   18:19   0:07 /usr/lib/gcc/x8
julio    19980  0.0  0.0  37368  3268 pts/9    R+   18:19   0:00 ps ux
#+end_example

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
git commit -a -m "wip"
git status
git pull --rebase origin $expId
#+end_src

#+RESULTS:
#+begin_example
On branch exp20170914091842
Untracked files:
	../../../.#LabBook.org
	../../../LabBook.org.bkp
	../../../LabBook.org.orig
	../../../benchmarks/bench_insert_remove_count.cpp.orig
	../exp20170830124159/
	../exp20170904152622/
	../exp20170904153555/
	$HA
	.#exp.org
	exp.html
	exp.pdf
	exp.rst
	exp.tex
	../../../include/types.h.orig

nothing added to commit but untracked files present
On branch exp20170914091842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../.#LabBook.org
	../../../LabBook.org.bkp
	../../../LabBook.org.orig
	../../../benchmarks/bench_insert_remove_count.cpp.orig
	../exp20170830124159/
	../exp20170904152622/
	../exp20170904153555/
	$HA
	.#exp.org
	exp.html
	exp.pdf
	exp.rst
	exp.tex
	../../../include/types.h.orig

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Fast-forwarded exp20170914091842 to 1adced939ed1e68bf901e82bd40097309abecf9e.
#+end_example


* TODO Analysis
** Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tgzFiles
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tgzFiles
| log_1505411932.tgz |
| log_1505412384.tgz |

:NOTE: the execution from log_1505411932.tgz was executed on inf-desktop by mistake. But results might be ok.

Take the last archive from the list above:
#+begin_src sh :results output :exports both :var f=tgzFiles[-1]
echo $f
#+end_src

#+RESULTS:
: log_1505412384.tgz

#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tgzFiles[-1]
tar xvzf $f
#+end_src

#+RESULTS: logFile
#+begin_example
bench_ins_rm_11745_1505412384.log
bench_ins_rm_17616_1505412384.log
bench_ins_rm_20552_1505412384.log
bench_ins_rm_22020_1505412384.log
bench_ins_rm_22754_1505412384.log
bench_ins_rm_23121_1505412384.log
bench_ins_rm_23305_1505412384.log
bench_ins_rm_23396_1505412384.log
bench_ins_rm_23442_1505412384.log
bench_ins_rm_23465_1505412384.log
#+end_example

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile

f=$(echo $logFileList | cut -d" " -f1)

output=$( basename -s .log $f | sed "s/_[[:digit:]]\{5\}_/_/g").csv
echo $output
rm $output
touch $output

for logFile in $logFileList ; 
do
grep "GeoHashBinary\|BTree\|RTree ;" $logFile | sed "s/InsertionRemoveBench//g" >>  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
: bench_ins_rm_1505412384.csv

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

*** Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile
library(tidyverse)

df <- f[[1]] %>%
    read_delim(delim=";",trim_ws = TRUE, col_names = paste("V",c(1:9),sep="") , progress=FALSE)

str(df)
#+end_src

#+RESULTS:
#+begin_example
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_integer(),
  V3 = col_integer(),
  V4 = col_character(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_character()
)
Warning: 775032 parsing failures.
row # A tibble: 5 x 5 col     row   col  expected    actual                          file expected   <int> <chr>     <chr>     <chr>                         <chr> actual 1     1  <NA> 9 columns 8 columns 'bench_ins_rm_1505412384.csv' file 2     2  <NA> 9 columns 8 columns 'bench_ins_rm_1505412384.csv' row 3     3  <NA> 9 columns 8 columns 'bench_ins_rm_1505412384.csv' col 4     4  <NA> 9 columns 8 columns 'bench_ins_rm_1505412384.csv' expected 5     5  <NA> 9 columns 8 columns 'bench_ins_rm_1505412384.csv'
... ................. ... ............................................................... ........ ............................................................... ...... ............................................................... .... ............................................................... ... ............................................................... ... ............................................................... ........ ............... [... truncated]
Warning message:
In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	775032 obs. of  9 variables:
 $ V1: chr  "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" ...
 $ V2: int  11745 11745 11745 11745 11745 11745 11745 11745 11745 11745 ...
 $ V3: int  11745 11746 11747 11748 11749 11750 11751 11752 11753 11754 ...
 $ V4: chr  "count" "count" "count" "count" ...
 $ V5: int  11746000 11747000 11748000 11749000 11750000 11751000 11752000 11753000 11754000 11755000 ...
 $ V6: chr  "insert" "insert" "insert" "insert" ...
 $ V7: num  1.06 1.06 1.05 1.06 1.05 ...
 $ V8: chr  NA NA NA NA ...
 $ V9: chr  NA NA NA NA ...
 - attr(*, "problems")=Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	775032 obs. of  5 variables:
  ..$ row     : int  1 2 3 4 5 6 7 8 9 10 ...
  ..$ col     : chr  NA NA NA NA ...
  ..$ expected: chr  "9 columns" "9 columns" "9 columns" "9 columns" ...
  ..$ actual  : chr  "8 columns" "8 columns" "8 columns" "8 columns" ...
  ..$ file    : chr  "'bench_ins_rm_1505412384.csv'" "'bench_ins_rm_1505412384.csv'" "'bench_ins_rm_1505412384.csv'" "'bench_ins_rm_1505412384.csv'" ...
 - attr(*, "spec")=List of 2
  ..$ cols   :List of 9
  .. ..$ V1: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V2: list()
  .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
  .. ..$ V3: list()
  .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
  .. ..$ V4: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V5: list()
  .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
  .. ..$ V6: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V7: list()
  .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
  .. ..$ V8: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V9: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  ..$ default: list()
  .. ..- attr(*, "class")= chr  "collector_guess" "collector"
  ..- attr(*, "class")= chr "col_spec"
#+end_example

Remove useless columns
#+begin_src R :results output :exports both :session 

names(df) <- c("algo", "T", "id", "V4", "count", "V5", "insert" , "V8" , "remove")

df <- select(df, -V4, -V5, -V8)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 775,032 x 6
            algo     T    id    count  insert remove
           <chr> <int> <int>    <int>   <dbl>  <chr>
 1 GeoHashBinary 11745 11745 11746000 1.06247   <NA>
 2 GeoHashBinary 11745 11746 11747000 1.05632   <NA>
 3 GeoHashBinary 11745 11747 11748000 1.05376   <NA>
 4 GeoHashBinary 11745 11748 11749000 1.06071   <NA>
 5 GeoHashBinary 11745 11749 11750000 1.05004   <NA>
 6 GeoHashBinary 11745 11750 11751000 1.04954   <NA>
 7 GeoHashBinary 11745 11751 11752000 1.12759   <NA>
 8 GeoHashBinary 11745 11752 11753000 1.06108   <NA>
 9 GeoHashBinary 11745 11753 11754000 1.05192   <NA>
10 GeoHashBinary 11745 11754 11755000 1.04592   <NA>
# ... with 775,022 more rows
#+end_example

*** Summary Tables of Remove Times                                 :export:

#+begin_src R :results table :exports both :session :colnames yes
df %>% filter(!is.na(remove)) %>%
    mutate(remove = as.numeric(remove)) %>%
    group_by(algo,T) %>%
    summarize(RemoveTime = signif(mean(remove)), stdv = signif(sd(remove))) %>%
    arrange(T,algo)
#+end_src

#+RESULTS:
| algo          |     T | RemoveTime |    stdv |
|---------------+-------+------------+---------|
| BTree         | 11745 |    2938.56 | 31.9188 |
| GeoHashBinary | 11745 |    719.014 | 134.508 |
| RTree         | 11745 |      10268 | 345.705 |
| BTree         | 17616 |    1897.55 | 15.5949 |
| GeoHashBinary | 17616 |    633.379 | 12.9222 |
| RTree         | 17616 |    6008.85 | 230.542 |
| BTree         | 20552 |    1316.32 | 21.9188 |
| GeoHashBinary | 20552 |    617.114 | 10.5823 |
| RTree         | 20552 |    3569.99 | 85.4322 |
| BTree         | 22020 |    970.417 | 17.2736 |
| GeoHashBinary | 22020 |    616.018 | 7.95398 |
| RTree         | 22020 |    2223.55 |  69.507 |
| BTree         | 22754 |    760.748 | 10.4232 |
| GeoHashBinary | 22754 |    604.105 | 5.68478 |
| RTree         | 22754 |    1393.05 | 41.2335 |
| BTree         | 23121 |    649.198 | 8.72301 |
| GeoHashBinary | 23121 |    556.607 | 4.00548 |
| RTree         | 23121 |    960.784 | 28.5292 |
| BTree         | 23305 |    588.715 | 8.76007 |
| GeoHashBinary | 23305 |    558.013 | 2.78677 |
| RTree         | 23305 |    690.343 |   18.71 |
| BTree         | 23396 |    563.927 | 14.1947 |
| GeoHashBinary | 23396 |    562.311 | 8.97959 |
| RTree         | 23396 |    568.807 | 25.1274 |
| BTree         | 23442 |    535.037 | 8.12749 |
| GeoHashBinary | 23442 |    560.956 | 13.4803 |
| RTree         | 23442 |    491.738 | 10.9601 |
| BTree         | 23465 |    525.734 | 8.05529 |
| GeoHashBinary | 23465 |    564.266 | 10.6004 |
| RTree         | 23465 |     441.24 | 22.8475 |

*** Overview of results                                       :export:plot:

Plot an overview of every benchmark , doing average of times. 
#+begin_src R :results output :exports code
df %>% filter(!is.na(remove)) %>% 
    mutate(remove=as.numeric(remove)) %>%
    mutate(remove=ifelse(algo != "GeoHashBinary", remove + insert, remove)) %>% # Remove actually accounts for remove + a small insertion 
    group_by(algo,T) %>%
    summarize(RemoveTime = mean(remove), RemoveSum = sum(remove), stdv = sd(remove)) %>%
    mutate(T = as.factor(T))-> dfplot

dfplot
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 30 x 5
# Groups:   algo [3]
    algo      T RemoveTime  RemoveSum      stdv
   <chr> <fctr>      <dbl>      <dbl>     <dbl>
 1 BTree  11745  2939.0430   5878.086 31.976994
 2 BTree  17616  1898.0251   7592.100 15.561384
 3 BTree  20552  1316.7902  10534.321 21.896304
 4 BTree  22020   970.8734  15533.975 17.255611
 5 BTree  22754   761.1887  24358.037 10.410706
 6 BTree  23121   649.6426  41577.128  8.713129
 7 BTree  23305   589.1553  75411.882  8.752589
 8 BTree  23396   564.3692 142785.420 14.193076
 9 BTree  23442   535.4773 267738.659  8.123516
10 BTree  23465   526.1890 515139.003  8.053196
# ... with 20 more rows
#+end_example

#+begin_src R :results output graphics :file "./img/overview.png" :exports both :width 600 :height 400
library(ggplot2)

dfplot %>%
#    filter(algo == "GeoHashBinary") %>%
    ggplot( aes(x=T,y=RemoveTime, fill=factor(algo))) + 
    geom_bar(stat="identity", position="dodge")+
    geom_errorbar( position=position_dodge(0.9), 
                   aes(ymin = RemoveTime - stdv, ymax = RemoveTime + stdv), width=0.5)+
    labs(title = "Average time of removal operations") 
#+end_src

#+RESULTS:
[[file:./img/overview.png]]

The average remove time decreases logarithmicly for BTree and Rtree. 
However for the PMQ the time seems much more stable no matter the amount of removals. 

*** DONE Insertion performance

#+begin_src R :results output :exports code :session 
df %>% filter(is.na(remove)) %>%  # get only lines with no removes
       mutate(remove=as.numeric(remove)) %>%
       mutate(T = as.factor(T))-> dfinsert

dfinsert
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 769,074 x 6
            algo      T    id    count  insert remove
           <chr> <fctr> <int>    <int>   <dbl>  <dbl>
 1 GeoHashBinary  11745 11745 11746000 1.06247     NA
 2 GeoHashBinary  11745 11746 11747000 1.05632     NA
 3 GeoHashBinary  11745 11747 11748000 1.05376     NA
 4 GeoHashBinary  11745 11748 11749000 1.06071     NA
 5 GeoHashBinary  11745 11749 11750000 1.05004     NA
 6 GeoHashBinary  11745 11750 11751000 1.04954     NA
 7 GeoHashBinary  11745 11751 11752000 1.12759     NA
 8 GeoHashBinary  11745 11752 11753000 1.06108     NA
 9 GeoHashBinary  11745 11753 11754000 1.05192     NA
10 GeoHashBinary  11745 11754 11755000 1.04592     NA
# ... with 769,064 more rows
#+end_example

**** Overall                                                 :export:plot:

#+begin_src R :results output graphics :file "./img/overallInsertion.png" :exports both :width 800 :height 600
dfinsert %>%
ggplot(aes(x=id,y=insert, color=factor(algo))) + 
geom_line() +
labs(title = "Insertions") + 
facet_wrap(~T, scales="free")
#+end_src

#+RESULTS:
[[file:./img/overallInsertion.png]]

***** Total insertion time (without the removals) :
#+begin_src R :results table :session :exports both :colnames yes
dfinsert %>% 
    group_by(algo, T) %>%
    summarize(Average = signif(mean(insert)), Stdv = signif(sd(insert)), Total = signif(sum(insert))) %>%
arrange(T,algo)

#+end_src

#+RESULTS:
| algo          |     T |  Average |       Stdv |   Total |
|---------------+-------+----------+------------+---------|
| BTree         | 11745 | 0.448848 |  0.0293204 | 15812.5 |
| GeoHashBinary | 11745 |  1.09319 |  0.0628743 | 38512.1 |
| RTree         | 11745 |  1.01856 |  0.0746711 | 35882.8 |
| BTree         | 17616 | 0.451324 |  0.0268541 | 13249.1 |
| GeoHashBinary | 17616 |  1.08116 |  0.0407717 | 31738.5 |
| RTree         | 17616 |  1.01504 |  0.0658717 | 29797.5 |
| BTree         | 20552 | 0.448744 |  0.0240568 |   11854 |
| GeoHashBinary | 20552 |  1.07296 |  0.0117558 | 28343.3 |
| RTree         | 20552 |  1.00105 |  0.0582154 | 26443.7 |
| BTree         | 22020 | 0.447691 |  0.0193614 | 11165.4 |
| GeoHashBinary | 22020 |  1.07911 | 0.00986746 | 26913.1 |
| RTree         | 22020 |  1.00231 |  0.0462575 | 24997.5 |
| BTree         | 22754 | 0.440994 |  0.0164774 | 10667.7 |
| GeoHashBinary | 22754 |  1.07052 |  0.0297494 | 25895.8 |
| RTree         | 22754 |  1.00679 |  0.0544763 | 24354.3 |
| BTree         | 23121 | 0.445606 |  0.0155989 | 10601.4 |
| GeoHashBinary | 23121 |  1.06871 | 0.00692898 | 25425.7 |
| RTree         | 23121 |  1.02198 |  0.0449252 | 24313.9 |
| BTree         | 23305 | 0.443706 |  0.0145924 | 10446.2 |
| GeoHashBinary | 23305 |  1.06769 | 0.00791834 | 25136.6 |
| RTree         | 23305 |  1.02155 |  0.0360618 | 24050.3 |
| BTree         | 23396 | 0.452135 |  0.0186625 |   10547 |
| GeoHashBinary | 23396 |  1.08239 |  0.0299922 |   25249 |
| RTree         | 23396 |  1.06218 |  0.0817903 | 24777.4 |
| BTree         | 23442 | 0.457403 |  0.0168122 | 10535.8 |
| GeoHashBinary | 23442 |  1.07103 |  0.0219033 |   24670 |
| RTree         | 23442 |  1.03593 |  0.0410284 | 23861.6 |
| BTree         | 23465 | 0.472946 |  0.0166012 | 10656.4 |
| GeoHashBinary | 23465 |  1.07213 |  0.0241641 | 24157.3 |
| RTree         | 23465 |  1.05569 |  0.0745693 | 23786.8 |

#+begin_src R :results output graphics :file "./img/averageInsOnly.png" :exports both :width 600 :height 400
library(ggplot2)

dfinsert %>% 
    group_by(algo, T) %>%
    summarize(avg = mean(insert), stdv = sd(insert)) %>%
    ggplot( aes(x=T,y=avg, fill=factor(algo))) + 
    geom_bar(stat="identity", position="dodge")+
    geom_errorbar( position=position_dodge(0.9), 
                   aes(ymin = avg - stdv, ymax = avg + stdv), width=0.5) +
    #facet_wrap(~T, scale="free_x")+ 
    labs(title = "Average Insertions (without removals)") 
#+end_src

#+RESULTS:
[[file:./img/averageInsOnly.png]]


In average the insertions are 2X faster with standard Btrees. 
PMQ and Rtree are not statistically different in general (except maybe on T=20552). 

This means that the insertion time doesn't change with T.
No matter the parameter T choosed, the insertions take the same time.

***** Total benchmark time with the removals:
#+begin_src R :results table :session :exports both :colnames yes
options(digits=6)
df %>% 
    mutate(remove = if_else(is.na(remove), 0 , as.numeric(remove))) %>%
    mutate(ins_rm=if_else(algo == "GeoHashBinary", insert, as.numeric(remove) + insert)) %>% 
    group_by(algo,T) %>%
    summarize(AvgTime = signif(mean(ins_rm)), stdv = signif(sd(ins_rm)), total = signif(sum(ins_rm))) %>%
    mutate(T = as.factor(T))-> dfTotals

dfTotals %>% arrange(T,algo)
#+end_src

#+RESULTS:
| algo          |     T |  AvgTime |    stdv |   total |
|---------------+-------+----------+---------+---------|
| BTree         | 11745 | 0.615667 | 22.1411 | 21690.6 |
| GeoHashBinary | 11745 |  1.13395 |  5.4567 | 39950.2 |
| RTree         | 11745 |  1.60146 | 77.3851 | 56420.9 |
| BTree         | 17616 | 0.709849 | 22.1483 | 20841.2 |
| GeoHashBinary | 17616 |   1.1673 | 7.38119 |   34272 |
| RTree         | 17616 |  1.83369 | 70.1725 | 53837.3 |
| BTree         | 20552 | 0.847273 | 22.9039 | 22388.3 |
| GeoHashBinary | 20552 |  1.25947 |  10.719 | 33280.2 |
| RTree         | 20552 |   2.0819 | 62.1257 | 55012.1 |
| BTree         | 22020 |  1.06986 | 24.5679 | 26699.4 |
| GeoHashBinary | 22020 |  1.47337 | 15.5671 | 36769.4 |
| RTree         | 22020 |  2.42791 | 56.3111 | 60590.8 |
| BTree         | 22754 |  1.44603 | 27.6358 | 35025.7 |
| GeoHashBinary | 22754 |  1.86719 | 21.9055 | 45227.1 |
| RTree         | 22754 |   2.8472 | 50.6234 |   68965 |
| BTree         | 23121 |  2.18732 | 33.5847 | 52178.5 |
| GeoHashBinary | 23121 |  2.55915 | 28.7376 | 61048.6 |
| RTree         | 23121 |  3.59963 | 49.7209 | 85869.3 |
| BTree         | 23305 |  3.62714 | 43.1796 | 85858.1 |
| GeoHashBinary | 23305 |  4.07935 | 40.8457 | 96562.3 |
| RTree         | 23305 |  4.75458 | 50.6472 |  112546 |
| BTree         | 23396 |  6.50265 | 58.1178 |  153332 |
| GeoHashBinary | 23396 |  7.10406 | 57.8297 |  167514 |
| RTree         | 23396 |  7.16493 | 58.6585 |  168949 |
| BTree         | 23442 |  11.8244 | 77.1621 |  278274 |
| GeoHashBinary | 23442 |  12.9663 | 80.7626 |  305148 |
| RTree         | 23442 |  11.4832 | 70.9287 |  270246 |
| BTree         | 23465 |  22.3638 | 105.035 |  525795 |
| GeoHashBinary | 23465 |  24.5236 |  112.53 |  576573 |
| RTree         | 23465 |  19.4282 | 88.2658 |  456776 |

#+begin_src R :results output :exports code :session 
df %>% 
    mutate(remove = if_else(is.na(remove), 0 , as.numeric(remove))) %>%
    mutate(ins_rm=if_else(algo == "GeoHashBinary", insert, as.numeric(remove) + insert)) %>% 
    group_by(algo,T) %>%
    summarize(total = sum(ins_rm) , avg = mean(ins_rm), std= sd(ins_rm)) %>%
    mutate(T = as.factor(T)) -> totalPlot
totalPlot
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 30 x 5
# Groups:   algo [3]
    algo      T     total        avg       std
   <chr> <fctr>     <dbl>      <dbl>     <dbl>
 1 BTree  11745  21690.56  0.6156669  22.14110
 2 BTree  17616  20841.16  0.7098489  22.14827
 3 BTree  20552  22388.34  0.8472730  22.90391
 4 BTree  22020  26699.39  1.0698584  24.56794
 5 BTree  22754  35025.69  1.4460280  27.63580
 6 BTree  23121  52178.54  2.1873207  33.58468
 7 BTree  23305  85858.06  3.6271412  43.17964
 8 BTree  23396 153332.38  6.5026455  58.11776
 9 BTree  23442 278274.48 11.8243597  77.16212
10 BTree  23465 525795.42 22.3638052 105.03491
# ... with 20 more rows
#+end_example

#+begin_src R :results output graphics :file "./img/totalInsRm.png" :exports both :width 600 :height 400
library(ggplot2)

totalPlot %>%
    ggplot( aes(x=T,y=total, fill=factor(algo))) + 
    geom_bar(stat="identity", position="dodge")+
    labs(title = "Total sum of Insertions and Removals") 
#+end_src

#+RESULTS:
[[file:./img/totalInsRm.png]]

The total insertion time increased with parameter T. 
Because with a lager T (closer to the limit 23488) as show in [[tbl:ExpVariables]], the frequency of expensive remotions increases. 
The best value of T is lower than 22754 for every algorithm. 

***** Average benchmark time with the removals:

Bimodal behaviour, it doesn't make sense to do an average of removals together with insertions. 

#+begin_src R :results output graphics :file "./img/totalAvgRm.png" :exports both :width 600 :height 400
library(ggplot2)

totalPlot %>%
    ggplot( aes(x=T,y=avg, fill=factor(algo))) + 
    geom_bar(stat="identity", position="dodge")+
    geom_errorbar( position=position_dodge(0.9), 
                   aes(ymin = avg - std, ymax = avg + std), width=0.5) +
    labs(title = "Average Insertions and Removals") 
#+end_src

#+RESULTS:
[[file:./img/totalAvgRm.png]]


*** DONE Conclusion                                                :export:

We need to find a tradeoff between these two plots: 

[[file:img/totalInsRm.png]] [[file:img/overview.png]]

Best T value for optimal Remove Time:
#+begin_src R :results table :exports results :session :colnames yes 
dfplot %>% 
group_by(algo) %>% 
top_n(-1,RemoveTime)
#+end_src

#+RESULTS:
| algo          |     T | RemoveTime avg (ms) |      stdv |
|---------------+-------+---------------------+-----------|
| BTree         | 23465 |          526.188970 |  8.053197 |
| GeoHashBinary | 23121 |          556.606700 |  4.005477 |
| RTree         | 23465 |          442.277040 | 22.851265 |
#+TBLFM: @2$3..@4$4=$0;%03f

Best T value for optimal total execution time:
#+begin_src R :results table :exports results :session :colnames yes 
totalPlot %>%
group_by(algo) %>% 
top_n(-1,total)
#+end_src

#+RESULTS:
| algo          |     T |     sum (ms) | avg (ms) |       std |
|---------------+-------+--------------+----------+-----------|
| BTree         | 17616 | 20841.163000 | 0.709849 | 22.148266 |
| GeoHashBinary | 20552 | 33280.174000 | 1.259468 | 10.718993 |
| RTree         | 17616 | 53837.282000 | 1.833695 | 70.172511 |
#+TBLFM: @2$3..@4$5=$0;%03f

Compute a tradeoff between total running time and time spent on removals. 
#+begin_src R :results output graphics :file "./img/removalTradeoff.png" :exports both :width 600 :height 400 :session 
library(ggplot2)
require(grid)

inner_join(dfplot,totalPlot) %>% 
#mutate ( ratio = (sqrt(RemoveTime * total))) %>%
#mutate ( ratio = sqrt(RemoveSum * total)) %>%
mutate ( ratio = (sqrt(RemoveTime * avg))) %>%
    ggplot( aes(x=T,y=ratio, fill=factor(algo))) + 
    geom_bar(stat="identity", position="dodge") + 
    annotate(geom = "text",x = unique(dfplot$T), y = 132,
             #label = (23488 - unique(as.numeric(as.character(dfplot$T)))), size = 4) + # size of the removal 
             label = paste( round((23488 - unique(as.numeric(as.character(dfplot$T))))/23488 * 100,2), "%"), size = 4) + # percentage remove from the max allowed. 
    annotate(geom = "text",x = unique(dfplot$T), y = 140,
             label = paste( round((23488 - unique(as.numeric(as.character(dfplot$T))))/ unique(as.numeric(as.character(dfplot$T))) * 100,2), "%"), size = 4) + # perecentage of overflow relative to the min elements required.
    labs(x = "T", 
         y = "sqrt(Avg Remove Time X Avg total running time)  ms",
         title="% of overflow allowed relative to T \n% of removed elements relative to the max (23.488.000 elements)"
         )-> p

p
#+end_src

#+RESULTS:
[[file:./img/removalTradeoff.png]]


Best T Values based on relation ( Avg Remove time \times Avg running time): 
#+begin_src R :results table :exports both :session :colnames yes
inner_join(dfplot,totalPlot) %>% 
mutate ( ratio = sqrt(RemoveTime * avg)) %>%
group_by(algo) %>% 
top_n(-1,ratio) -> tmp
names(tmp) = c("algo","T","Rm Time Avg","Rm Time Sum","Rm  stdv","Total Time sum","Total Time Avg","Total stdv","ratio")
    
tmp
#+end_src

#+RESULTS:
| algo          |     T | Rm Time Avg | Rm Time Sum | Rm  stdv | Total Time sum | Total Time Avg | Total stdv |  ratio |
|---------------+-------+-------------+-------------+----------+----------------+----------------+------------+--------|
| BTree         | 22020 |     970.873 |   15533.975 |   17.256 |      26699.386 |          1.070 |     24.568 | 32.229 |
| GeoHashBinary | 17616 |     633.379 |    2533.517 |   12.922 |      34272.027 |          1.167 |      7.381 | 27.191 |
| RTree         | 23305 |     691.369 |   88495.277 |   18.703 |     112545.550 |          4.755 |     50.647 | 57.334 |
#+TBLFM: @2$3..@4$9=$0;%0.3f


*** Next tests                                                     :export:
We will have to run this benchmark again using the optimal T parameter for the PMQ (17616) and configuring the optimal removal frequency / size for the Rtree and the Btree.


|       | optimal % of overflow |
|-------+-----------------------|
| BTree |                 6.67% |
| RTree |                 0.79% |
| PMQ   |                33.33% |



