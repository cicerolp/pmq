# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries - Twitter Dataset
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* DONE Description                                                   :export:

Test the queries on Twitter Dataset and compare the following performances.

- PMQ / GEOHASH
- DENSE Vector + timSort
- BTREE 
- RTREE - quadratic algorithm 
- RTREE - quadratic algorithm with bulk loading

Use the refinement level = 8 

Elements:
- Batch size = 1000
- Variate T = from 1M to 128M
- Variate Element size also = from 16 to 256
 
#+begin_src python :results output :exports both

minitems = 1000
maxitems = 128000

items = minitems
while( items <= maxitems) :
    print(items*1000)
    items *=2
#+end_src

#+RESULTS:
: 1000000
: 2000000
: 4000000
: 8000000
: 16000000
: 32000000
: 64000000
: 128000000

- Total elements = T * 1000  
  
Use the synthetic queries generated by the DoE in [[file:~/Projects/pmq/data/queriesLHS.org::#queries20170923145357][Twitter Data]].

queries Dataset : [[file:~/Projects/pmq/data/queriesTwitter.csv]]


# Results in [[file:exp.pdf]]

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  
  
* DONE Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171016155353

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: [master 79c39e9] LBK: new entry for exp20171016155353
:  1 file changed, 45 insertions(+), 1 deletion(-)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171016155353
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171016155353 08e6c64] Initial commit for exp20171016155353
 1 file changed, 835 insertions(+)
 create mode 100644 data/cicero/exp20171016155353/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 08e6c64 (HEAD -> exp20171016155353) Initial commit for exp20171016155353
: * 79c39e9 (master) LBK: new entry for exp20171016155353
: * d0189bf workaround - performance bugs ?

** DONE Export run script 

#+begin_src sh :results output :exports both

for I in 1 2 4 8 16 32 64 128 ; do
    T=$(($I * 1000))
    echo "$T"
done
#+end_src

#+RESULTS:
: 1000
: 2000
: 4000
: 8000
: 16000
: 32000
: 64000
: 128000

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DELT_SIZE=0 -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=ON -DBENCH_BTREE=ON -DBENCH_RTREE=ON -DBENCH_DENSE=ON -DBENCH_RTREE_BULK=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 

#Run queries
b=1000
#n=$(($t*$b))
ref=8

for EL in 16 32 64 128 256 ; do
    ELTSIZE=$(($EL-16))
    cmake -DELT_SIZE=$ELTSIZE . ; make
    
    for i in 1 2 4 8 16 32 64 128 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dat -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesTwitter.csv >  ${TMPDIR}/bench_queries_region_twitter_${t}_${b}_${ref}_${ELTSIZE}_${EXECID}.log
    done
done

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org
	modified:   run.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	exp.pdf
	exp.tex
	img/

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171012184842 2292431] UPD: run.sh script
:  2 files changed, 2 insertions(+), 2 deletions(-)

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

75 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Fri Oct 13 16:41:34 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 23, done.
(1/20)           remote: Compressing objects:  10% (2/20)           remote: Compressing objects:  15% (3/20)           remote: Compressing objects:  20% (4/20)           remote: Compressing objects:  25% (5/20)           remote: Compressing objects:  30% (6/20)           remote: Compressing objects:  35% (7/20)           remote: Compressing objects:  40% (8/20)           remote: Compressing objects:  45% (9/20)           remote: Compressing objects:  50% (10/20)           remote: Compressing objects:  55% (11/20)           remote: Compressing objects:  60% (12/20)           remote: Compressing objects:  65% (13/20)           remote: Compressing objects:  70% (14/20)           remote: Compressing objects:  75% (15/20)           remote: Compressing objects:  80% (16/20)           remote: Compressing objects:  85% (17/20)           remote: Compressing objects:  90% (18/20)           remote: Compressing objects:  95% (19/20)           remote: Compressing objects: 100% (20/20)           remote: Compressing objects: 100% (20/20), done.        
remote: Total 23 (delta 16), reused 0 (delta 0)
(1/23)   Unpacking objects:   8% (2/23)   Unpacking objects:  13% (3/23)   Unpacking objects:  17% (4/23)   Unpacking objects:  21% (5/23)   Unpacking objects:  26% (6/23)   Unpacking objects:  30% (7/23)   Unpacking objects:  34% (8/23)   Unpacking objects:  39% (9/23)   Unpacking objects:  43% (10/23)   Unpacking objects:  47% (11/23)   Unpacking objects:  52% (12/23)   Unpacking objects:  56% (13/23)   Unpacking objects:  60% (14/23)   Unpacking objects:  65% (15/23)   Unpacking objects:  69% (16/23)   Unpacking objects:  73% (17/23)   Unpacking objects:  78% (18/23)   Unpacking objects:  82% (19/23)   Unpacking objects:  86% (20/23)   Unpacking objects:  91% (21/23)   Unpacking objects:  95% (22/23)   Unpacking objects: 100% (23/23)   Unpacking objects: 100% (23/23), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20171012184842
M	data/cicero/exp20171012184842/run_1507849705
Already on 'exp20171012184842'
Your branch is behind 'origin/exp20171012184842' by 4 commits, and can be fast-forwarded.
  (use "git pull" to update your local branch)
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Updating 6c842e8..2292431
Fast-forward
 benchmarks/bench_queries_region.cpp   |   3 +
 data/cicero/exp20171012184842/exp.org | 724 +++++++++++++++++-----------------
 data/cicero/exp20171012184842/run.sh  |   2 +-
 3 files changed, 376 insertions(+), 353 deletions(-)
commit 229243171c14b0e2c7cc9d9a4b1ffc0d6017cc79
Date:   Fri Oct 13 16:43:23 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171016155353$ julio@cicero:~/Projects/pmq/data/cicero/exp20171016155353$ julio@cicero:~/Projects/pmq/data/cicero/exp20171016155353$ julio@cicero:~/Projects/pmq/data/cicero/exp20171016155353$ 1510826640

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
: no server running on /tmp/tmux-1001/default
: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
: julio    12837  0.0  0.0  45248  4652 ?        Ss   08:34   0:00 /lib/systemd/systemd --user
: julio    12839  0.0  0.0 145796  2544 ?        S    08:34   0:00 (sd-pam)
: julio    12889  0.0  0.0  97576  3440 ?        R    08:34   0:00 sshd: julio@pts/8
: julio    12890  0.0  0.0  22748  5224 pts/8    Ss   08:34   0:00 -bash
: julio    12914  0.0  0.0  37368  3224 pts/8    R+   08:35   0:00 ps ux

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
git commit -a -m "wip"
git status
git pull --rebase origin $expId
#+end_src

#+RESULTS:
#+begin_example
[exp20171012184842 37984b2] wip
 1 file changed, 29 insertions(+), 26 deletions(-)
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../exp20170825181747/
	../exp20170830124159/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	../exp20170923144931/
	../exp20170923193058/
	../exp20171009155025/
	exp.pdf
	exp.tex
	img/
	../../queriesLHS.html
	../../queriesLHS_BACKUP_23848.org
	../../queriesLHS_BASE_23848.org
	../../queriesLHS_LOCAL_23848.org
	../../queriesLHS_REMOTE_23848.org
	../../randomLhsQueries.png
	../../../history.txt
	../../../qqqq

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Applying: wip
#+end_example


* DONE Analysis
:PROPERTIES:
:CUSTOM_ID: exp20171115231329
:END:
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tarFile
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tarFile
| log_1508177471.tgz |
| log_1510826640.tgz |

#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tarFile[1]
for i in $f; do 
    tar xvzf $i
done
#+end_src

#+RESULTS: logFile
: bench_queries_region_twitter_1000_1000_10_0_1510826640.log
: bench_queries_region_twitter_128000_1000_10_0_1510826640.log
: bench_queries_region_twitter_16000_1000_10_0_1510826640.log
: bench_queries_region_twitter_2000_1000_10_0_1510826640.log
: bench_queries_region_twitter_32000_1000_10_0_1510826640.log
: bench_queries_region_twitter_4000_1000_10_0_1510826640.log
: bench_queries_region_twitter_64000_1000_10_0_1510826640.log
: bench_queries_region_twitter_8000_1000_10_0_1510826640.log

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile
for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
: bench_queries_region_twitter_1000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_128000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_16000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_2000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_32000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_4000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_64000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_8000_1000_10_0_1510826640.csv

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

#+LATEX_HEADER:  \usepackage[a4paper,includeheadfoot,margin=2cm]{geometry}
 
*** Prepare
:PROPERTIES:
:CUSTOM_ID: df20171115232624
:END:


Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep=""), col_types="ccicicdcici" ) %>%
         mutate (
            EltSize = as.factor(
                 gsub("bench_queries_region_twitter_[[:digit:]]+_[[:digit:]]+_[[:digit:]]+_([[:digit:]]+)_[[:digit:]]+.csv","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
[1] "bench_queries_region_twitter_1000_1000_10_0_1510826640.csv"  
[2] "bench_queries_region_twitter_128000_1000_10_0_1510826640.csv"
[3] "bench_queries_region_twitter_16000_1000_10_0_1510826640.csv" 
[4] "bench_queries_region_twitter_2000_1000_10_0_1510826640.csv"  
[5] "bench_queries_region_twitter_32000_1000_10_0_1510826640.csv" 
[6] "bench_queries_region_twitter_4000_1000_10_0_1510826640.csv"  
[7] "bench_queries_region_twitter_64000_1000_10_0_1510826640.csv" 
[8] "bench_queries_region_twitter_8000_1000_10_0_1510826640.csv"
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning messages:
1: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
2: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
3: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
4: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
5: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
6: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
7: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
8: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
#+end_example


Remove useless columns
#+begin_src R :results output :exports both 
names(df) <- c("algo" , "V2" , "queryId", "V4", "T", "bench" , "ms" , "V8", "Refine","V10","Count","EltSize")

df <- select(df, -V2, -V4, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 38,400 x 8
            algo queryId     T           bench       ms Refine  Count EltSize
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>  <fctr>
 1 GeoHashBinary       0  1000 apply_at_region 0.368360     69 924827       0
 2 GeoHashBinary       0  1000 apply_at_region 0.360399     69 924827       0
 3 GeoHashBinary       0  1000 apply_at_region 0.358103     69 924827       0
 4 GeoHashBinary       0  1000 apply_at_region 0.357762     69 924827       0
 5 GeoHashBinary       0  1000 apply_at_region 0.357186     69 924827       0
 6 GeoHashBinary       0  1000 apply_at_region 0.357522     69 924827       0
 7 GeoHashBinary       0  1000 apply_at_region 0.357021     69 924827       0
 8 GeoHashBinary       0  1000 apply_at_region 0.357517     69 924827       0
 9 GeoHashBinary       0  1000 apply_at_region 0.357360     69 924827       0
10 GeoHashBinary       0  1000 apply_at_region 0.356737     69 924827       0
# ... with 38,390 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both
df <- 
    df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) %>%
    mutate(queryWidth = 90 / 2**(queryId %/% 10))  %>%   # comput info about query width
    mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> df # adjust the actual size of the elements
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :exports both
options(tibble.width = Inf)
df 

dfAvg <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfAvg
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 38,400 x 9
            algo queryId     T           bench       ms Refine  Count EltSize queryWidth
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>   <dbl>      <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 0.368360     69 924827      16         90
 2 GeoHashBinary       0  1000 apply_at_region 0.360399     69 924827      16         90
 3 GeoHashBinary       0  1000 apply_at_region 0.358103     69 924827      16         90
 4 GeoHashBinary       0  1000 apply_at_region 0.357762     69 924827      16         90
 5 GeoHashBinary       0  1000 apply_at_region 0.357186     69 924827      16         90
 6 GeoHashBinary       0  1000 apply_at_region 0.357522     69 924827      16         90
 7 GeoHashBinary       0  1000 apply_at_region 0.357021     69 924827      16         90
 8 GeoHashBinary       0  1000 apply_at_region 0.357517     69 924827      16         90
 9 GeoHashBinary       0  1000 apply_at_region 0.357360     69 924827      16         90
10 GeoHashBinary       0  1000 apply_at_region 0.356737     69 924827      16         90
# ... with 38,390 more rows
# A tibble: 3,840 x 10
# Groups:   algo, queryId, T, bench, Refine, Count, EltSize [?]
    algo queryId     T           bench Refine    Count EltSize queryWidth    avg_ms        stdv
   <chr>   <int> <int>           <chr>  <int>    <int>   <dbl>      <dbl>     <dbl>       <dbl>
 1 BTree       0  1000 apply_at_region     56   924827      16         90  10.79794 0.043603700
 2 BTree       0  1000  scan_at_region     69       NA      16         90  17.57096 0.032625832
 3 BTree       0  2000 apply_at_region     64  1855890      16         90  26.17268 0.925384913
 4 BTree       0  2000  scan_at_region     78       NA      16         90  36.56094 0.020643277
 5 BTree       0  4000 apply_at_region     77  3706387      16         90  55.67314 0.452575406
 6 BTree       0  4000  scan_at_region     95       NA      16         90  73.39717 0.009398233
 7 BTree       0  8000 apply_at_region     99  7417949      16         90 116.19720 0.128751958
 8 BTree       0  8000  scan_at_region    119       NA      16         90 148.95930 0.045864656
 9 BTree       0 16000 apply_at_region    119 14876686      16         90 235.58370 0.473857703
10 BTree       0 16000  scan_at_region    138       NA      16         90 306.47150 4.774481688
# ... with 3,830 more rows
#+end_example

Set the Count values on scan_at_region lines
#+begin_src R :results output :exports both
dfAvg %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,T,EltSize) %>%
    left_join( 
        filter(ungroup(dfAvg), bench == "scan_at_region") %>% select(-Count)
    ) -> dfCount
dfCount
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "algo", "T", "EltSize")
# A tibble: 1,920 x 10
   queryId  algo     Count      T EltSize          bench Refine queryWidth     avg_ms        stdv
     <int> <chr>     <int>  <int>   <dbl>          <chr>  <int>      <dbl>      <dbl>       <dbl>
 1       0 BTree    924827   1000      16 scan_at_region     69         90   17.57096 0.032625832
 2       0 BTree   1855890   2000      16 scan_at_region     78         90   36.56094 0.020643277
 3       0 BTree   3706387   4000      16 scan_at_region     95         90   73.39717 0.009398233
 4       0 BTree   7417949   8000      16 scan_at_region    119         90  148.95930 0.045864656
 5       0 BTree  14876686  16000      16 scan_at_region    138         90  306.47150 4.774481688
 6       0 BTree  29764961  32000      16 scan_at_region    164         90  691.26310 3.454733997
 7       0 BTree  59715461  64000      16 scan_at_region    215         90 1674.93300 1.434565749
 8       0 BTree 119931295 128000      16 scan_at_region    280         90 3774.81400 0.300710270
 9       1 BTree    929918   1000      16 scan_at_region     75         90   17.72378 0.065363272
10       1 BTree   1866101   2000      16 scan_at_region     82         90   36.78344 0.013010013
# ... with 1,910 more rows
#+end_example

#+begin_src R :results output :exports both 
dfAvg %>% filter(EltSize == 16) %>%
#filter(algo=="GeoHashBinary", bench == "scan_at_region")
filter(queryId == 54, bench == "scan_at_region") %>% print(n= 40)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 24 x 10
# Groups:   algo, queryId, T, bench, Refine, Count, EltSize [24]
            algo queryId      T          bench Refine Count EltSize queryWidth     avg_ms        stdv
           <chr>   <int>  <int>          <chr>  <int> <int>   <dbl>      <dbl>      <dbl>       <dbl>
 1         BTree      54   1000 scan_at_region     54    NA      16     2.8125  0.1722442 0.001359936
 2         BTree      54   2000 scan_at_region     54    NA      16     2.8125  0.3623205 0.001412936
 3         BTree      54   4000 scan_at_region     57    NA      16     2.8125  0.8564829 0.015671454
 4         BTree      54   8000 scan_at_region     57    NA      16     2.8125  2.4987550 0.338523891
 5         BTree      54  16000 scan_at_region     57    NA      16     2.8125  6.1519900 0.067033666
 6         BTree      54  32000 scan_at_region     57    NA      16     2.8125 13.4682500 0.081955981
 7         BTree      54  64000 scan_at_region     57    NA      16     2.8125 29.0319100 0.161425741
 8         BTree      54 128000 scan_at_region     57    NA      16     2.8125 63.6237300 0.223006198
 9 GeoHashBinary      54   1000 scan_at_region     54    NA      16     2.8125  0.1022972 0.001229695
10 GeoHashBinary      54   2000 scan_at_region     54    NA      16     2.8125  0.2077824 0.003245627
11 GeoHashBinary      54   4000 scan_at_region     57    NA      16     2.8125  0.4410869 0.016237251
12 GeoHashBinary      54   8000 scan_at_region     57    NA      16     2.8125  0.9292315 0.026309190
13 GeoHashBinary      54  16000 scan_at_region     57    NA      16     2.8125  2.0094240 0.006193988
14 GeoHashBinary      54  32000 scan_at_region     57    NA      16     2.8125  4.0019020 0.010871094
15 GeoHashBinary      54  64000 scan_at_region     57    NA      16     2.8125  8.1800160 0.009785588
16 GeoHashBinary      54 128000 scan_at_region     57    NA      16     2.8125 16.2683800 0.014675664
17         RTree      54   1000 scan_at_region     NA    NA      16     2.8125  0.0781826 0.001813145
18         RTree      54   2000 scan_at_region     NA    NA      16     2.8125  0.1915856 0.011464444
19         RTree      54   4000 scan_at_region     NA    NA      16     2.8125  0.3287351 0.001247785
20         RTree      54   8000 scan_at_region     NA    NA      16     2.8125  0.8917463 0.009426463
21         RTree      54  16000 scan_at_region     NA    NA      16     2.8125  2.8800780 0.133889354
22         RTree      54  32000 scan_at_region     NA    NA      16     2.8125  5.8271660 0.024404850
23         RTree      54  64000 scan_at_region     NA    NA      16     2.8125 12.6769400 0.014649930
24         RTree      54 128000 scan_at_region     NA    NA      16     2.8125 24.3338000 0.022087402
#+end_example

*** Plot: change of query count with size of T.                    :export:
#+begin_src R :results output graphics :file "./img/count_by_T.png" :exports both :width 600 :height 400 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region") %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId%%10))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryWidth, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:./img/count_by_T.png]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryId, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038hDO.pdf]]

*** Plot: Scan Query Time by T facet by queryId                    :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
#    filter(EltSize == 0) %>%
    filter(bench == "scan_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free_y",ncol=10,labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038uNU.pdf]]

*** Plot: Scan query time by Query Count faceted by QueryId        :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure20387Xa.pdf]]

*** Plot: Scan query throughput by Query Count faceted by QueryId  :export:
:PROPERTIES: 
:HEADER-ARGS:R: :exports results
:END:      

#+begin_src R :results output :exports both :session 

tgpPlot <- function(dfCount){
    dfCount %>% 
        filter(queryWidth == w) %>%
        arrange(Count,T) %>%
        mutate(lbls = paste(Count," (",T/1000,")",sep="")) %>%
        ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +  
                                        #ggplot(aes(x = factor(Count), group=algo, y = Count / avg_ms, color = algo)) +  
        geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
        geom_line() +
        facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
        theme(legend.position = "bottom",
              axis.text.x = element_text(angle = 45, hjust = 1)) +
        labs(x = "Query Count ( Dataset size x 10^{6} )",
             title = paste("Query width = ", w)) 

}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Iig.pdf]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot(dfCount)

#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Vsm.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038i2s.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038vAz.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038hKC.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[6]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038uUI.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[7]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure20387eO.pdf]]


*** Plot: Element Size influence on Throughput Analysis by QueryId

#+begin_src R :results output :exports both :session 

tgpPlot2 <- function(dfCount){

dfCount %>% 
    filter(queryWidth == w) %>%
    #filter(queryId == 70) %>%
    arrange(T,Count) %>%
    mutate(lbls = paste(T/1000,"M (",Count,")",sep="")) %>%
    mutate(`T (Count)` = factor(lbls,levels=unique(lbls))) %>%
    #ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +
    ggplot(aes(x = factor(EltSize), group=algo, y = Count / avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
    geom_line() +
    facet_wrap(queryId~ `T (Count)` , scale = "fixed", ncol = 8,  labeller = label_both) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = paste("Query width = ", w)) 
}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w1.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot2(dfCount)

#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w1.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w2.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w2.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w3.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w3.pdf]]

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w4.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w4.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w5.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w5.pdf]]

*** Plot: Histogram of Queries Return Size

- Count histogram
- Aggregation histogram

#+begin_src R :results output :exports both :session 
dfHist <- 
dfCount %>% 
#    filter(algo=="BTree") %>%
#    filter(queryWidth == levelsWidth[[3]] ) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) 

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 800 :height 400 :session 
dfHist %>% 
    ggplot(aes(x = factor(EltsBin) , y = n/length(levels(factor(dfCount$EltSize)))  , fill=(T))) +
    geom_bar(stat="identity") +
    facet_grid(algo~queryWidth, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038IpU.png]]


We should separate the "Ts"

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 


dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
#    filter(T == 1000) %>% 
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge") +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Vza.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black",size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038i9g.pdf]]

**** Plot: Througput barchart by T for a small query 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
#    mutate(EltsBin = Count %/% 100) %>%
#    group_by(algo,EltsBin, queryWidth,T) %>%
#    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count) , y = Count / avg_ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black", size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038vHn.pdf]]

* DONE Analysis of "False positive" ratios
:PROPERTIES:
:CUSTOM_ID: exp20171113110624
:END:

** Running

Running src from master branch.

#+begin_src sh :session teste :results output :exports both :eval query
cd ~/Projects/pmq/build-release/
 
    for i in 1 2 4 8 16 32 64 128 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dmp -x 1 -rate 1000 -min_t ${t} -max_t ${t} -ref 10 -bf ../data/queriesTwitter.csv >> bench_queries_region_twitter_ratio_analysis_1_to_128.log
    done
#+end_src

#+begin_src sh :results output :exports both
cd ~/Projects/pmq/build-release/
git log -1 master --oneline

git add bench_queries_region_twitter_ratio_analysis_1_to_128.log

tail bench_queries_region_twitter_ratio_analysis_1_to_128.log
#+end_src

#+RESULTS:
#+begin_example
a181d8c pmq: return true positives
QueryBench GeoHashBinary ; query ; 76 ; T ; 128000 ; scan_at_region ; 3.10143 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 30670 ; scan_at_region_false_positives ; 243768 ; scan_at_region_true_and_false ; 274438 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 20.4222 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 77 ; T ; 128000 ; apply_at_region ; 26.8968 ; apply_at_region_refinements ; 2 ; count ; 1521534 ; 
QueryBench GeoHashBinary ; query ; 77 ; T ; 128000 ; scan_at_region ; 29.137 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 1.52153e+06 ; scan_at_region_false_positives ; 1.11056e+06 ; scan_at_region_true_and_false ; 2.6321e+06 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 19.7861 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 78 ; T ; 128000 ; apply_at_region ; 31.5211 ; apply_at_region_refinements ; 2 ; count ; 868311 ; 
QueryBench GeoHashBinary ; query ; 78 ; T ; 128000 ; scan_at_region ; 37.4016 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 868311 ; scan_at_region_false_positives ; 2.69751e+06 ; scan_at_region_true_and_false ; 3.56582e+06 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 19.6703 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 79 ; T ; 128000 ; apply_at_region ; 9.45635 ; apply_at_region_refinements ; 2 ; count ; 822726 ; 
QueryBench GeoHashBinary ; query ; 79 ; T ; 128000 ; scan_at_region ; 10.5603 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 822726 ; scan_at_region_false_positives ; 137430 ; scan_at_region_true_and_false ; 960156 ; 
#+end_example


#+begin_src sh :results output :exports both
cp ~/Projects/pmq/build-release/bench_queries_region_twitter_ratio_analysis_1_to_128.log .
git add -f bench_queries_region_twitter_ratio_analysis_1_to_128.log
#+end_src

#+RESULTS:



** Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList="bench_queries_region_twitter_ratio_analysis_1_to_128.log"

for logFile in $logFileList ; 
do
    output=$( basename -s .log $logFile).csv
    echo $output 
    grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile2
#+RESULTS:
: bench_queries_region_twitter_ratio_analysis_1_to_128.csv


** Prepare

Load the CSV into R

Remove useless columns
- Milliseconds and Refine are note relevant in this test
- FPOS FPOS and ALLPOS must be float
  
#+begin_src R :results output :exports both :var path=(print default-directory) :session
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

f="bench_queries_region_twitter_ratio_analysis_1_to_128.csv"

# Warining: make sure we read the values as doudle because for csv format number issues
df2 <- read_delim(f,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:15),sep=""), col_types="ccicicdcicdcdcd" )
# df2

names(df2) <- c("algo" , "V2" , "queryId", "V4", 
                "T", "bench" , "ms" , "V8","Refine","V10","TPOS","V12","FPOS","V14","ALLPOS")
df2 <- select(df2, -V2, -V4, -V8, -V10, -V12, -V14, -ms, -Refine)
df2 

                                        #df2 %>% filter(queryId == 55, T == 16000) 
#+end_src

#+RESULTS:
#+begin_example
Warning: 1280 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' file 2     2  <NA> 15 columns 16 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' col 4     4  <NA> 15 columns 16 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning message:
In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
# A tibble: 1,280 x 7
            algo queryId     T           bench   TPOS  FPOS ALLPOS
           <chr>   <int> <int>           <chr>  <dbl> <dbl>  <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 924827    NA     NA
 2 GeoHashBinary       0  1000  scan_at_region  30329   798  31127
 3 GeoHashBinary       1  1000 apply_at_region 929918    NA     NA
 4 GeoHashBinary       1  1000  scan_at_region   4293  2357   6650
 5 GeoHashBinary       2  1000 apply_at_region 753921    NA     NA
 6 GeoHashBinary       2  1000  scan_at_region   6017 65222  71239
 7 GeoHashBinary       3  1000 apply_at_region 989228    NA     NA
 8 GeoHashBinary       3  1000  scan_at_region      0     1      1
 9 GeoHashBinary       4  1000 apply_at_region 929320    NA     NA
10 GeoHashBinary       4  1000  scan_at_region   3695  2955   6650
# ... with 1,270 more rows
#+end_example

#+begin_src R :results output :exports both :session 
str(df2)
#+end_src

#+RESULTS:
#+begin_example
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	1280 obs. of  7 variables:
 $ algo   : chr  "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" ...
 $ queryId: int  0 0 1 1 2 2 3 3 4 4 ...
 $ T      : int  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...
 $ bench  : chr  "apply_at_region" "scan_at_region" "apply_at_region" "scan_at_region" ...
 $ TPOS   : num  924827 30329 929918 4293 753921 ...
 $ FPOS   : num  NA 798 NA 2357 NA ...
 $ ALLPOS : num  NA 31127 NA 6650 NA ...
 - attr(*, "problems")=Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	1280 obs. of  5 variables:
  ..$ row     : int  1 2 3 4 5 6 7 8 9 10 ...
  ..$ col     : chr  NA NA NA NA ...
  ..$ expected: chr  "15 columns" "15 columns" "15 columns" "15 columns" ...
  ..$ actual  : chr  "12 columns" "16 columns" "12 columns" "16 columns" ...
  ..$ file    : chr  "'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'" "'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'" "'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'" "'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'" ...
 - attr(*, "spec")=List of 2
  ..$ cols   :List of 15
  .. ..$ V1 : list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V2 : list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V3 : list()
  .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
  .. ..$ V4 : list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V5 : list()
  .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
  .. ..$ V6 : list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V7 : list()
  .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
  .. ..$ V8 : list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V9 : list()
  .. .. ..- attr(*, "class")= chr  "collector_integer" "collector"
  .. ..$ V10: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V11: list()
  .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
  .. ..$ V12: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V13: list()
  .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
  .. ..$ V14: list()
  .. .. ..- attr(*, "class")= chr  "collector_character" "collector"
  .. ..$ V15: list()
  .. .. ..- attr(*, "class")= chr  "collector_double" "collector"
  ..$ default: list()
  .. ..- attr(*, "class")= chr  "collector_guess" "collector"
  ..- attr(*, "class")= chr "col_spec"
#+end_example

#+begin_src R :results output :exports both :session 
df2 %>% 
    filter(bench=="scan_at_region") %>%
    select(-algo,-bench) -> dfRatios
    
dfRatios
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 5
   queryId     T  TPOS  FPOS ALLPOS
     <int> <int> <dbl> <dbl>  <dbl>
 1       0  1000 30329   798  31127
 2       1  1000  4293  2357   6650
 3       2  1000  6017 65222  71239
 4       3  1000     0     1      1
 5       4  1000  3695  2955   6650
 6       5  1000 28635  2492  31127
 7       6  1000     0     1      1
 8       7  1000 79127   773  79900
 9       8  1000     1     0      1
10       9  1000 76054  3846  79900
# ... with 630 more rows
#+end_example

Join with dataframe with real performance measurements to get the false-positives
#+begin_src R :results output :exports both :session 
dfCount %>% 
    left_join( dfRatios ) %>%
    mutate(TotScans = ALLPOS + (Count - TPOS)) -> dfCR  # to compute the ratio based on all the elements scanned. 

dfCR
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "T")
# A tibble: 1,920 x 14
   queryId  algo     Count      T EltSize          bench Refine queryWidth     avg_ms        stdv    TPOS   FPOS  ALLPOS  TotScans
     <int> <chr>     <int>  <int>   <dbl>          <chr>  <int>      <dbl>      <dbl>       <dbl>   <dbl>  <dbl>   <dbl>     <dbl>
 1       0 BTree    924827   1000      16 scan_at_region     69         90   17.57096 0.032625832   30329    798   31127    925625
 2       0 BTree   1855890   2000      16 scan_at_region     78         90   36.56094 0.020643277   60741   1727   62468   1857617
 3       0 BTree   3706387   4000      16 scan_at_region     95         90   73.39717 0.009398233  126563   3543  130106   3709930
 4       0 BTree   7417949   8000      16 scan_at_region    119         90  148.95930 0.045864656  256772   7505  264277   7425454
 5       0 BTree  14876686  16000      16 scan_at_region    138         90  306.47150 4.774481688  496802  16719  513521  14893405
 6       0 BTree  29764961  32000      16 scan_at_region    164         90  691.26310 3.454733997  985658  36281 1021940  29801243
 7       0 BTree  59715461  64000      16 scan_at_region    215         90 1674.93300 1.434565749 1926400  72424 1998820  59787881
 8       0 BTree 119931295 128000      16 scan_at_region    280         90 3774.81400 0.300710270 3738880 149232 3888110 120080525
 9       1 BTree    929918   1000      16 scan_at_region     75         90   17.72378 0.065363272    4293   2357    6650    932275
10       1 BTree   1866101   2000      16 scan_at_region     82         90   36.78344 0.013010013    8484   4148   12632   1870249
# ... with 1,910 more rows
#+end_example


+Compute Speedups+ (issue with mismatched count)
#+begin_src R :results output :exports both :session :eval never
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree")) %>%
    filter(EltSize == 16) %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up_RTree  = RTree_avg_ms / GeoHashBinary_avg_ms ) -> dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up_RTree)) %>% arrange(ord)

#+end_src

Compute Speedups
#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree","BTree")) %>%
    filter(EltSize == 16) %>%
#Because the Count diverges for Rtree and PMQ , gathering with count 
    #select(queryId, algo, T, queryWidth, avg_ms, stdv, Count, FPOS, ALLPOS, TotScans ) %>% #print() %>%
    #gather(variable, value, Count, avg_ms, stdv) %>% #print() %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up_RTree  = RTree_avg_ms / GeoHashBinary_avg_ms, speed_up_BTree  = BTree_avg_ms / GeoHashBinary_avg_ms  ) %>% print() %>%
    inner_join( dfCR %>% 
                filter(algo == "GeoHashBinary", EltSize == 16) %>% 
                select(queryId, T, Count, TotScans)) ->  dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up_RTree)) %>% arrange(ord)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 13
   queryId      T queryWidth   FPOS  ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms   RTree_stdv speed_up_RTree speed_up_BTree
     <int>  <int>      <dbl>  <dbl>   <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>        <dbl>          <dbl>          <dbl>
 1       0   1000         90    798   31127     17.57096 0.032625832             4.198085        0.009008462     21.00175   0.04536605       5.002698       4.185470
 2       0   2000         90   1727   62468     36.56094 0.020643277             8.254625        0.005502699     42.95545   0.03396309       5.203804       4.429146
 3       0   4000         90   3543  130106     73.39717 0.009398233            16.377700        0.013458991     88.08160   0.11862215       5.378142       4.481531
 4       0   8000         90   7505  264277    148.95930 0.045864656            32.672720        0.050353965    181.96160   0.12293106       5.569221       4.559134
 5       0  16000         90  16719  513521    306.47150 4.774481688            65.360360        0.110740249    377.70870   0.14220490       5.778865       4.688951
 6       0  32000         90  36281 1021940    691.26310 3.454733997           130.588700        0.072374105    773.43210   0.14898132       5.922657       5.293437
 7       0  64000         90  72424 1998820   1674.93300 1.434565749           262.437700        0.275556024   1578.40300   8.93511183       6.014391       6.382212
 8       0 128000         90 149232 3888110   3774.81400 0.300710270           530.257100        0.155919103   3263.20400 128.02417273       6.154003       7.118837
 9       1   1000         90   2357    6650     17.72378 0.065363272             4.213655        0.003204241     21.07038   0.03316333       5.000500       4.206272
10       1   2000         90   4148   12632     36.78344 0.013010013             8.320638        0.008929386     43.12540   0.03046503       5.182944       4.420748
# ... with 630 more rows
Joining, by = c("queryId", "T")
# A tibble: 640 x 16
   queryId      T queryWidth    FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv speed_up_RTree speed_up_BTree    Count TotScans   ord
     <int>  <int>      <dbl>   <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>       <dbl>          <dbl>          <dbl>    <int>    <dbl> <int>
 1      34   1000  11.250000   22533   50484    2.8952230 0.33305870            0.8615798        0.009300642    4.9551050 0.009395758       5.751185       3.360365   165612   188145     1
 2      41  64000   5.625000   10501  135979   55.9229700 0.16999272           10.1798600        0.007992246   68.3978100 0.092908443       6.718934       5.493491  2287819  2298320     2
 3      47  32000   5.625000   41417  269830   13.3127000 0.08582571            4.5306840        0.010303397   17.1525800 0.006907934       3.785870       2.938342   547038   588455     3
 4      53  16000   2.812500  145440  235088   14.2082200 0.02378103            3.5537020        0.006424662   18.9419500 0.027733664       5.330202       3.998146   636045   781485     4
 5      60  64000   1.406250   64542   91381    0.3689068 0.03067647            0.1760892        0.013167764    0.3240643 0.001859956       1.840342       2.095000    26839    91381     5
 6      65  32000   1.406250   15513   62419    0.5459984 0.11431863            0.2149952        0.021671385    0.6719342 0.011436340       3.125345       2.539584    46906    62419     6
 7      69  32000   1.406250  201633  434241    7.7877380 0.14131783            2.1977730        0.032129896    6.1399340 0.021064367       2.793707       3.543468   232608   434241     7
 8      73 128000   0.703125 3330990 6079850  145.8899000 0.10745588           40.1004100        0.013920445   79.0795000 0.152388531       1.972037       3.638115  2748860  6079850     8
 9      33 128000  11.250000 4495900 5663650 1040.6140000 1.26497431          174.7358000        0.046972332 1073.3830000 0.455413121       6.142891       5.955357 38228535 42724435     9
10      40  16000   5.625000   67008   78620   11.7886100 0.05001465            2.5880440        0.006300115   20.7199900 0.032249701       8.006042       4.555027   567574   634582    10
# ... with 630 more rows
#+end_example

** DONE Analysis                                                    :export:

#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(EltSize == 16) %>%
    mutate(Ratio = FPOS / TotScans) %>%
    mutate(Ratio2 = FPOS / ALLPOS) %>%
    mutate(tgp = Count / avg_ms) %>%
    #filter(queryWidth < 1) %>%
    filter(algo == "BTree") %>%
    filter(Count != TPOS) %>%
    arrange(Count)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 493 x 17
   queryId  algo Count     T EltSize          bench Refine queryWidth    avg_ms        stdv  TPOS  FPOS ALLPOS TotScans        Ratio    Ratio2      tgp
     <int> <chr> <int> <int>   <dbl>          <chr>  <int>      <dbl>     <dbl>       <dbl> <dbl> <dbl>  <dbl>    <dbl>        <dbl>     <dbl>    <dbl>
 1      51 BTree  2377  1000      16 scan_at_region     25     2.8125 0.0689187 0.001079228   800  5221   6021     7598 0.6871545143 0.8671317 34489.91
 2      51 BTree  4537  2000      16 scan_at_region     29     2.8125 0.1719340 0.010951927  1503 10271  11774    14808 0.6936115613 0.8723458 26388.03
 3      54 BTree  6164  1000      16 scan_at_region     54     2.8125 0.1722442 0.001359936  4031 39312  43343    45476 0.8644559768 0.9069977 35786.40
 4      15 BTree  8096  1000      16 scan_at_region      6    45.0000 0.2047523 0.006654131     0     0      0     8096 0.0000000000       NaN 39540.46
 5      51 BTree  9398  4000      16 scan_at_region     33     2.8125 0.3176793 0.067314278  3507 21023  24530    30421 0.6910686697 0.8570322 29583.29
 6      42 BTree 11446  1000      16 scan_at_region     40     5.6250 0.1520312 0.011462768   525  7715   8240    19161 0.4026407808 0.9362864 75287.18
 7      45 BTree 11619  1000      16 scan_at_region     39     5.6250 0.1891810 0.001166753   698  7542   8240    19161 0.3936120244 0.9152913 61417.37
 8      54 BTree 12970  2000      16 scan_at_region     54     2.8125 0.3623205 0.001412936  8517 82252  90769    95222 0.8637919808 0.9061684 35797.04
 9      15 BTree 13292  2000      16 scan_at_region      8    45.0000 0.2949124 0.013975346     0     2      2    13294 0.0001504438 1.0000000 45071.01
10      44 BTree 16350  1000      16 scan_at_region     98     5.6250 0.1789286 0.001243622  4390  3232   7622    19582 0.1650495353 0.4240357 91377.23
# ... with 483 more rows
#+end_example

*** Plot : ratios
#+RESULTS:
#+begin_example
# A tibble: 493 x 17
   queryId  algo Count     T EltSize          bench Refine queryWidth    avg_ms         stdv  TPOS  FPOS ALLPOS TotScans        Ratio    Ratio2       tgp
     <int> <chr> <int> <int>   <dbl>          <chr>  <int>      <dbl>     <dbl>        <dbl> <dbl> <dbl>  <dbl>    <dbl>        <dbl>     <dbl>     <dbl>
 1      51 BTree  2377  1000      16 scan_at_region     12     2.8125 0.0935207 0.0050950426   800  5221   6021     7598 0.6871545143 0.8671317 25416.833
 2      51 BTree  4537  2000      16 scan_at_region     12     2.8125 0.2101530 0.0128721513  1503 10271  11774    14808 0.6936115613 0.8723458 21589.033
 3      54 BTree  6164  1000      16 scan_at_region     12     2.8125 0.4925632 0.0012249881  4031 39312  43343    45476 0.8644559768 0.9069977 12514.130
 4      15 BTree  8096  1000      16 scan_at_region      6    45.0000 0.1224318 0.0069989957     0     0      0     8096 0.0000000000       NaN 66126.611
 5      51 BTree  9398  4000      16 scan_at_region     12     2.8125 0.4072375 0.0935481622  3507 21023  24530    30421 0.6910686697 0.8570322 23077.443
 6      42 BTree 11446  1000      16 scan_at_region     23     5.6250 0.2468979 0.0145591697   525  7715   8240    19161 0.4026407808 0.9362864 46359.244
 7      45 BTree 11619  1000      16 scan_at_region     23     5.6250 0.1860315 0.0009233704   698  7542   8240    19161 0.3936120244 0.9152913 62457.165
 8      54 BTree 12970  2000      16 scan_at_region     12     2.8125 1.4501400 0.0451758183  8517 82252  90769    95222 0.8637919808 0.9061684  8943.964
 9      15 BTree 13292  2000      16 scan_at_region      9    45.0000 0.2019838 0.0146983636     0     2      2    13294 0.0001504438 1.0000000 65807.258
10      44 BTree 16350  1000      16 scan_at_region     24     5.6250 0.2060321 0.0008137880  4390  3232   7622    19582 0.1650495353 0.4240357 79356.566
# ... with 483 more rows
#+end_example

#+begin_src R :results output :exports both :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
#w = levelsWidth[[1]]
levelsWidth

#+end_src

#+RESULTS:
: [1] "0.703125" "1.40625"  "2.8125"   "5.625"    "11.25"    "22.5"     "45"      
: [8] "90"

#+begin_src R :results output :exports both :session 
plotRatios <- function(dfCR,w){
    dfCR %>% 
    filter(EltSize == 16) %>%
    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(Count), y = Count / avg_ms, fill=(FPOS / TotScans)) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position="dodge",color="black", size =0.2) +
    facet_wrap(~algo, scale="free", ncol = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
   labs(title = paste("Query width = ", w))
}
#+end_src

#+RESULTS:

Facet free 
#+begin_src R :results output graphics :file "./img/tgp-false-positves2.pdf" :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w)
    print(p)
}
#+end_src

#+RESULTS:
[[file:./img/tgp-false-positves2.pdf]]

Facet free_x
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w) + 
    facet_wrap(~algo, scale="free_x", ncol = 1)
    print(p)
}

#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure20388Rt.pdf]]

*** Speed-Ups of Throughputs                                       :export:

#+begin_src R :results output :exports both :session 
dfCR
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,920 x 14
   queryId  algo     Count      T EltSize          bench Refine queryWidth     avg_ms        stdv    TPOS   FPOS  ALLPOS  TotScans
     <int> <chr>     <int>  <int>   <dbl>          <chr>  <int>      <dbl>      <dbl>       <dbl>   <dbl>  <dbl>   <dbl>     <dbl>
 1       0 BTree    924827   1000      16 scan_at_region     69         90   17.57096 0.032625832   30329    798   31127    925625
 2       0 BTree   1855890   2000      16 scan_at_region     78         90   36.56094 0.020643277   60741   1727   62468   1857617
 3       0 BTree   3706387   4000      16 scan_at_region     95         90   73.39717 0.009398233  126563   3543  130106   3709930
 4       0 BTree   7417949   8000      16 scan_at_region    119         90  148.95930 0.045864656  256772   7505  264277   7425454
 5       0 BTree  14876686  16000      16 scan_at_region    138         90  306.47150 4.774481688  496802  16719  513521  14893405
 6       0 BTree  29764961  32000      16 scan_at_region    164         90  691.26310 3.454733997  985658  36281 1021940  29801243
 7       0 BTree  59715461  64000      16 scan_at_region    215         90 1674.93300 1.434565749 1926400  72424 1998820  59787881
 8       0 BTree 119931295 128000      16 scan_at_region    280         90 3774.81400 0.300710270 3738880 149232 3888110 120080525
 9       1 BTree    929918   1000      16 scan_at_region     75         90   17.72378 0.065363272    4293   2357    6650    932275
10       1 BTree   1866101   2000      16 scan_at_region     82         90   36.78344 0.013010013    8484   4148   12632   1870249
# ... with 1,910 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree","BTree")) %>%
    filter(EltSize == 16) %>%
                                        #    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(queryId%%10), y = Count / avg_ms, fill=(FPOS / TotScans), group=T) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position=position_dodge(),color="black", size =0.2) +
    geom_errorbar(aes(ymin = Count / (avg_ms-stdv), ymax = Count / (avg_ms + stdv)), position=position_dodge())+ 
    facet_grid(queryWidth~algo, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
                                        #  labs(title = paste("Query width = ", w))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Jcz.pdf]]


*** Speedup - correlations                                         :export:
:PROPERTIES:
:HEADER-ARGS:R: :exports result
:END:


- Order the results by speedup

- try several mapping to a continuos color map

Count
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(Count)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure20387lC.pdf]]

QueryWidth
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=factor(queryWidth)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038IwI.pdf]]

Ratio : FPOS / ALLPOS
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / ALLPOS)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038V6O.pdf]]

Ratio : FPOS / TotScans
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038iEV.pdf]]


The color scale show that the speed-up increases according to ratio of False positives.

Mapping to divergent scale:
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) +scale_fill_distiller(limit=c(0,1),type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038vOb.pdf]]


The divergent scale show that GeoHash is better that the RTree index when the amount of false positives is lower 50% in general.

But when do we have small FPOS Ratio ? 

By grouping queries by queryWidth we can see that FPositive ratio is only a problem on small queries. 
Nevertheless, even on the smallest queries we are able to perform better that the RTree on more than 50% of the cases.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
dfSpeedUp %>% group_by(queryWidth) %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    #geom_bar(aes(fill=(FPOS / TotScans)), color="black", stat="identity",width=1) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity") +
    scale_fill_distiller(limit=c(0,1),type="div") +
    facet_wrap(~queryWidth)+ 
    geom_hline(yintercept = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) -> plt
plt
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure20388Yh.pdf]]

Compute the percentage where PMQ is better than RTree
#+begin_src R :results output  :session 
dfSpeedUp %>% 
    group_by(queryWidth) %>% 
    summarize(tot = length(speed_up_RTree), PmqBest = length(speed_up_RTree[speed_up_RTree>1]), `%` = PmqBest / tot * 100) ->
dfPct 

dfPct
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 8 x 4
  queryWidth   tot PmqBest    `%`
       <dbl> <int>   <int>  <dbl>
1   0.703125    80      69  86.25
2   1.406250    80      80 100.00
3   2.812500    80      72  90.00
4   5.625000    80      80 100.00
5  11.250000    80      80 100.00
6  22.500000    80      80 100.00
7  45.000000    80      80 100.00
8  90.000000    80      80 100.00
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
plt +
    geom_vline(data=dfPct, aes(xintercept = 80-PmqBest)) +  
    geom_text(data=dfPct, aes(x=80-PmqBest + 6, y=2, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Jjn.pdf]]


*** Speedup comparison with BTree and RTree                        :export:
:PROPERTIES:
:EXPORT_FILE_NAME: speedup_comparison_BTree_RTree
:HEADER-ARGS:R: :exports results
:END:

These results show the query's throughput speed-up for PMQ over BTree and RTree.

#+begin_src R :results output :exports none :session 
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 15
   queryId      T queryWidth   FPOS  ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms   RTree_stdv     Count  TotScans   algo_speedup val_speedup
     <int>  <int>      <dbl>  <dbl>   <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>        <dbl>     <int>     <dbl>          <chr>       <dbl>
 1       0   1000         90    798   31127     17.57096 0.032625832             4.198085        0.009008462     21.00175   0.04536605    924827    925625 speed_up_RTree    5.002698
 2       0   2000         90   1727   62468     36.56094 0.020643277             8.254625        0.005502699     42.95545   0.03396309   1855890   1857617 speed_up_RTree    5.203804
 3       0   4000         90   3543  130106     73.39717 0.009398233            16.377700        0.013458991     88.08160   0.11862215   3706387   3709930 speed_up_RTree    5.378142
 4       0   8000         90   7505  264277    148.95930 0.045864656            32.672720        0.050353965    181.96160   0.12293106   7417949   7425454 speed_up_RTree    5.569221
 5       0  16000         90  16719  513521    306.47150 4.774481688            65.360360        0.110740249    377.70870   0.14220490  14876686  14893405 speed_up_RTree    5.778865
 6       0  32000         90  36281 1021940    691.26310 3.454733997           130.588700        0.072374105    773.43210   0.14898132  29764961  29801243 speed_up_RTree    5.922657
 7       0  64000         90  72424 1998820   1674.93300 1.434565749           262.437700        0.275556024   1578.40300   8.93511183  59715461  59787881 speed_up_RTree    6.014391
 8       0 128000         90 149232 3888110   3774.81400 0.300710270           530.257100        0.155919103   3263.20400 128.02417273 119931295 120080525 speed_up_RTree    6.154003
 9       1   1000         90   2357    6650     17.72378 0.065363272             4.213655        0.003204241     21.07038   0.03316333    929918    932275 speed_up_RTree    5.000500
10       1   2000         90   4148   12632     36.78344 0.013010013             8.320638        0.008929386     43.12540   0.03046503   1866101   1870249 speed_up_RTree    5.182944
# ... with 1,270 more rows
#+end_example

All the results were sorted by increasing speedUp value. 
The colors show the ratio between amount of false positive elements over the total amount of elements scanned by the query. 
#+begin_src R :results output graphics :file "./img/speedup_BTREE_RTREE.svg"  :width 14 :height 10 :session 
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree) %>%
    arrange( val_speedup) %>% group_by(algo_speedup) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_hline(yintercept = 1) + 
    facet_grid(~algo_speedup) +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_BTREE_RTREE.svg]]

#+CAPTION: Compute the percentages 
#+begin_src R :results output :session :exports none
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree) %>%
    arrange( val_speedup) %>% 
    group_by(algo_speedup,queryWidth) %>% 
    mutate(ord = row_number()) -> dfSpeedUp2

dfSpeedUp2

dfSpeedUp2 %>% 
    summarize(tot = length(val_speedup), PmqBest = length( val_speedup[val_speedup>1]), `%` = PmqBest / tot * 100) ->
dfPct2 

dfPct2
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 16
# Groups:   algo_speedup, queryWidth [16]
   queryId      T queryWidth   FPOS ALLPOS BTree_avg_ms   BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv Count TotScans   algo_speedup val_speedup   ord
     <int>  <int>      <dbl>  <dbl>  <dbl>        <dbl>        <dbl>                <dbl>              <dbl>        <dbl>       <dbl> <int>    <dbl>          <chr>       <dbl> <int>
 1      76 128000   0.703125 243768 274438    4.7404690 0.1530567009            1.6319720       0.0069445467    0.4130925 0.001403953 30670   274438 speed_up_RTree   0.2531247     1
 2      76   8000   0.703125  14409  16086    0.1711801 0.0087036180            0.0900663       0.0018738275    0.0253403 0.002840178  1677    16086 speed_up_RTree   0.2813516     2
 3      76  16000   0.703125  27147  30395    0.3400201 0.0469819861            0.1650061       0.0028614154    0.0500156 0.003817182  3248    30395 speed_up_RTree   0.3031136     3
 4      76   4000   0.703125   7193   7970    0.0759321 0.0030948708            0.0459063       0.0005714718    0.0141471 0.002139187   777     7970 speed_up_RTree   0.3081734     4
 5      76  64000   0.703125 115063 129740    1.9148250 0.0965446851            0.6544457       0.0015265332    0.2114444 0.001966248 14677   129740 speed_up_RTree   0.3230893     5
 6      76   2000   0.703125   3202   3668    0.0277051 0.0008233037            0.0235509       0.0003705055    0.0087181 0.001285961   466     3668 speed_up_RTree   0.3701812     6
 7      76  32000   0.703125  54424  61633    0.5320830 0.0013620595            0.3226035       0.0008273446    0.1264704 0.008279208  7209    61633 speed_up_RTree   0.3920305     7
 8      76   1000   0.703125   1520   1807    0.0138993 0.0009454926            0.0126594       0.0003219521    0.0058668 0.001385825   287     1807 speed_up_RTree   0.4634343     8
 9      51   1000   2.812500   5221   6021    0.0689187 0.0010792281            0.0428774       0.0006956230    0.0300493 0.002875075  2377     7598 speed_up_RTree   0.7008191     1
10      51   2000   2.812500  10271  11774    0.1719340 0.0109519274            0.0820946       0.0016602163    0.0607502 0.004103779  4537    14808 speed_up_RTree   0.7400024     2
# ... with 1,270 more rows
# A tibble: 16 x 5
# Groups:   algo_speedup [?]
     algo_speedup queryWidth   tot PmqBest    `%`
            <chr>      <dbl> <int>   <int>  <dbl>
 1 speed_up_BTree   0.703125    80      80 100.00
 2 speed_up_BTree   1.406250    80      80 100.00
 3 speed_up_BTree   2.812500    80      80 100.00
 4 speed_up_BTree   5.625000    80      80 100.00
 5 speed_up_BTree  11.250000    80      80 100.00
 6 speed_up_BTree  22.500000    80      80 100.00
 7 speed_up_BTree  45.000000    80      80 100.00
 8 speed_up_BTree  90.000000    80      80 100.00
 9 speed_up_RTree   0.703125    80      69  86.25
10 speed_up_RTree   1.406250    80      80 100.00
11 speed_up_RTree   2.812500    80      72  90.00
12 speed_up_RTree   5.625000    80      80 100.00
13 speed_up_RTree  11.250000    80      80 100.00
14 speed_up_RTree  22.500000    80      80 100.00
15 speed_up_RTree  45.000000    80      80 100.00
16 speed_up_RTree  90.000000    80      80 100.00
#+end_example

We can see a correlation between the amount false positives and the Speedup over *RTrees* on small queries. 
This is due to the discontinuities caused by the Z-ordering scheme, used in the GeoHash algorithm. 
We can see that this correlation doesn't appear in the *BTree* speedups.
#+begin_src R :results output graphics :file "./img/speedup_BTREE_RTREE_facet.svg"  :width 14 :height 10 :session 

dfSpeedUp2 %>%
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(limits=c(0,1),type="div") +
    facet_grid(queryWidth~algo_speedup) +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))

#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_BTREE_RTREE_facet.svg]]


We normalized the element count within each querySize to check it the BTree's Speedup increases with the number os elements queried.
# +However, there is not a clear correlation to explain increase of speedup for the BTree case.+
#+begin_src R :results output :exports none :session 
dfSpeedUp2 %>% mutate(max = max(Count)) %>%
    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count)))  %>% arrange(algo_speedup,queryWidth,-ord)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 18
# Groups:   algo_speedup, queryWidth [16]
   queryId      T queryWidth    FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv   Count TotScans   algo_speedup val_speedup   ord     max normCount
     <int>  <int>      <dbl>   <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>       <dbl>   <int>    <dbl>          <chr>       <dbl> <int>   <dbl>     <dbl>
 1      75 128000   0.703125 1436760 4270330     89.56010 0.19456395            19.547030        0.022145481     76.66449  0.10394617 2833578  4270328 speed_up_BTree    4.581775    80 3682101 0.7695367
 2      70 128000   0.703125 2888540 6570640    130.32050 0.17029337            28.723420        0.018174695    104.75755 10.85447705 3682101  6570641 speed_up_BTree    4.537082    79 3682101 1.0000000
 3      75  64000   0.703125  711985 2167550     42.44549 0.26569103             9.989538        0.006512350     39.69537  0.06579260 1455569  2167549 speed_up_BTree    4.248994    78 3682101 0.3952622
 4      70  64000   0.703125 1473250 3337750     60.92869 0.19935289            14.526930        0.022523620     49.83485  0.05642466 1864494  3337754 speed_up_BTree    4.194189    77 3682101 0.5063284
 5      78 128000   0.703125 2697510 3565820     26.14609 0.06857428             6.526117        0.012623471     23.61555  0.02694658  868311  3565820 speed_up_BTree    4.006378    76 3682101 0.2357599
 6      75  32000   0.703125  352145 1086650     19.87228 0.09188685             4.990425        0.009256544     20.29562  0.03942097  734506  1086650 speed_up_BTree    3.982082    75 3682101 0.1994177
 7      70  32000   0.703125  720294 1675500     28.29783 0.13054008             7.298191        0.007861139     25.92945  0.01847516  955205  1675500 speed_up_BTree    3.877376    74 3682101 0.2593607
 8      72 128000   0.703125  184494 1075400     25.98778 0.04825299             6.798633        0.005155347     23.33216  0.02491065  890901  1075400 speed_up_BTree    3.822501    73 3682101 0.2418954
 9      78  64000   0.703125 1348500 1792150     12.49333 0.11335547             3.287206        0.004356041     12.60251  0.01032015  443642  1792150 speed_up_BTree    3.800592    72 3682101 0.1204175
10      75  16000   0.703125  175996  546397      9.46002 0.04787214             2.541230        0.006074275     12.01168  0.01118181  370401   546397 speed_up_BTree    3.722615    71 3682101 0.1005249
# ... with 1,270 more rows
#+end_example

The color scale is normalized between the min and max count of the elements count in each facet.
Note that Count increases as power of 2 . Therefore we take the logarithm to map it to linear color scale. (see Extra sections on org file)
#+begin_src R :results output graphics :file "./img/speedup_color_count.svg" :exports both :width 14 :height 10 :session 
dfSpeedUp2 %>%
    mutate(Count = log2(Count)) %>%
    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count))) %>%
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=normCount), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(type="seq") +
    facet_grid(queryWidth~algo_speedup) +
    labs(fill="Normalized log2(Count)") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_color_count.svg]]

**** EXTRA: verify if relation between SpeedUp and Ratio is linear. :noexport:

To verify that SpeedUp linearly decreases with SpeedUp we plot orderded by ratio.
Although for RTree is not a clear straight line , there is a neat inverse relation between SpeedUp and False Positive ratio. 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session 
dfSpeedUp2 %>% filter(queryWidth < 2) %>%
    ggplot(aes(x = FPOS/TotScans, y = val_speedup) ) +
    geom_line() + geom_point() + 
    facet_wrap(queryWidth~algo_speedup,scale="free",ncol=2) +
    theme(legend.position = "bottom",
#          strip.text = element_blank() , 
          axis.text.x = element_text(angle = 0, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Wtt.png]]

**** EXTRA: Logarithmic interpolation of colors ?               :noexport:

Note that the the dataset doubles the size at each experiments. 
The counts should double in the same way.

So to have a linear X scale we must take the log of Count on x axis.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".svg") :exports both :width 14 :height 10 :session 
dfSpeedUp2 %>%
#    mutate(Count = log2(Count)) %>%
#    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count))) %>%
    ggplot(aes(x = log2(Count), y = val_speedup) ) +
#    geom_bar(aes(fill=normCount), stat="identity",position="dodge",width=1) + 
    geom_line() + geom_point() + 
#    scale_fill_distiller(type="seq") +
    facet_wrap(queryWidth~algo_speedup,scale="free_y",ncol=2) +
    theme(legend.position = "bottom",
          strip.text = element_blank() , 
          axis.text.x = element_text(angle = 0, hjust = 1)) 
#    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
#    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
#    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038j3z.svg]]


#+begin_src R :results output :exports both :session 
s = 2**seq(1,10)
si = (s - min(s)) / (max(s) - min(s))
s
si

log2(s)
#+end_src

#+RESULTS:
:  [1]    2    4    8   16   32   64  128  256  512 1024
:  [1] 0.000000000 0.001956947 0.005870841 0.013698630 0.029354207 0.060665362
:  [7] 0.123287671 0.248532290 0.499021526 1.000000000
:  [1]  1  2  3  4  5  6  7  8  9 10


** [2017-11-17 sex] Analysis with reflevel 10

*** Prepare
Load the CSV into R

Remove useless columns
- Milliseconds and Refine are note relevant in this test
- FPOS FPOS and ALLPOS must be float

[[file:~/Projects/pmq/data/cicero/exp20171111161232/falsePosAnalysisRef10.rds]]

#+begin_src R :results output :exports both :session 
dfFalsePos <- read_rds("~/Projects/pmq/data/cicero/exp20171111161232/falsePosAnalysisRef10.rds")
dfFalsePos
#+end_src  

#+RESULTS:
#+begin_example
# A tibble: 12,800 x 10
            algo queryId     T          bench      ms Refine  TPOS  FPOS ALLPOS RefLevel
           <chr>   <int> <int>          <chr>   <dbl>  <int> <dbl> <dbl>  <dbl>   <fctr>
 1 GeoHashBinary       0  1000 scan_at_region 4.33983     69   445   798   1243       10
 2 GeoHashBinary       0  1000 scan_at_region 4.33345     69   445   798   1243       10
 3 GeoHashBinary       0  1000 scan_at_region 4.34487     69   445   798   1243       10
 4 GeoHashBinary       0  1000 scan_at_region 4.33744     69   445   798   1243       10
 5 GeoHashBinary       0  1000 scan_at_region 4.33808     69   445   798   1243       10
 6 GeoHashBinary       0  1000 scan_at_region 4.32375     69   445   798   1243       10
 7 GeoHashBinary       0  1000 scan_at_region 4.32229     69   445   798   1243       10
 8 GeoHashBinary       0  1000 scan_at_region 4.32376     69   445   798   1243       10
 9 GeoHashBinary       0  1000 scan_at_region 4.32555     69   445   798   1243       10
10 GeoHashBinary       0  1000 scan_at_region 4.32632     69   445   798   1243       10
# ... with 12,790 more rows
#+end_example

#+begin_src R :results output :exports both :session 
dfFalsePos %>% filter(algo == "GeoHashBinary") %>% 
    group_by_at(vars(-ms)) %>% #group_by all expect ms
    summarize(avg_ms = mean(ms), std_ms = sd(ms)) %>%
    ungroup() %>% 
    select(queryId,T,TPOS,FPOS,ALLPOS) -> dfRatios
dfRatios
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 5
   queryId      T  TPOS   FPOS ALLPOS
     <int>  <int> <dbl>  <dbl>  <dbl>
 1       0   1000   445    798   1243
 2       0   2000   980   1727   2707
 3       0   4000  2050   3543   5593
 4       0   8000  4131   7505  11636
 5       0  16000  9012  16719  25731
 6       0  32000 18162  36281  54443
 7       0  64000 41950  72424 114374
 8       0 128000 80707 149232 229939
 9       1   1000   304   1606   1910
10       1   2000   439   2634   3073
# ... with 630 more rows
#+end_example


Join with dataframe with real performance measurements to get the false-positives
#+begin_src R :results output :exports both :session 
dfCount %>% 
    left_join( dfRatios10 ) %>%
    mutate(TotScans = ALLPOS + (Count - TPOS)) -> dfCR  # to compute the ratio based on all the elements scanned. 

dfCR
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "T")
# A tibble: 1,920 x 14
   queryId  algo     Count      T EltSize          bench Refine queryWidth     avg_ms        stdv  TPOS   FPOS ALLPOS  TotScans
     <int> <chr>     <int>  <int>   <dbl>          <chr>  <int>      <dbl>      <dbl>       <dbl> <dbl>  <dbl>  <dbl>     <dbl>
 1       0 BTree    924827   1000      16 scan_at_region     69         90   17.57096 0.032625832   445    798   1243    925625
 2       0 BTree   1855890   2000      16 scan_at_region     78         90   36.56094 0.020643277   980   1727   2707   1857617
 3       0 BTree   3706387   4000      16 scan_at_region     95         90   73.39717 0.009398233  2050   3543   5593   3709930
 4       0 BTree   7417949   8000      16 scan_at_region    119         90  148.95930 0.045864656  4131   7505  11636   7425454
 5       0 BTree  14876686  16000      16 scan_at_region    138         90  306.47150 4.774481688  9012  16719  25731  14893405
 6       0 BTree  29764961  32000      16 scan_at_region    164         90  691.26310 3.454733997 18162  36281  54443  29801242
 7       0 BTree  59715461  64000      16 scan_at_region    215         90 1674.93300 1.434565749 41950  72424 114374  59787885
 8       0 BTree 119931295 128000      16 scan_at_region    280         90 3774.81400 0.300710270 80707 149232 229939 120080527
 9       1 BTree    929918   1000      16 scan_at_region     75         90   17.72378 0.065363272   304   1606   1910    931524
10       1 BTree   1866101   2000      16 scan_at_region     82         90   36.78344 0.013010013   439   2634   3073   1868735
# ... with 1,910 more rows
#+end_example


+Compute Speedups+ (issue with mismatched count)
#+begin_src R :results output :exports both :session :eval never
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree")) %>%
    filter(EltSize == 16) %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up_RTree  = RTree_avg_ms / GeoHashBinary_avg_ms ) -> dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up_RTree)) %>% arrange(ord)

#+end_src

Compute Speedups
#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree","BTree")) %>%
    filter(EltSize == 16) %>%
#Because the Count diverges for Rtree and PMQ , gathering with count 
    #select(queryId, algo, T, queryWidth, avg_ms, stdv, Count, FPOS, ALLPOS, TotScans ) %>% #print() %>%
    #gather(variable, value, Count, avg_ms, stdv) %>% #print() %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up_RTree  = RTree_avg_ms / GeoHashBinary_avg_ms, speed_up_BTree  = BTree_avg_ms / GeoHashBinary_avg_ms  ) %>% print() %>%
    inner_join( dfCR %>% 
                filter(algo == "GeoHashBinary", EltSize == 16) %>% 
                select(queryId, T, Count, TotScans)) ->  dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up_RTree)) %>% arrange(ord)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 13
   queryId      T queryWidth   FPOS ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms   RTree_stdv speed_up_RTree speed_up_BTree
     <int>  <int>      <dbl>  <dbl>  <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>        <dbl>          <dbl>          <dbl>
 1       0   1000         90    798   1243     17.57096 0.032625832             4.198085        0.009008462     21.00175   0.04536605       5.002698       4.185470
 2       0   2000         90   1727   2707     36.56094 0.020643277             8.254625        0.005502699     42.95545   0.03396309       5.203804       4.429146
 3       0   4000         90   3543   5593     73.39717 0.009398233            16.377700        0.013458991     88.08160   0.11862215       5.378142       4.481531
 4       0   8000         90   7505  11636    148.95930 0.045864656            32.672720        0.050353965    181.96160   0.12293106       5.569221       4.559134
 5       0  16000         90  16719  25731    306.47150 4.774481688            65.360360        0.110740249    377.70870   0.14220490       5.778865       4.688951
 6       0  32000         90  36281  54443    691.26310 3.454733997           130.588700        0.072374105    773.43210   0.14898132       5.922657       5.293437
 7       0  64000         90  72424 114374   1674.93300 1.434565749           262.437700        0.275556024   1578.40300   8.93511183       6.014391       6.382212
 8       0 128000         90 149232 229939   3774.81400 0.300710270           530.257100        0.155919103   3263.20400 128.02417273       6.154003       7.118837
 9       1   1000         90   1606   1910     17.72378 0.065363272             4.213655        0.003204241     21.07038   0.03316333       5.000500       4.206272
10       1   2000         90   2634   3073     36.78344 0.013010013             8.320638        0.008929386     43.12540   0.03046503       5.182944       4.420748
# ... with 630 more rows
Joining, by = c("queryId", "T")
# A tibble: 640 x 16
   queryId      T queryWidth    FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv speed_up_RTree speed_up_BTree    Count TotScans   ord
     <int>  <int>      <dbl>   <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>       <dbl>          <dbl>          <dbl>    <int>    <dbl> <int>
 1      34   1000  11.250000   16280   21813    2.8952230 0.33305870            0.8615798        0.009300642    4.9551050 0.009395758       5.751185       3.360365   165612   181892     1
 2      41  64000   5.625000    7216   16426   55.9229700 0.16999272           10.1798600        0.007992246   68.3978100 0.092908443       6.718934       5.493491  2287819  2295035     2
 3      47  32000   5.625000   34366  125331   13.3127000 0.08582571            4.5306840        0.010303397   17.1525800 0.006907934       3.785870       2.938342   547038   581404     3
 4      53  16000   2.812500   38446  102456   14.2082200 0.02378103            3.5537020        0.006424662   18.9419500 0.027733664       5.330202       3.998146   636045   674491     4
 5      60  64000   1.406250    7424   10785    0.3689068 0.03067647            0.1760892        0.013167764    0.3240643 0.001859956       1.840342       2.095000    26839    34263     5
 6      65  32000   1.406250    3375    4784    0.5459984 0.11431863            0.2149952        0.021671385    0.6719342 0.011436340       3.125345       2.539584    46906    50281     6
 7      69  32000   1.406250  101512  165672    7.7877380 0.14131783            2.1977730        0.032129896    6.1399340 0.021064367       2.793707       3.543468   232608   334120     7
 8      73 128000   0.703125 2163940 4120520  145.8899000 0.10745588           40.1004100        0.013920445   79.0795000 0.152388531       1.972037       3.638115  2748860  4912800     8
 9      33 128000  11.250000  351129  955699 1040.6140000 1.26497431          174.7358000        0.046972332 1073.3830000 0.455413121       6.142891       5.955357 38228535 38579664     9
10      40  16000   5.625000    7908   15205   11.7886100 0.05001465            2.5880440        0.006300115   20.7199900 0.032249701       8.006042       4.555027   567574   575482    10
# ... with 630 more rows
#+end_example



*** DONE Analysis                                                  :export:

#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(EltSize == 16) %>%
    mutate(Ratio = FPOS / TotScans) %>%
    mutate(Ratio2 = FPOS / ALLPOS) %>%
    mutate(tgp = Count / avg_ms) %>%
    #filter(queryWidth < 1) %>%
    filter(algo == "BTree") %>%
    filter(Count != TPOS) %>%
    arrange(Count)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 17
   queryId  algo Count     T EltSize          bench Refine queryWidth    avg_ms         stdv  TPOS  FPOS ALLPOS TotScans      Ratio     Ratio2       tgp
     <int> <chr> <int> <int>   <dbl>          <chr>  <int>      <dbl>     <dbl>        <dbl> <dbl> <dbl>  <dbl>    <dbl>      <dbl>      <dbl>     <dbl>
 1      76 BTree   287  1000      16 scan_at_region     12   0.703125 0.0138993 0.0009454926   130  1166   1296     1453 0.80247763 0.89969136 20648.522
 2      76 BTree   466  2000      16 scan_at_region     12   0.703125 0.0277051 0.0008233037   255  2455   2710     2921 0.84046559 0.90590406 16820.008
 3      60 BTree   484  1000      16 scan_at_region     16   1.406250 0.0071375 0.0002993594    42   149    191      633 0.23538705 0.78010471 67810.858
 4      76 BTree   777  4000      16 scan_at_region     12   0.703125 0.0759321 0.0030948708   479  5182   5661     5959 0.86960899 0.91538597 10232.826
 5      60 BTree   862  2000      16 scan_at_region     19   1.406250 0.0121564 0.0006025904    73   288    361     1150 0.25043478 0.79778393 70909.151
 6      65 BTree  1346  1000      16 scan_at_region     15   1.406250 0.0191106 0.0032255473    32   155    187     1501 0.10326449 0.82887701 70432.116
 7      76 BTree  1677  8000      16 scan_at_region     12   0.703125 0.1711801 0.0087036180  1015 10175  11190    11852 0.85850489 0.90929401  9796.699
 8      71 BTree  1782  1000      16 scan_at_region     12   0.703125 0.0224960 0.0002520145  1306   136   1442     1918 0.07090719 0.09431345 79214.083
 9      60 BTree  1803  4000      16 scan_at_region     22   1.406250 0.0234335 0.0012386426   217   499    716     2302 0.21676803 0.69692737 76941.131
10      51 BTree  2377  1000      16 scan_at_region     25   2.812500 0.0689187 0.0010792281   387  3982   4369     6359 0.62619909 0.91142138 34489.913
# ... with 630 more rows
#+end_example

**** Plot : ratios
#+RESULTS:
#+begin_example
# A tibble: 493 x 17
   queryId  algo Count     T EltSize          bench Refine queryWidth    avg_ms         stdv  TPOS  FPOS ALLPOS TotScans        Ratio    Ratio2       tgp
     <int> <chr> <int> <int>   <dbl>          <chr>  <int>      <dbl>     <dbl>        <dbl> <dbl> <dbl>  <dbl>    <dbl>        <dbl>     <dbl>     <dbl>
 1      51 BTree  2377  1000      16 scan_at_region     12     2.8125 0.0935207 0.0050950426   800  5221   6021     7598 0.6871545143 0.8671317 25416.833
 2      51 BTree  4537  2000      16 scan_at_region     12     2.8125 0.2101530 0.0128721513  1503 10271  11774    14808 0.6936115613 0.8723458 21589.033
 3      54 BTree  6164  1000      16 scan_at_region     12     2.8125 0.4925632 0.0012249881  4031 39312  43343    45476 0.8644559768 0.9069977 12514.130
 4      15 BTree  8096  1000      16 scan_at_region      6    45.0000 0.1224318 0.0069989957     0     0      0     8096 0.0000000000       NaN 66126.611
 5      51 BTree  9398  4000      16 scan_at_region     12     2.8125 0.4072375 0.0935481622  3507 21023  24530    30421 0.6910686697 0.8570322 23077.443
 6      42 BTree 11446  1000      16 scan_at_region     23     5.6250 0.2468979 0.0145591697   525  7715   8240    19161 0.4026407808 0.9362864 46359.244
 7      45 BTree 11619  1000      16 scan_at_region     23     5.6250 0.1860315 0.0009233704   698  7542   8240    19161 0.3936120244 0.9152913 62457.165
 8      54 BTree 12970  2000      16 scan_at_region     12     2.8125 1.4501400 0.0451758183  8517 82252  90769    95222 0.8637919808 0.9061684  8943.964
 9      15 BTree 13292  2000      16 scan_at_region      9    45.0000 0.2019838 0.0146983636     0     2      2    13294 0.0001504438 1.0000000 65807.258
10      44 BTree 16350  1000      16 scan_at_region     24     5.6250 0.2060321 0.0008137880  4390  3232   7622    19582 0.1650495353 0.4240357 79356.566
# ... with 483 more rows
#+end_example

#+begin_src R :results output :exports both :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
#w = levelsWidth[[1]]
levelsWidth

#+end_src

#+RESULTS:
: [1] "0.703125" "1.40625"  "2.8125"   "5.625"    "11.25"    "22.5"     "45"      
: [8] "90"

#+begin_src R :results output :exports both :session 
plotRatios <- function(dfCR,w){
    dfCR %>% 
    filter(EltSize == 16) %>%
    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(Count), y = Count / avg_ms, fill=(FPOS / TotScans)) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position="dodge",color="black", size =0.2) +
    facet_wrap(~algo, scale="free", ncol = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
   labs(title = paste("Query width = ", w))
}
#+end_src

#+RESULTS:

Facet free 
#+begin_src R :results output graphics :file "./img/tgp-false-positves2-10.pdf" :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w)
    print(p)
}
#+end_src

#+RESULTS:
[[file:./img/tgp-false-positves2-10.pdf]]

Facet free_x
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w) + 
    facet_wrap(~algo, scale="free_x", ncol = 1)
    print(p)
}

#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038Kri.pdf]]

**** Speed-Ups of Throughputs                                     :export:

#+begin_src R :results output :exports both :session 
dfCR
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,920 x 14
   queryId  algo     Count      T EltSize          bench Refine queryWidth     avg_ms        stdv  TPOS   FPOS ALLPOS  TotScans
     <int> <chr>     <int>  <int>   <dbl>          <chr>  <int>      <dbl>      <dbl>       <dbl> <dbl>  <dbl>  <dbl>     <dbl>
 1       0 BTree    924827   1000      16 scan_at_region     69         90   17.57096 0.032625832   445    798   1243    925625
 2       0 BTree   1855890   2000      16 scan_at_region     78         90   36.56094 0.020643277   980   1727   2707   1857617
 3       0 BTree   3706387   4000      16 scan_at_region     95         90   73.39717 0.009398233  2050   3543   5593   3709930
 4       0 BTree   7417949   8000      16 scan_at_region    119         90  148.95930 0.045864656  4131   7505  11636   7425454
 5       0 BTree  14876686  16000      16 scan_at_region    138         90  306.47150 4.774481688  9012  16719  25731  14893405
 6       0 BTree  29764961  32000      16 scan_at_region    164         90  691.26310 3.454733997 18162  36281  54443  29801242
 7       0 BTree  59715461  64000      16 scan_at_region    215         90 1674.93300 1.434565749 41950  72424 114374  59787885
 8       0 BTree 119931295 128000      16 scan_at_region    280         90 3774.81400 0.300710270 80707 149232 229939 120080527
 9       1 BTree    929918   1000      16 scan_at_region     75         90   17.72378 0.065363272   304   1606   1910    931524
10       1 BTree   1866101   2000      16 scan_at_region     82         90   36.78344 0.013010013   439   2634   3073   1868735
# ... with 1,910 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree","BTree")) %>%
    filter(EltSize == 16) %>%
                                        #    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(queryId%%10), y = Count / avg_ms, fill=(FPOS / TotScans), group=T) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position=position_dodge(),color="black", size =0.2) +
    geom_errorbar(aes(ymin = Count / (avg_ms-stdv), ymax = Count / (avg_ms + stdv)), position=position_dodge())+ 
    facet_grid(queryWidth~algo, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
                                        #  labs(title = paste("Query width = ", w))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038X1o.pdf]]


**** Speedup - correlations                                       :export:
:PROPERTIES:
:HEADER-ARGS:R: :exports result
:END:


- Order the results by speedup

- try several mapping to a continuos color map

Count
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(Count)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038k_u.pdf]]

QueryWidth
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=factor(queryWidth)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038xJ1.pdf]]

Ratio : FPOS / ALLPOS
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / ALLPOS)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038jTE.pdf]]

Ratio : FPOS / TotScans
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038wdK.pdf]]


The color scale show that the speed-up increases according to ratio of False positives.

Mapping to divergent scale:
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) +scale_fill_distiller(limit=c(0,1),type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure20389nQ.pdf]]


The divergent scale show that GeoHash is better that the RTree index when the amount of false positives is lower 50% in general.

But when do we have small FPOS Ratio ? 

By grouping queries by queryWidth we can see that FPositive ratio is only a problem on small queries. 
Nevertheless, even on the smallest queries we are able to perform better that the RTree on more than 50% of the cases.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
dfSpeedUp %>% group_by(queryWidth) %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    #geom_bar(aes(fill=(FPOS / TotScans)), color="black", stat="identity",width=1) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity") +
    scale_fill_distiller(limit=c(0,1),type="div") +
    facet_wrap(~queryWidth)+ 
    geom_hline(yintercept = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) -> plt
plt
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038KyW.pdf]]

Compute the percentage where PMQ is better than RTree
#+begin_src R :results output  :session 
dfSpeedUp %>% 
    group_by(queryWidth) %>% 
    summarize(tot = length(speed_up_RTree), PmqBest = length(speed_up_RTree[speed_up_RTree>1]), `%` = PmqBest / tot * 100) ->
dfPct 

dfPct
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 8 x 4
  queryWidth   tot PmqBest    `%`
       <dbl> <int>   <int>  <dbl>
1   0.703125    80      69  86.25
2   1.406250    80      80 100.00
3   2.812500    80      72  90.00
4   5.625000    80      80 100.00
5  11.250000    80      80 100.00
6  22.500000    80      80 100.00
7  45.000000    80      80 100.00
8  90.000000    80      80 100.00
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
plt +
    geom_vline(data=dfPct, aes(xintercept = 80-PmqBest)) +  
    geom_text(data=dfPct, aes(x=80-PmqBest + 6, y=2, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038X8c.pdf]]


**** Speedup comparison with BTree and RTree                      :export:
:PROPERTIES:
:EXPORT_FILE_NAME: speedup_comparison_BTree_RTree
:HEADER-ARGS:R: :exports results
:END:

These results show the query's throughput speed-up for PMQ over BTree and RTree.

#+begin_src R :results output :exports none :session 
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 15
   queryId      T queryWidth   FPOS ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms   RTree_stdv     Count  TotScans   algo_speedup val_speedup
     <int>  <int>      <dbl>  <dbl>  <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>        <dbl>     <int>     <dbl>          <chr>       <dbl>
 1       0   1000         90    798   1243     17.57096 0.032625832             4.198085        0.009008462     21.00175   0.04536605    924827    925625 speed_up_RTree    5.002698
 2       0   2000         90   1727   2707     36.56094 0.020643277             8.254625        0.005502699     42.95545   0.03396309   1855890   1857617 speed_up_RTree    5.203804
 3       0   4000         90   3543   5593     73.39717 0.009398233            16.377700        0.013458991     88.08160   0.11862215   3706387   3709930 speed_up_RTree    5.378142
 4       0   8000         90   7505  11636    148.95930 0.045864656            32.672720        0.050353965    181.96160   0.12293106   7417949   7425454 speed_up_RTree    5.569221
 5       0  16000         90  16719  25731    306.47150 4.774481688            65.360360        0.110740249    377.70870   0.14220490  14876686  14893405 speed_up_RTree    5.778865
 6       0  32000         90  36281  54443    691.26310 3.454733997           130.588700        0.072374105    773.43210   0.14898132  29764961  29801242 speed_up_RTree    5.922657
 7       0  64000         90  72424 114374   1674.93300 1.434565749           262.437700        0.275556024   1578.40300   8.93511183  59715461  59787885 speed_up_RTree    6.014391
 8       0 128000         90 149232 229939   3774.81400 0.300710270           530.257100        0.155919103   3263.20400 128.02417273 119931295 120080527 speed_up_RTree    6.154003
 9       1   1000         90   1606   1910     17.72378 0.065363272             4.213655        0.003204241     21.07038   0.03316333    929918    931524 speed_up_RTree    5.000500
10       1   2000         90   2634   3073     36.78344 0.013010013             8.320638        0.008929386     43.12540   0.03046503   1866101   1868735 speed_up_RTree    5.182944
# ... with 1,270 more rows
#+end_example

All the results were sorted by increasing speedUp value. 
The colors show the ratio between amount of false positive elements over the total amount of elements scanned by the query. 
#+begin_src R :results output graphics :file "./img/speedup_BTREE_RTREE_ref10.svg"  :width 14 :height 10 :session 
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree) %>%
    arrange( val_speedup) %>% group_by(algo_speedup) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_hline(yintercept = 1) + 
    facet_grid(~algo_speedup) +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_BTREE_RTREE_ref10.svg]]

#+CAPTION: Compute the percentages 
#+begin_src R :results output :session :exports none
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree) %>%
    arrange( val_speedup) %>% 
    group_by(algo_speedup,queryWidth) %>% 
    mutate(ord = row_number()) -> dfSpeedUp2

dfSpeedUp2

dfSpeedUp2 %>% 
    summarize(tot = length(val_speedup), PmqBest = length( val_speedup[val_speedup>1]), `%` = PmqBest / tot * 100) ->
dfPct2 

dfPct2
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 16
# Groups:   algo_speedup, queryWidth [16]
   queryId      T queryWidth   FPOS ALLPOS BTree_avg_ms   BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv Count TotScans   algo_speedup val_speedup   ord
     <int>  <int>      <dbl>  <dbl>  <dbl>        <dbl>        <dbl>                <dbl>              <dbl>        <dbl>       <dbl> <int>    <dbl>          <chr>       <dbl> <int>
 1      76 128000   0.703125 171485 192084    4.7404690 0.1530567009            1.6319720       0.0069445467    0.4130925 0.001403953 30670   202155 speed_up_RTree   0.2531247     1
 2      76   8000   0.703125  10175  11190    0.1711801 0.0087036180            0.0900663       0.0018738275    0.0253403 0.002840178  1677    11852 speed_up_RTree   0.2813516     2
 3      76  16000   0.703125  18530  20726    0.3400201 0.0469819861            0.1650061       0.0028614154    0.0500156 0.003817182  3248    21778 speed_up_RTree   0.3031136     3
 4      76   4000   0.703125   5182   5661    0.0759321 0.0030948708            0.0459063       0.0005714718    0.0141471 0.002139187   777     5959 speed_up_RTree   0.3081734     4
 5      76  64000   0.703125  76885  87101    1.9148250 0.0965446851            0.6544457       0.0015265332    0.2114444 0.001966248 14677    91562 speed_up_RTree   0.3230893     5
 6      76   2000   0.703125   2455   2710    0.0277051 0.0008233037            0.0235509       0.0003705055    0.0087181 0.001285961   466     2921 speed_up_RTree   0.3701812     6
 7      76  32000   0.703125  37464  42342    0.5320830 0.0013620595            0.3226035       0.0008273446    0.1264704 0.008279208  7209    44673 speed_up_RTree   0.3920305     7
 8      76   1000   0.703125   1166   1296    0.0138993 0.0009454926            0.0126594       0.0003219521    0.0058668 0.001385825   287     1453 speed_up_RTree   0.4634343     8
 9      51   1000   2.812500   3982   4369    0.0689187 0.0010792281            0.0428774       0.0006956230    0.0300493 0.002875075  2377     6359 speed_up_RTree   0.7008191     1
10      51   2000   2.812500   7797   8544    0.1719340 0.0109519274            0.0820946       0.0016602163    0.0607502 0.004103779  4537    12334 speed_up_RTree   0.7400024     2
# ... with 1,270 more rows
# A tibble: 16 x 5
# Groups:   algo_speedup [?]
     algo_speedup queryWidth   tot PmqBest    `%`
            <chr>      <dbl> <int>   <int>  <dbl>
 1 speed_up_BTree   0.703125    80      80 100.00
 2 speed_up_BTree   1.406250    80      80 100.00
 3 speed_up_BTree   2.812500    80      80 100.00
 4 speed_up_BTree   5.625000    80      80 100.00
 5 speed_up_BTree  11.250000    80      80 100.00
 6 speed_up_BTree  22.500000    80      80 100.00
 7 speed_up_BTree  45.000000    80      80 100.00
 8 speed_up_BTree  90.000000    80      80 100.00
 9 speed_up_RTree   0.703125    80      69  86.25
10 speed_up_RTree   1.406250    80      80 100.00
11 speed_up_RTree   2.812500    80      72  90.00
12 speed_up_RTree   5.625000    80      80 100.00
13 speed_up_RTree  11.250000    80      80 100.00
14 speed_up_RTree  22.500000    80      80 100.00
15 speed_up_RTree  45.000000    80      80 100.00
16 speed_up_RTree  90.000000    80      80 100.00
#+end_example

We can see a correlation between the amount false positives and the Speedup over *RTrees* on small queries. 
This is due to the discontinuities caused by the Z-ordering scheme, used in the GeoHash algorithm. 
We can see that this correlation doesn't appear in the *BTree* speedups.
#+begin_src R :results output graphics :file "./img/speedup_BTREE_RTREE_facet_ref10.svg"  :width 14 :height 10 :session 

dfSpeedUp2 %>%
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(limits=c(0,1),type="div") +
    facet_grid(queryWidth~algo_speedup) +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))

#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_BTREE_RTREE_facet_ref10.svg]]


We normalized the element count within each querySize to check it the BTree's Speedup increases with the number os elements queried.
# +However, there is not a clear correlation to explain increase of speedup for the BTree case.+
#+begin_src R :results output :exports none :session 
dfSpeedUp2 %>% mutate(max = max(Count)) %>%
    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count)))  %>% arrange(algo_speedup,queryWidth,-ord)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 18
# Groups:   algo_speedup, queryWidth [16]
   queryId      T queryWidth    FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv   Count TotScans   algo_speedup val_speedup   ord     max normCount
     <int>  <int>      <dbl>   <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>       <dbl>   <int>    <dbl>          <chr>       <dbl> <int>   <dbl>     <dbl>
 1      75 128000   0.703125  499836 1035400     89.56010 0.19456395            19.547030        0.022145481     76.66449  0.10394617 2833578  3333419 speed_up_BTree    4.581775    80 3682101 0.7695367
 2      70 128000   0.703125 1014620 1792980    130.32050 0.17029337            28.723420        0.018174695    104.75755 10.85447705 3682101  4696730 speed_up_BTree    4.537082    79 3682101 1.0000000
 3      75  64000   0.703125  254677  530921     42.44549 0.26569103             9.989538        0.006512350     39.69537  0.06579260 1455569  1710246 speed_up_BTree    4.248994    78 3682101 0.3952622
 4      70  64000   0.703125  518587  896202     60.92869 0.19935289            14.526930        0.022523620     49.83485  0.05642466 1864494  2383081 speed_up_BTree    4.194189    77 3682101 0.5063284
 5      78 128000   0.703125  118596  411686     26.14609 0.06857428             6.526117        0.012623471     23.61555  0.02694658  868311   986907 speed_up_BTree    4.006378    76 3682101 0.2357599
 6      75  32000   0.703125  125883  260128     19.87228 0.09188685             4.990425        0.009256544     20.29562  0.03942097  734506   860389 speed_up_BTree    3.982082    75 3682101 0.1994177
 7      70  32000   0.703125  253445  444055     28.29783 0.13054008             7.298191        0.007861139     25.92945  0.01847516  955205  1208650 speed_up_BTree    3.877376    74 3682101 0.2593607
 8      72 128000   0.703125   81723  457479     25.98778 0.04825299             6.798633        0.005155347     23.33216  0.02491065  890901   972624 speed_up_BTree    3.822501    73 3682101 0.2418954
 9      78  64000   0.703125   58586  207805     12.49333 0.11335547             3.287206        0.004356041     12.60251  0.01032015  443642   502228 speed_up_BTree    3.800592    72 3682101 0.1204175
10      75  16000   0.703125   65168  135925      9.46002 0.04787214             2.541230        0.006074275     12.01168  0.01118181  370401   435569 speed_up_BTree    3.722615    71 3682101 0.1005249
# ... with 1,270 more rows
#+end_example

The color scale is normalized between the min and max count of the elements count in each facet.
Note that Count increases as power of 2 . Therefore we take the logarithm to map it to linear color scale. (see Extra sections on org file)
#+begin_src R :results output graphics :file "./img/speedup_color_count_ref10.svg" :exports both :width 14 :height 10 :session 
dfSpeedUp2 %>%
    mutate(Count = log2(Count)) %>%
    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count))) %>%
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=normCount), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(type="seq") +
    facet_grid(queryWidth~algo_speedup) +
    labs(fill="Normalized log2(Count)") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_color_count_ref10.svg]]

***** EXTRA: verify if relation between SpeedUp and Ratio is linear. :noexport:

To verify that SpeedUp linearly decreases with SpeedUp we plot orderded by ratio.
Although for RTree is not a clear straight line , there is a neat inverse relation between SpeedUp and False Positive ratio. 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session 
dfSpeedUp2 %>% filter(queryWidth < 2) %>%
    ggplot(aes(x = FPOS/TotScans, y = val_speedup) ) +
    geom_line() + geom_point() + 
    facet_wrap(queryWidth~algo_speedup,scale="free",ncol=2) +
    theme(legend.position = "bottom",
#          strip.text = element_blank() , 
          axis.text.x = element_text(angle = 0, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038kGj.png]]

***** EXTRA: Logarithmic interpolation of colors ?             :noexport:

Note that the the dataset doubles the size at each experiments. 
The counts should double in the same way.

So to have a linear X scale we must take the log of Count on x axis.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".svg") :exports both :width 14 :height 10 :session 
dfSpeedUp2 %>%
#    mutate(Count = log2(Count)) %>%
#    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count))) %>%
    ggplot(aes(x = log2(Count), y = val_speedup) ) +
#    geom_bar(aes(fill=normCount), stat="identity",position="dodge",width=1) + 
    geom_line() + geom_point() + 
#    scale_fill_distiller(type="seq") +
    facet_wrap(queryWidth~algo_speedup,scale="free_y",ncol=2) +
    theme(legend.position = "bottom",
          strip.text = element_blank() , 
          axis.text.x = element_text(angle = 0, hjust = 1)) 
#    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
#    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
#    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-2038Urr/figure2038xQp.svg]]


#+begin_src R :results output :exports both :session 
s = 2**seq(1,10)
si = (s - min(s)) / (max(s) - min(s))
s
si

log2(s)
#+end_src

#+RESULTS:
:  [1]    2    4    8   16   32   64  128  256  512 1024
:  [1] 0.000000000 0.001956947 0.005870841 0.013698630 0.029354207 0.060665362
:  [7] 0.123287671 0.248532290 0.499021526 1.000000000
:  [1]  1  2  3  4  5  6  7  8  9 10


* Final notes 
- The element size doesn't seems to affect the throughput  (in elements / ms). 
  
- PMQ vs BTREE : we always win 

- Bad queries for PMQ VS RTREE
  - queryId = 76 , 78 , 68 , 69

    



* [2017-11-15 qua] speedUp2

#+begin_src R :results output :exports both :session
dfSpeedUp2 %>% filter(T == 1000, val_speedup < 1 ) %>% print( n=10)

#dfSpeedUp2 %>% filter(val_speedup >= 1 ) %>% tail( 15)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 18 x 16
# Groups:   algo_speedup, queryWidth [3]
   queryId     T queryWidth  FPOS ALLPOS BTree_avg_ms   BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms   RTree_stdv Count TotScans   algo_speedup val_speedup   ord
     <int> <int>      <dbl> <dbl>  <dbl>        <dbl>        <dbl>                <dbl>              <dbl>        <dbl>        <dbl> <int>    <dbl>          <chr>       <dbl> <int>
 1      54  1000   2.812500 39312  43343    0.4925632 0.0012249881            0.3213411       0.0007721667    0.0989732 0.0018424037  6164    45476 speed_up_RTree   0.3080004     4
 2      74  1000   0.703125 24525  29640    0.3299340 0.0003758534            0.2159905       0.0022841853    0.0758642 0.0014741572  5115    29640 speed_up_RTree   0.3512386    11
 3      76  1000   0.703125  1520   1807    0.0164722 0.0004290982            0.0152044       0.0002708022    0.0057027 0.0011740942   287     1807 speed_up_RTree   0.3750691    14
 4      71  1000   0.703125  3538   5320    0.0618625 0.0002080087            0.0418915       0.0002031678    0.0178042 0.0011940597  1782     5320 speed_up_RTree   0.4250075    15
 5      68  1000   1.406250 21784  29810    0.3479530 0.0012115966            0.2267741       0.0009070750    0.0982087 0.0006997804  8026    29810 speed_up_RTree   0.4330684     5
 6      60  1000   1.406250   938   1422    0.0145924 0.0005184081            0.0121613       0.0001403726    0.0053589 0.0007006748   484     1422 speed_up_RTree   0.4406519     7
 7      78  1000   0.703125 20446  27658    0.3179734 0.0013460463            0.2062383       0.0009772532    0.0922635 0.0008458373  7212    27658 speed_up_RTree   0.4473636    17
 8      51  1000   2.812500  5221   6021    0.0935207 0.0050950426            0.0524640       0.0005935597    0.0284795 0.0027218424  2377     7598 speed_up_RTree   0.5428389     9
 9      73  1000   0.703125 26066  48127    0.6870644 0.1474425700            0.4029896       0.0008922860    0.2736919 0.0075400833 22061    48127 speed_up_RTree   0.6791538    28
10      70  1000   0.703125 21863  51727    0.6713312 0.0017039426            0.4548048       0.0008036825    0.3308333 0.0019055984 29864    51727 speed_up_RTree   0.7274182    31
# ... with 8 more rows
#+end_example


#+begin_src R :results output :exports both :session
dfSpeedUp2 %>% filter(T == 1000) %>% tail(n=15) 


#+end_src

#+RESULTS:
#+begin_example
# A tibble: 15 x 16
# Groups:   algo_speedup, queryWidth [4]
   queryId     T queryWidth  FPOS ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv  Count TotScans   algo_speedup val_speedup   ord
     <int> <int>      <dbl> <dbl>  <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>       <dbl>  <int>    <dbl>          <chr>       <dbl> <int>
 1      14  1000     45.000  9279  23987   15.0434500 0.011691711            3.9640170       0.0038129547   18.6350500 0.029051535 796946   806225 speed_up_RTree    4.701052     7
 2      41  1000      5.625   260   1765    0.3496083 0.005823749            0.1359166       0.0001675246    0.6489826 0.001530290  37386    37646 speed_up_RTree    4.774859    67
 3      19  1000     45.000  3403   4016   15.5577700 0.011309588            4.0077290       0.0037639768   19.3070500 0.015477529 835620   839023 speed_up_RTree    4.817454     8
 4      16  1000     45.000  3080   4016   15.5756000 0.016501044            4.0063450       0.0100601052   19.3124800 0.034471882 835943   839023 speed_up_RTree    4.820474     9
 5      13  1000     45.000    68   2288   15.2324200 0.024421657            3.9162350       0.0019209156   18.9495100 0.025645379 821227   821295 speed_up_RTree    4.838706    10
 6      17  1000     45.000 23712  27123   16.0491400 0.009717247            3.9850390       0.0030494898   19.3569600 0.034838682 838210   861922 speed_up_RTree    4.857408    11
 7      10  1000     45.000  7933   9623   15.9890300 0.014782125            3.9391150       0.0097016473   19.5438200 0.021561581 851874   859807 speed_up_RTree    4.961475    13
 8      11  1000     45.000   165    874   15.8060200 0.018444560            3.8682410       0.0037694192   19.5458800 0.027376543 851647   851812 speed_up_RTree    5.052912    18
 9      33  1000     11.250 34909  43140    6.0973330 0.033950910            1.6352490       0.0079877857    8.2961240 0.026943386 309302   344211 speed_up_RTree    5.073309    68
10      20  1000     22.500  9031   9430    0.7992440 0.003246054            0.3487180       0.0366828181    1.8470110 0.068899197  69663    78694 speed_up_RTree    5.296575    63
11      29  1000     22.500  5904  22380    3.3719200 0.342602234            0.9774320       0.0123667078    5.2841840 0.007135439 199164   205068 speed_up_RTree    5.406191    66
12      12  1000     45.000 16227  19774    5.3540460 0.018358867            1.4566620       0.0394781768    8.0275680 0.009018849 290684   306911 speed_up_RTree    5.510934    34
13      39  1000     11.250   249   2035    1.2561120 0.166223081            0.4060387       0.0078307376    2.5029000 0.060598643 108123   108372 speed_up_RTree    6.164191    80
14      21  1000     22.500  3127   4697    2.0267420 0.341305426            0.7018848       0.0039716798    4.3753410 0.015860482 142932   146059 speed_up_RTree    6.233702    74
15      23  1000     22.500  8338  11047    1.8587020 0.037829720            0.6596478       0.0046452659    4.2397390 0.016492991 131460   139798 speed_up_RTree    6.427277    77
#+end_example

* [2017-11-17 sex] New Results with Reflevel == 10 :ARCHIVE:

** DONE Analysis
:PROPERTIES:
:END:
*** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS:
| log_1508177471.tgz |
| log_1510826640.tgz |


#+NAME: logFile2
#+begin_src sh :results output :exports both :var f="log_1510826640.tgz"
for i in $f; do 
    tar xvzf $i
done
#+end_src

#+RESULTS: logFile2
: bench_queries_region_twitter_1000_1000_10_0_1510826640.log
: bench_queries_region_twitter_128000_1000_10_0_1510826640.log
: bench_queries_region_twitter_16000_1000_10_0_1510826640.log
: bench_queries_region_twitter_2000_1000_10_0_1510826640.log
: bench_queries_region_twitter_32000_1000_10_0_1510826640.log
: bench_queries_region_twitter_4000_1000_10_0_1510826640.log
: bench_queries_region_twitter_64000_1000_10_0_1510826640.log
: bench_queries_region_twitter_8000_1000_10_0_1510826640.log


Create CSV using logFile 
#+NAME: csvFile3
#+begin_src sh :results output :exports both :var logFileList=logFile2
for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+RESULTS: csvFile3
: bench_queries_region_twitter_1000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_128000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_16000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_2000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_32000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_4000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_64000_1000_10_0_1510826640.csv
: bench_queries_region_twitter_8000_1000_10_0_1510826640.csv


Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

*** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

#+LATEX_HEADER:  \usepackage[a4paper,includeheadfoot,margin=2cm]{geometry}
 
**** Prepare

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile3 path=(print default-directory)
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep=""), col_types="ccicicdcici" ) %>%
         mutate (
            EltSize = as.factor(
                 gsub("bench_queries_region_twitter_[[:digit:]]+_[[:digit:]]+_[[:digit:]]+_([[:digit:]]+)_[[:digit:]]+.csv","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
[1] "bench_queries_region_twitter_1000_1000_10_0_1510826640.csv"  
[2] "bench_queries_region_twitter_128000_1000_10_0_1510826640.csv"
[3] "bench_queries_region_twitter_16000_1000_10_0_1510826640.csv" 
[4] "bench_queries_region_twitter_2000_1000_10_0_1510826640.csv"  
[5] "bench_queries_region_twitter_32000_1000_10_0_1510826640.csv" 
[6] "bench_queries_region_twitter_4000_1000_10_0_1510826640.csv"  
[7] "bench_queries_region_twitter_64000_1000_10_0_1510826640.csv" 
[8] "bench_queries_region_twitter_8000_1000_10_0_1510826640.csv"
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_0_1510826640.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 4800 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_0_1510826640.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning messages:
1: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
2: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
3: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
4: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
5: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
6: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
7: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
8: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
#+end_example


Remove useless columns
#+begin_src R :results output :exports both 
names(df) <- c("algo" , "V2" , "queryId", "V4", "T", "bench" , "ms" , "V8", "Refine","V10","Count","EltSize")

df <- select(df, -V2, -V4, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 38,400 x 8
            algo queryId     T           bench       ms Refine  Count EltSize
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>  <fctr>
 1 GeoHashBinary       0  1000 apply_at_region 0.368360     69 924827       0
 2 GeoHashBinary       0  1000 apply_at_region 0.360399     69 924827       0
 3 GeoHashBinary       0  1000 apply_at_region 0.358103     69 924827       0
 4 GeoHashBinary       0  1000 apply_at_region 0.357762     69 924827       0
 5 GeoHashBinary       0  1000 apply_at_region 0.357186     69 924827       0
 6 GeoHashBinary       0  1000 apply_at_region 0.357522     69 924827       0
 7 GeoHashBinary       0  1000 apply_at_region 0.357021     69 924827       0
 8 GeoHashBinary       0  1000 apply_at_region 0.357517     69 924827       0
 9 GeoHashBinary       0  1000 apply_at_region 0.357360     69 924827       0
10 GeoHashBinary       0  1000 apply_at_region 0.356737     69 924827       0
# ... with 38,390 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both
df <- 
    df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) %>%
    mutate(queryWidth = 90 / 2**(queryId %/% 10))  %>%   # comput info about query width
    mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> df # adjust the actual size of the elements
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :exports both
options(tibble.width = Inf)
df 

dfAvg <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfAvg
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 38,400 x 9
            algo queryId     T           bench       ms Refine  Count EltSize queryWidth
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>   <dbl>      <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 0.368360     69 924827      16         90
 2 GeoHashBinary       0  1000 apply_at_region 0.360399     69 924827      16         90
 3 GeoHashBinary       0  1000 apply_at_region 0.358103     69 924827      16         90
 4 GeoHashBinary       0  1000 apply_at_region 0.357762     69 924827      16         90
 5 GeoHashBinary       0  1000 apply_at_region 0.357186     69 924827      16         90
 6 GeoHashBinary       0  1000 apply_at_region 0.357522     69 924827      16         90
 7 GeoHashBinary       0  1000 apply_at_region 0.357021     69 924827      16         90
 8 GeoHashBinary       0  1000 apply_at_region 0.357517     69 924827      16         90
 9 GeoHashBinary       0  1000 apply_at_region 0.357360     69 924827      16         90
10 GeoHashBinary       0  1000 apply_at_region 0.356737     69 924827      16         90
# ... with 38,390 more rows
# A tibble: 3,840 x 10
# Groups:   algo, queryId, T, bench, Refine, Count, EltSize [?]
    algo queryId     T           bench Refine    Count EltSize queryWidth    avg_ms        stdv
   <chr>   <int> <int>           <chr>  <int>    <int>   <dbl>      <dbl>     <dbl>       <dbl>
 1 BTree       0  1000 apply_at_region     56   924827      16         90  10.79794 0.043603700
 2 BTree       0  1000  scan_at_region     69       NA      16         90  17.57096 0.032625832
 3 BTree       0  2000 apply_at_region     64  1855890      16         90  26.17268 0.925384913
 4 BTree       0  2000  scan_at_region     78       NA      16         90  36.56094 0.020643277
 5 BTree       0  4000 apply_at_region     77  3706387      16         90  55.67314 0.452575406
 6 BTree       0  4000  scan_at_region     95       NA      16         90  73.39717 0.009398233
 7 BTree       0  8000 apply_at_region     99  7417949      16         90 116.19720 0.128751958
 8 BTree       0  8000  scan_at_region    119       NA      16         90 148.95930 0.045864656
 9 BTree       0 16000 apply_at_region    119 14876686      16         90 235.58370 0.473857703
10 BTree       0 16000  scan_at_region    138       NA      16         90 306.47150 4.774481688
# ... with 3,830 more rows
#+end_example

Set the Count values on scan_at_region lines
#+begin_src R :results output :exports both
dfAvg %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,T,EltSize) %>%
    left_join( 
        filter(ungroup(dfAvg), bench == "scan_at_region") %>% select(-Count)
    ) -> dfCount
dfCount
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "algo", "T", "EltSize")
# A tibble: 1,920 x 10
   queryId  algo     Count      T EltSize          bench Refine queryWidth     avg_ms        stdv
     <int> <chr>     <int>  <int>   <dbl>          <chr>  <int>      <dbl>      <dbl>       <dbl>
 1       0 BTree    924827   1000      16 scan_at_region     69         90   17.57096 0.032625832
 2       0 BTree   1855890   2000      16 scan_at_region     78         90   36.56094 0.020643277
 3       0 BTree   3706387   4000      16 scan_at_region     95         90   73.39717 0.009398233
 4       0 BTree   7417949   8000      16 scan_at_region    119         90  148.95930 0.045864656
 5       0 BTree  14876686  16000      16 scan_at_region    138         90  306.47150 4.774481688
 6       0 BTree  29764961  32000      16 scan_at_region    164         90  691.26310 3.454733997
 7       0 BTree  59715461  64000      16 scan_at_region    215         90 1674.93300 1.434565749
 8       0 BTree 119931295 128000      16 scan_at_region    280         90 3774.81400 0.300710270
 9       1 BTree    929918   1000      16 scan_at_region     75         90   17.72378 0.065363272
10       1 BTree   1866101   2000      16 scan_at_region     82         90   36.78344 0.013010013
# ... with 1,910 more rows
#+end_example

#+begin_src R :results output :exports both 
dfAvg %>% filter(EltSize == 16) %>%
#filter(algo=="GeoHashBinary", bench == "scan_at_region")
filter(queryId == 54, bench == "scan_at_region") %>% print(n= 40)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 24 x 10
# Groups:   algo, queryId, T, bench, Refine, Count, EltSize [24]
            algo queryId      T          bench Refine Count EltSize queryWidth     avg_ms        stdv
           <chr>   <int>  <int>          <chr>  <int> <int>   <dbl>      <dbl>      <dbl>       <dbl>
 1         BTree      54   1000 scan_at_region     54    NA      16     2.8125  0.1722442 0.001359936
 2         BTree      54   2000 scan_at_region     54    NA      16     2.8125  0.3623205 0.001412936
 3         BTree      54   4000 scan_at_region     57    NA      16     2.8125  0.8564829 0.015671454
 4         BTree      54   8000 scan_at_region     57    NA      16     2.8125  2.4987550 0.338523891
 5         BTree      54  16000 scan_at_region     57    NA      16     2.8125  6.1519900 0.067033666
 6         BTree      54  32000 scan_at_region     57    NA      16     2.8125 13.4682500 0.081955981
 7         BTree      54  64000 scan_at_region     57    NA      16     2.8125 29.0319100 0.161425741
 8         BTree      54 128000 scan_at_region     57    NA      16     2.8125 63.6237300 0.223006198
 9 GeoHashBinary      54   1000 scan_at_region     54    NA      16     2.8125  0.1022972 0.001229695
10 GeoHashBinary      54   2000 scan_at_region     54    NA      16     2.8125  0.2077824 0.003245627
11 GeoHashBinary      54   4000 scan_at_region     57    NA      16     2.8125  0.4410869 0.016237251
12 GeoHashBinary      54   8000 scan_at_region     57    NA      16     2.8125  0.9292315 0.026309190
13 GeoHashBinary      54  16000 scan_at_region     57    NA      16     2.8125  2.0094240 0.006193988
14 GeoHashBinary      54  32000 scan_at_region     57    NA      16     2.8125  4.0019020 0.010871094
15 GeoHashBinary      54  64000 scan_at_region     57    NA      16     2.8125  8.1800160 0.009785588
16 GeoHashBinary      54 128000 scan_at_region     57    NA      16     2.8125 16.2683800 0.014675664
17         RTree      54   1000 scan_at_region     NA    NA      16     2.8125  0.0781826 0.001813145
18         RTree      54   2000 scan_at_region     NA    NA      16     2.8125  0.1915856 0.011464444
19         RTree      54   4000 scan_at_region     NA    NA      16     2.8125  0.3287351 0.001247785
20         RTree      54   8000 scan_at_region     NA    NA      16     2.8125  0.8917463 0.009426463
21         RTree      54  16000 scan_at_region     NA    NA      16     2.8125  2.8800780 0.133889354
22         RTree      54  32000 scan_at_region     NA    NA      16     2.8125  5.8271660 0.024404850
23         RTree      54  64000 scan_at_region     NA    NA      16     2.8125 12.6769400 0.014649930
24         RTree      54 128000 scan_at_region     NA    NA      16     2.8125 24.3338000 0.022087402
#+end_example

**** Plot: change of query count with size of T.                  :export:
#+begin_src R :results output graphics :file "./img/count_by_T.png" :exports both :width 600 :height 400 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region") %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId%%10))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryWidth, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:./img/count_by_T.png]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryId, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980T9Z.pdf]]

**** Plot: Scan Query Time by T facet by queryId                  :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
#    filter(EltSize == 0) %>%
    filter(bench == "scan_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free_y",ncol=10,labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980ExF.pdf]]

**** Plot: Scan query time by Query Count faceted by QueryId      :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980L42.pdf]]

**** Plot: Scan query throughput by Query Count faceted by QueryId :export:
:PROPERTIES: 
:HEADER-ARGS:R: :exports results
:END:      

#+begin_src R :results output :exports both :session 

tgpPlot <- function(dfCount){
    dfCount %>% 
        filter(queryWidth == w) %>%
        arrange(Count,T) %>%
        mutate(lbls = paste(Count," (",T/1000,")",sep="")) %>%
        ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +  
                                        #ggplot(aes(x = factor(Count), group=algo, y = Count / avg_ms, color = algo)) +  
        geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
        geom_line() +
        facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
        theme(legend.position = "bottom",
              axis.text.x = element_text(angle = 45, hjust = 1)) +
        labs(x = "Query Count ( Dataset size x 10^{6} )",
             title = paste("Query width = ", w)) 

}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure139809BG.pdf]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot(dfCount)

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980KMM.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980XWS.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980kgY.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980xqe.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[6]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980-0k.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[7]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980L_q.pdf]]


**** Plot: Element Size influence on Throughput Analysis by QueryId

#+begin_src R :results output :exports both :session 

tgpPlot2 <- function(dfCount){

dfCount %>% 
    filter(queryWidth == w) %>%
    #filter(queryId == 70) %>%
    arrange(T,Count) %>%
    mutate(lbls = paste(T/1000,"M (",Count,")",sep="")) %>%
    mutate(`T (Count)` = factor(lbls,levels=unique(lbls))) %>%
    #ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +
    ggplot(aes(x = factor(EltSize), group=algo, y = Count / avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
    geom_line() +
    facet_wrap(queryId~ `T (Count)` , scale = "fixed", ncol = 8,  labeller = label_both) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = paste("Query width = ", w)) 
}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w1.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot2(dfCount)

#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w1.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w2.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w2.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w3.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w3.pdf]]

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w4.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w4.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w5.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w5.pdf]]

**** Plot: Histogram of Queries Return Size

- Count histogram
- Aggregation histogram

#+begin_src R :results output :exports both :session 
dfHist <- 
dfCount %>% 
#    filter(algo=="BTree") %>%
#    filter(queryWidth == levelsWidth[[3]] ) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) 

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 800 :height 400 :session 
dfHist %>% 
    ggplot(aes(x = factor(EltsBin) , y = n/length(levels(factor(dfCount$EltSize)))  , fill=(T))) +
    geom_bar(stat="identity") +
    facet_grid(algo~queryWidth, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980OeQ.png]]


We should separate the "Ts"

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 


dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
#    filter(T == 1000) %>% 
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge") +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980o5Q.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black",size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281oNN.pdf]]

***** Plot: Througput barchart by T for a small query 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
#    mutate(EltsBin = Count %/% 100) %>%
#    group_by(algo,EltsBin, queryWidth,T) %>%
#    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count) , y = Count / avg_ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black", size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281O5A.pdf]]
