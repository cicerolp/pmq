# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries - Twitter Dataset
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* DONE Description                                                   :export:

Test the queries on Twitter Dataset and compare the following performances.

- PMQ / GEOHASH
- DENSE Vector + timSort
- BTREE 
- RTREE - quadratic algorithm 
- RTREE - quadratic algorithm with bulk loading

Use the refinement level = 8 

Elements:
- Batch size = 1000
- Variate T = from 1M to 128M
- Variate Element size also = from 16 to 256
 
#+begin_src python :results output :exports both

minitems = 1000
maxitems = 128000

items = minitems
while( items <= maxitems) :
    print(items*1000)
    items *=2
#+end_src

#+RESULTS:
: 1000000
: 2000000
: 4000000
: 8000000
: 16000000
: 32000000
: 64000000
: 128000000

- Total elements = T * 1000  
  
Use the synthetic queries generated by the DoE in [[file:~/Projects/pmq/data/queriesLHS.org::#queries20170923145357][Twitter Data]].

queries Dataset : [[file:~/Projects/pmq/data/queriesTwitter.csv]]


# Results in [[file:exp.pdf]]

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  
  
* DONE Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171016155353

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: [master 79c39e9] LBK: new entry for exp20171016155353
:  1 file changed, 45 insertions(+), 1 deletion(-)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171016155353
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171016155353 08e6c64] Initial commit for exp20171016155353
 1 file changed, 835 insertions(+)
 create mode 100644 data/cicero/exp20171016155353/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 08e6c64 (HEAD -> exp20171016155353) Initial commit for exp20171016155353
: * 79c39e9 (master) LBK: new entry for exp20171016155353
: * d0189bf workaround - performance bugs ?

** DONE Export run script 

#+begin_src sh :results output :exports both

for I in 1 2 4 8 16 32 64 128 ; do
    T=$(($I * 1000))
    echo "$T"
done
#+end_src

#+RESULTS:
: 1000
: 2000
: 4000
: 8000
: 16000
: 32000
: 64000
: 128000

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DELT_SIZE=0 -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=ON -DBENCH_BTREE=ON -DBENCH_RTREE=ON -DBENCH_DENSE=ON -DBENCH_RTREE_BULK=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 

#Run queries
b=1000
#n=$(($t*$b))
ref=8

for EL in 16 32 64 128 256 ; do
    ELTSIZE=$(($EL-16))
    cmake -DELT_SIZE=$ELTSIZE . ; make
    
    for i in 1 2 4 8 16 32 64 128 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dat -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesTwitter.csv >  ${TMPDIR}/bench_queries_region_twitter_${t}_${b}_${ref}_${ELTSIZE}_${EXECID}.log
    done
done

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org
	modified:   run.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	exp.pdf
	exp.tex
	img/

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171012184842 2292431] UPD: run.sh script
:  2 files changed, 2 insertions(+), 2 deletions(-)

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

75 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Fri Oct 13 16:41:34 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 23, done.
(1/20)           remote: Compressing objects:  10% (2/20)           remote: Compressing objects:  15% (3/20)           remote: Compressing objects:  20% (4/20)           remote: Compressing objects:  25% (5/20)           remote: Compressing objects:  30% (6/20)           remote: Compressing objects:  35% (7/20)           remote: Compressing objects:  40% (8/20)           remote: Compressing objects:  45% (9/20)           remote: Compressing objects:  50% (10/20)           remote: Compressing objects:  55% (11/20)           remote: Compressing objects:  60% (12/20)           remote: Compressing objects:  65% (13/20)           remote: Compressing objects:  70% (14/20)           remote: Compressing objects:  75% (15/20)           remote: Compressing objects:  80% (16/20)           remote: Compressing objects:  85% (17/20)           remote: Compressing objects:  90% (18/20)           remote: Compressing objects:  95% (19/20)           remote: Compressing objects: 100% (20/20)           remote: Compressing objects: 100% (20/20), done.        
remote: Total 23 (delta 16), reused 0 (delta 0)
(1/23)   Unpacking objects:   8% (2/23)   Unpacking objects:  13% (3/23)   Unpacking objects:  17% (4/23)   Unpacking objects:  21% (5/23)   Unpacking objects:  26% (6/23)   Unpacking objects:  30% (7/23)   Unpacking objects:  34% (8/23)   Unpacking objects:  39% (9/23)   Unpacking objects:  43% (10/23)   Unpacking objects:  47% (11/23)   Unpacking objects:  52% (12/23)   Unpacking objects:  56% (13/23)   Unpacking objects:  60% (14/23)   Unpacking objects:  65% (15/23)   Unpacking objects:  69% (16/23)   Unpacking objects:  73% (17/23)   Unpacking objects:  78% (18/23)   Unpacking objects:  82% (19/23)   Unpacking objects:  86% (20/23)   Unpacking objects:  91% (21/23)   Unpacking objects:  95% (22/23)   Unpacking objects: 100% (23/23)   Unpacking objects: 100% (23/23), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20171012184842
M	data/cicero/exp20171012184842/run_1507849705
Already on 'exp20171012184842'
Your branch is behind 'origin/exp20171012184842' by 4 commits, and can be fast-forwarded.
  (use "git pull" to update your local branch)
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Updating 6c842e8..2292431
Fast-forward
 benchmarks/bench_queries_region.cpp   |   3 +
 data/cicero/exp20171012184842/exp.org | 724 +++++++++++++++++-----------------
 data/cicero/exp20171012184842/run.sh  |   2 +-
 3 files changed, 376 insertions(+), 353 deletions(-)
commit 229243171c14b0e2c7cc9d9a4b1ffc0d6017cc79
Date:   Fri Oct 13 16:43:23 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ 1507923856

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
: no server running on /tmp/tmux-1001/default
: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
: julio    12367  0.0  0.0  45248  4684 ?        Ss   16:43   0:00 /lib/systemd/sy
: julio    12370  0.0  0.0 145364  2112 ?        S    16:43   0:00 (sd-pam)
: julio    12398  0.0  0.0  97464  3296 ?        R    16:43   0:00 sshd: julio@pts
: julio    12399  0.0  0.0  22688  5132 pts/8    Ss   16:43   0:00 -bash
: julio    12746  0.0  0.0  37368  3296 pts/8    R+   17:11   0:00 ps ux

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
git commit -a -m "wip"
git status
git pull --rebase origin $expId
#+end_src

#+RESULTS:
#+begin_example
[exp20171012184842 37984b2] wip
 1 file changed, 29 insertions(+), 26 deletions(-)
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../exp20170825181747/
	../exp20170830124159/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	../exp20170923144931/
	../exp20170923193058/
	../exp20171009155025/
	exp.pdf
	exp.tex
	img/
	../../queriesLHS.html
	../../queriesLHS_BACKUP_23848.org
	../../queriesLHS_BASE_23848.org
	../../queriesLHS_LOCAL_23848.org
	../../queriesLHS_REMOTE_23848.org
	../../randomLhsQueries.png
	../../../history.txt
	../../../qqqq

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Applying: wip
#+end_example


* DONE Analysis
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tarFile
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tarFile
| log_1508177471.tgz |

#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tarFile
for i in $f; do 
    tar xvzf $i
done
#+end_src

#+RESULTS: logFile
#+begin_example
bench_queries_region_twitter_1000_1000_8_0_1508177471.log
bench_queries_region_twitter_1000_1000_8_112_1508177471.log
bench_queries_region_twitter_1000_1000_8_16_1508177471.log
bench_queries_region_twitter_1000_1000_8_240_1508177471.log
bench_queries_region_twitter_1000_1000_8_48_1508177471.log
bench_queries_region_twitter_128000_1000_8_0_1508177471.log
bench_queries_region_twitter_128000_1000_8_112_1508177471.log
bench_queries_region_twitter_128000_1000_8_16_1508177471.log
bench_queries_region_twitter_128000_1000_8_240_1508177471.log
bench_queries_region_twitter_128000_1000_8_48_1508177471.log
bench_queries_region_twitter_16000_1000_8_0_1508177471.log
bench_queries_region_twitter_16000_1000_8_112_1508177471.log
bench_queries_region_twitter_16000_1000_8_16_1508177471.log
bench_queries_region_twitter_16000_1000_8_240_1508177471.log
bench_queries_region_twitter_16000_1000_8_48_1508177471.log
bench_queries_region_twitter_2000_1000_8_0_1508177471.log
bench_queries_region_twitter_2000_1000_8_112_1508177471.log
bench_queries_region_twitter_2000_1000_8_16_1508177471.log
bench_queries_region_twitter_2000_1000_8_240_1508177471.log
bench_queries_region_twitter_2000_1000_8_48_1508177471.log
bench_queries_region_twitter_32000_1000_8_0_1508177471.log
bench_queries_region_twitter_32000_1000_8_112_1508177471.log
bench_queries_region_twitter_32000_1000_8_16_1508177471.log
bench_queries_region_twitter_32000_1000_8_240_1508177471.log
bench_queries_region_twitter_32000_1000_8_48_1508177471.log
bench_queries_region_twitter_4000_1000_8_0_1508177471.log
bench_queries_region_twitter_4000_1000_8_112_1508177471.log
bench_queries_region_twitter_4000_1000_8_16_1508177471.log
bench_queries_region_twitter_4000_1000_8_240_1508177471.log
bench_queries_region_twitter_4000_1000_8_48_1508177471.log
bench_queries_region_twitter_64000_1000_8_0_1508177471.log
bench_queries_region_twitter_64000_1000_8_112_1508177471.log
bench_queries_region_twitter_64000_1000_8_16_1508177471.log
bench_queries_region_twitter_64000_1000_8_240_1508177471.log
bench_queries_region_twitter_64000_1000_8_48_1508177471.log
bench_queries_region_twitter_8000_1000_8_0_1508177471.log
bench_queries_region_twitter_8000_1000_8_112_1508177471.log
bench_queries_region_twitter_8000_1000_8_16_1508177471.log
bench_queries_region_twitter_8000_1000_8_240_1508177471.log
bench_queries_region_twitter_8000_1000_8_48_1508177471.log
#+end_example

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile
for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
#+begin_example
bench_queries_region_twitter_1000_1000_8_0_1508177471.csv
bench_queries_region_twitter_1000_1000_8_112_1508177471.csv
bench_queries_region_twitter_1000_1000_8_16_1508177471.csv
bench_queries_region_twitter_1000_1000_8_240_1508177471.csv
bench_queries_region_twitter_1000_1000_8_48_1508177471.csv
bench_queries_region_twitter_128000_1000_8_0_1508177471.csv
bench_queries_region_twitter_128000_1000_8_112_1508177471.csv
bench_queries_region_twitter_128000_1000_8_16_1508177471.csv
bench_queries_region_twitter_128000_1000_8_240_1508177471.csv
bench_queries_region_twitter_128000_1000_8_48_1508177471.csv
bench_queries_region_twitter_16000_1000_8_0_1508177471.csv
bench_queries_region_twitter_16000_1000_8_112_1508177471.csv
bench_queries_region_twitter_16000_1000_8_16_1508177471.csv
bench_queries_region_twitter_16000_1000_8_240_1508177471.csv
bench_queries_region_twitter_16000_1000_8_48_1508177471.csv
bench_queries_region_twitter_2000_1000_8_0_1508177471.csv
bench_queries_region_twitter_2000_1000_8_112_1508177471.csv
bench_queries_region_twitter_2000_1000_8_16_1508177471.csv
bench_queries_region_twitter_2000_1000_8_240_1508177471.csv
bench_queries_region_twitter_2000_1000_8_48_1508177471.csv
bench_queries_region_twitter_32000_1000_8_0_1508177471.csv
bench_queries_region_twitter_32000_1000_8_112_1508177471.csv
bench_queries_region_twitter_32000_1000_8_16_1508177471.csv
bench_queries_region_twitter_32000_1000_8_240_1508177471.csv
bench_queries_region_twitter_32000_1000_8_48_1508177471.csv
bench_queries_region_twitter_4000_1000_8_0_1508177471.csv
bench_queries_region_twitter_4000_1000_8_112_1508177471.csv
bench_queries_region_twitter_4000_1000_8_16_1508177471.csv
bench_queries_region_twitter_4000_1000_8_240_1508177471.csv
bench_queries_region_twitter_4000_1000_8_48_1508177471.csv
bench_queries_region_twitter_64000_1000_8_0_1508177471.csv
bench_queries_region_twitter_64000_1000_8_112_1508177471.csv
bench_queries_region_twitter_64000_1000_8_16_1508177471.csv
bench_queries_region_twitter_64000_1000_8_240_1508177471.csv
bench_queries_region_twitter_64000_1000_8_48_1508177471.csv
bench_queries_region_twitter_8000_1000_8_0_1508177471.csv
bench_queries_region_twitter_8000_1000_8_112_1508177471.csv
bench_queries_region_twitter_8000_1000_8_16_1508177471.csv
bench_queries_region_twitter_8000_1000_8_240_1508177471.csv
bench_queries_region_twitter_8000_1000_8_48_1508177471.csv
#+end_example

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

#+LATEX_HEADER:  \usepackage[a4paper,includeheadfoot,margin=2cm]{geometry}
 
*** Prepare

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep=""), col_types="ccicicdcici" ) %>%
         mutate (
            EltSize = as.factor(
                 gsub("bench_queries_region_twitter_[[:digit:]]+_1000_8_([[:digit:]]+)_[[:digit:]]+.csv","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
 [1] "bench_queries_region_twitter_1000_1000_8_0_1508177471.csv"    
 [2] "bench_queries_region_twitter_1000_1000_8_112_1508177471.csv"  
 [3] "bench_queries_region_twitter_1000_1000_8_16_1508177471.csv"   
 [4] "bench_queries_region_twitter_1000_1000_8_240_1508177471.csv"  
 [5] "bench_queries_region_twitter_1000_1000_8_48_1508177471.csv"   
 [6] "bench_queries_region_twitter_128000_1000_8_0_1508177471.csv"  
 [7] "bench_queries_region_twitter_128000_1000_8_112_1508177471.csv"
 [8] "bench_queries_region_twitter_128000_1000_8_16_1508177471.csv" 
 [9] "bench_queries_region_twitter_128000_1000_8_240_1508177471.csv"
[10] "bench_queries_region_twitter_128000_1000_8_48_1508177471.csv" 
[11] "bench_queries_region_twitter_16000_1000_8_0_1508177471.csv"   
[12] "bench_queries_region_twitter_16000_1000_8_112_1508177471.csv" 
[13] "bench_queries_region_twitter_16000_1000_8_16_1508177471.csv"  
[14] "bench_queries_region_twitter_16000_1000_8_240_1508177471.csv" 
[15] "bench_queries_region_twitter_16000_1000_8_48_1508177471.csv"  
[16] "bench_queries_region_twitter_2000_1000_8_0_1508177471.csv"    
[17] "bench_queries_region_twitter_2000_1000_8_112_1508177471.csv"  
[18] "bench_queries_region_twitter_2000_1000_8_16_1508177471.csv"   
[19] "bench_queries_region_twitter_2000_1000_8_240_1508177471.csv"  
[20] "bench_queries_region_twitter_2000_1000_8_48_1508177471.csv"   
[21] "bench_queries_region_twitter_32000_1000_8_0_1508177471.csv"   
[22] "bench_queries_region_twitter_32000_1000_8_112_1508177471.csv" 
[23] "bench_queries_region_twitter_32000_1000_8_16_1508177471.csv"  
[24] "bench_queries_region_twitter_32000_1000_8_240_1508177471.csv" 
[25] "bench_queries_region_twitter_32000_1000_8_48_1508177471.csv"  
[26] "bench_queries_region_twitter_4000_1000_8_0_1508177471.csv"    
[27] "bench_queries_region_twitter_4000_1000_8_112_1508177471.csv"  
[28] "bench_queries_region_twitter_4000_1000_8_16_1508177471.csv"   
[29] "bench_queries_region_twitter_4000_1000_8_240_1508177471.csv"  
[30] "bench_queries_region_twitter_4000_1000_8_48_1508177471.csv"   
[31] "bench_queries_region_twitter_64000_1000_8_0_1508177471.csv"   
[32] "bench_queries_region_twitter_64000_1000_8_112_1508177471.csv" 
[33] "bench_queries_region_twitter_64000_1000_8_16_1508177471.csv"  
[34] "bench_queries_region_twitter_64000_1000_8_240_1508177471.csv" 
[35] "bench_queries_region_twitter_64000_1000_8_48_1508177471.csv"  
[36] "bench_queries_region_twitter_8000_1000_8_0_1508177471.csv"    
[37] "bench_queries_region_twitter_8000_1000_8_112_1508177471.csv"  
[38] "bench_queries_region_twitter_8000_1000_8_16_1508177471.csv"   
[39] "bench_queries_region_twitter_8000_1000_8_240_1508177471.csv"  
[40] "bench_queries_region_twitter_8000_1000_8_48_1508177471.csv"
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                            file expected   <int> <chr>      <chr>      <chr>                                                           <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................... ........ .............................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                            file expected   <int> <chr>      <chr>      <chr>                                                           <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................... ........ .............................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
There were 40 warnings (use warnings() to see them)
#+end_example


Remove useless columns
#+begin_src R :results output :exports both :session 
names(df) <- c("algo" , "V2" , "queryId", "V4", "T", "bench" , "ms" , "V8", "Refine","V10","Count","EltSize")

df <- select(df, -V2, -V4, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 320,000 x 8
            algo queryId     T           bench       ms Refine  Count EltSize
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>  <fctr>
 1 GeoHashBinary       0  1000 apply_at_region 0.419478     29 924827       0
 2 GeoHashBinary       0  1000 apply_at_region 0.410266     29 924827       0
 3 GeoHashBinary       0  1000 apply_at_region 0.408498     29 924827       0
 4 GeoHashBinary       0  1000 apply_at_region 0.408071     29 924827       0
 5 GeoHashBinary       0  1000 apply_at_region 0.407314     29 924827       0
 6 GeoHashBinary       0  1000 apply_at_region 0.408054     29 924827       0
 7 GeoHashBinary       0  1000 apply_at_region 0.407821     29 924827       0
 8 GeoHashBinary       0  1000 apply_at_region 0.407836     29 924827       0
 9 GeoHashBinary       0  1000 apply_at_region 0.406955     29 924827       0
10 GeoHashBinary       0  1000 apply_at_region 0.421066     29 924827       0
# ... with 319,990 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both :session 
df <- 
    df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) %>%
    mutate(queryWidth = 90 / 2**(queryId %/% 10))  %>%   # comput info about query width
    mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> df # adjust the actual size of the elements
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :session :exports both
df 

dfAvg <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfAvg
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 320,000 x 9
            algo queryId     T           bench       ms Refine  Count EltSize queryWidth
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>   <dbl>      <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 0.419478     29 924827      16         90
 2 GeoHashBinary       0  1000 apply_at_region 0.410266     29 924827      16         90
 3 GeoHashBinary       0  1000 apply_at_region 0.408498     29 924827      16         90
 4 GeoHashBinary       0  1000 apply_at_region 0.408071     29 924827      16         90
 5 GeoHashBinary       0  1000 apply_at_region 0.407314     29 924827      16         90
 6 GeoHashBinary       0  1000 apply_at_region 0.408054     29 924827      16         90
 7 GeoHashBinary       0  1000 apply_at_region 0.407821     29 924827      16         90
 8 GeoHashBinary       0  1000 apply_at_region 0.407836     29 924827      16         90
 9 GeoHashBinary       0  1000 apply_at_region 0.406955     29 924827      16         90
10 GeoHashBinary       0  1000 apply_at_region 0.421066     29 924827      16         90
# ... with 319,990 more rows
# A tibble: 32,000 x 10
# Groups:   algo, queryId, T, bench, Refine, Count, EltSize [?]
    algo queryId     T           bench Refine  Count EltSize queryWidth   avg_ms        stdv
   <chr>   <int> <int>           <chr>  <int>  <int>   <dbl>      <dbl>    <dbl>       <dbl>
 1 BTree       0  1000 apply_at_region     27 924827      16         90 10.92556 0.839933454
 2 BTree       0  1000 apply_at_region     27 924827      32         90 10.60835 0.056736491
 3 BTree       0  1000 apply_at_region     27 924827      64         90 10.85630 0.851608473
 4 BTree       0  1000 apply_at_region     27 924827     128         90 10.97631 0.844460953
 5 BTree       0  1000 apply_at_region     27 924827     256         90 11.22784 0.832504941
 6 BTree       0  1000  scan_at_region     29     NA      16         90 17.43356 0.010635078
 7 BTree       0  1000  scan_at_region     29     NA      32         90 17.48158 0.022587843
 8 BTree       0  1000  scan_at_region     29     NA      64         90 17.50776 0.017892407
 9 BTree       0  1000  scan_at_region     29     NA     128         90 17.48583 0.008840569
10 BTree       0  1000  scan_at_region     29     NA     256         90 17.61844 0.032610912
# ... with 31,990 more rows
#+end_example

*** Plot: change of query count with size of T.                    :export:
#+begin_src R :results output graphics :file "./img/count_by_T.png" :exports both :width 600 :height 400 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region") %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId%%10))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryWidth, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:./img/count_by_T.png]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryId, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980T9Z.pdf]]

*** Plot: Scan Query Time by T facet by queryId                    :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
#    filter(EltSize == 0) %>%
    filter(bench == "scan_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free_y",ncol=10,labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980ExF.pdf]]

*** Plot: Scan query time by Query Count faceted by QueryId        :export:

#+begin_src R :results output :exports both :session 
dfAvg %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,T,EltSize) %>%
    left_join( 
        filter(ungroup(dfAvg), bench == "scan_at_region") %>% select(-Count)
   ) -> dfCount
dfCount
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "algo", "T", "EltSize")
# A tibble: 16,000 x 10
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms        stdv
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>       <dbl>
 1       0 BTree  924827  1000      16 scan_at_region     29         90 17.43356 0.010635078
 2       0 BTree  924827  1000      32 scan_at_region     29         90 17.48158 0.022587843
 3       0 BTree  924827  1000      64 scan_at_region     29         90 17.50776 0.017892407
 4       0 BTree  924827  1000     128 scan_at_region     29         90 17.48583 0.008840569
 5       0 BTree  924827  1000     256 scan_at_region     29         90 17.61844 0.032610912
 6       0 BTree 1855890  2000      16 scan_at_region     29         90 36.57063 0.040832532
 7       0 BTree 1855890  2000      32 scan_at_region     29         90 36.25142 0.015255221
 8       0 BTree 1855890  2000      64 scan_at_region     29         90 36.48388 0.014496421
 9       0 BTree 1855890  2000     128 scan_at_region     29         90 36.87419 0.016553445
10       0 BTree 1855890  2000     256 scan_at_region     29         90 36.47536 0.031228164
# ... with 15,990 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980L42.pdf]]

*** Plot: Scan query throughput by Query Count faceted by QueryId  :export:
:PROPERTIES: 
:HEADER-ARGS:R: :exports results
:END:      

#+begin_src R :results output :exports both :session 

tgpPlot <- function(dfCount){
    dfCount %>% 
        filter(queryWidth == w) %>%
        arrange(Count,T) %>%
        mutate(lbls = paste(Count," (",T/1000,")",sep="")) %>%
        ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +  
                                        #ggplot(aes(x = factor(Count), group=algo, y = Count / avg_ms, color = algo)) +  
        geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
        geom_line() +
        facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
        theme(legend.position = "bottom",
              axis.text.x = element_text(angle = 45, hjust = 1)) +
        labs(x = "Query Count ( Dataset size x 10^{6} )",
             title = paste("Query width = ", w)) 

}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure139809BG.pdf]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot(dfCount)

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980KMM.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980XWS.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980kgY.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980xqe.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[6]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980-0k.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[7]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980L_q.pdf]]


*** Plot: Element Size influence on Throughput Analysis by QueryId

#+begin_src R :results output :exports both :session 

tgpPlot2 <- function(dfCount){

dfCount %>% 
    filter(queryWidth == w) %>%
    #filter(queryId == 70) %>%
    arrange(T,Count) %>%
    mutate(lbls = paste(T/1000,"M (",Count,")",sep="")) %>%
    mutate(`T (Count)` = factor(lbls,levels=unique(lbls))) %>%
    #ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +
    ggplot(aes(x = factor(EltSize), group=algo, y = Count / avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
    geom_line() +
    facet_wrap(queryId~ `T (Count)` , scale = "fixed", ncol = 8,  labeller = label_both) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = paste("Query width = ", w)) 
}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w1.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot2(dfCount)

#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w1.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w2.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w2.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w3.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w3.pdf]]

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w4.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w4.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w5.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w5.pdf]]

*** Plot: Histogram of Queries Return Size

- Count histogram
- Aggregation histogram

#+begin_src R :results output :exports both :session 
dfHist <- 
dfCount %>% 
#    filter(algo=="BTree") %>%
#    filter(queryWidth == levelsWidth[[3]] ) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) 

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 800 :height 400 :session 
dfHist %>% 
    ggplot(aes(x = factor(EltsBin) , y = n/length(levels(factor(dfCount$EltSize)))  , fill=(T))) +
    geom_bar(stat="identity") +
    facet_grid(algo~queryWidth, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980OeQ.png]]


We should separate the "Ts"

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 


dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
#    filter(T == 1000) %>% 
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge") +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980o5Q.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black",size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281oNN.pdf]]

**** Plot: Througput barchart by T for a small query 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
#    mutate(EltsBin = Count %/% 100) %>%
#    group_by(algo,EltsBin, queryWidth,T) %>%
#    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count) , y = Count / avg_ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black", size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281O5A.pdf]]

* Testing "False positive ratios"

** Running

Running src from master branch.

#+begin_src sh :session teste :results output :exports both 
cd ~/Projects/pmq/build-release/
 
    for i in 1 2 4 8 16 32 64 128 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dmp -x 1 -rate 1000 -min_t ${t} -max_t ${t} -ref 8 -bf ../data/queriesTwitter.csv >> bench_queries_region_twitter_ratio_analysis_1_to_128.log
    done
#+end_src

#+begin_src sh :results output :exports both
cd ~/Projects/pmq/build-release/
git log -1 master --oneline

git add bench_queries_region_twitter_ratio_analysis_1_to_128.log

tail bench_queries_region_twitter_ratio_analysis_1_to_128.log
#+end_src

#+RESULTS:
#+begin_example
a181d8c pmq: return true positives
QueryBench GeoHashBinary ; query ; 76 ; T ; 128000 ; scan_at_region ; 3.10143 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 30670 ; scan_at_region_false_positives ; 243768 ; scan_at_region_true_and_false ; 274438 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 20.4222 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 77 ; T ; 128000 ; apply_at_region ; 26.8968 ; apply_at_region_refinements ; 2 ; count ; 1521534 ; 
QueryBench GeoHashBinary ; query ; 77 ; T ; 128000 ; scan_at_region ; 29.137 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 1.52153e+06 ; scan_at_region_false_positives ; 1.11056e+06 ; scan_at_region_true_and_false ; 2.6321e+06 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 19.7861 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 78 ; T ; 128000 ; apply_at_region ; 31.5211 ; apply_at_region_refinements ; 2 ; count ; 868311 ; 
QueryBench GeoHashBinary ; query ; 78 ; T ; 128000 ; scan_at_region ; 37.4016 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 868311 ; scan_at_region_false_positives ; 2.69751e+06 ; scan_at_region_true_and_false ; 3.56582e+06 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 19.6703 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 79 ; T ; 128000 ; apply_at_region ; 9.45635 ; apply_at_region_refinements ; 2 ; count ; 822726 ; 
QueryBench GeoHashBinary ; query ; 79 ; T ; 128000 ; scan_at_region ; 10.5603 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 822726 ; scan_at_region_false_positives ; 137430 ; scan_at_region_true_and_false ; 960156 ; 
#+end_example


#+begin_src sh :results output :exports both
cp ~/Projects/pmq/build-release/bench_queries_region_twitter_ratio_analysis_1_to_128.log .
git add -f bench_queries_region_twitter_ratio_analysis_1_to_128.log
#+end_src

#+RESULTS:



** Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList="bench_queries_region_twitter_ratio_analysis_1_to_128.log"

for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
: bench_queries_region_twitter_ratio_analysis_1_to_128.csv


** Prepare

Load the CSV into R

Remove useless columns
- Milliseconds and Refine are note relevant in this test
- FPOS FPOS and ALLPOS must be float

#+begin_src R :results output :exports both :var path=(print default-directory) :session
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

f="bench_queries_region_twitter_ratio_analysis_1_to_128.csv"

# Warining: make sure we read the values as doudle because for csv format number issues
df2 <- read_delim(f,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:15),sep=""), col_types="ccicicdcicdcdcd" )
# df2

names(df2) <- c("algo" , "V2" , "queryId", "V4", 
                "T", "bench" , "ms" , "V8","Refine","V10","TPOS","V12","FPOS","V14","ALLPOS")
df2 <- select(df2, -V2, -V4, -V8, -V10, -V12, -V14, -ms, -Refine)
df2

#df2 %>% filter(queryId == 55, T == 16000) 
#+end_src

#+RESULTS:
#+begin_example
Warning: 1280 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' file 2     2  <NA> 15 columns 16 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' col 4     4  <NA> 15 columns 16 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning message:
In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
# A tibble: 1,280 x 7
            algo queryId     T           bench   TPOS  FPOS ALLPOS
           <chr>   <int> <int>           <chr>  <dbl> <dbl>  <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 924827    NA     NA
 2 GeoHashBinary       0  1000  scan_at_region  30329   798  31127
 3 GeoHashBinary       1  1000 apply_at_region 929918    NA     NA
 4 GeoHashBinary       1  1000  scan_at_region   4293  2357   6650
 5 GeoHashBinary       2  1000 apply_at_region 753921    NA     NA
 6 GeoHashBinary       2  1000  scan_at_region   6017 65222  71239
 7 GeoHashBinary       3  1000 apply_at_region 989228    NA     NA
 8 GeoHashBinary       3  1000  scan_at_region      0     1      1
 9 GeoHashBinary       4  1000 apply_at_region 929320    NA     NA
10 GeoHashBinary       4  1000  scan_at_region   3695  2955   6650
# ... with 1,270 more rows
#+end_example

#+begin_src R :results output :exports both :session 
df2 %>% 
    filter(bench=="scan_at_region") %>%
    select(-algo,-bench) -> dfRatios
    
dfRatios
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 5
   queryId     T  TPOS  FPOS ALLPOS
     <int> <int> <dbl> <dbl>  <dbl>
 1       0  1000 30329   798  31127
 2       1  1000  4293  2357   6650
 3       2  1000  6017 65222  71239
 4       3  1000     0     1      1
 5       4  1000  3695  2955   6650
 6       5  1000 28635  2492  31127
 7       6  1000     0     1      1
 8       7  1000 79127   773  79900
 9       8  1000     1     0      1
10       9  1000 76054  3846  79900
# ... with 630 more rows
#+end_example

Join with dataframe with real performance measurements to get the false-positives
#+begin_src R :results output :exports both :session 
dfCount %>% 
    left_join( dfRatios ) %>%
    mutate(TotScans = ALLPOS + (Count - TPOS)) -> dfCR  # to compute the ratio based on all the elements scanned. 

dfCR
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "T")
# A tibble: 16,000 x 14
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms        stdv  TPOS  FPOS ALLPOS TotScans
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>       <dbl> <dbl> <dbl>  <dbl>    <dbl>
 1       0 BTree  924827  1000      16 scan_at_region     29         90 17.43356 0.010635078 30329   798  31127   925625
 2       0 BTree  924827  1000      32 scan_at_region     29         90 17.48158 0.022587843 30329   798  31127   925625
 3       0 BTree  924827  1000      64 scan_at_region     29         90 17.50776 0.017892407 30329   798  31127   925625
 4       0 BTree  924827  1000     128 scan_at_region     29         90 17.48583 0.008840569 30329   798  31127   925625
 5       0 BTree  924827  1000     256 scan_at_region     29         90 17.61844 0.032610912 30329   798  31127   925625
 6       0 BTree 1855890  2000      16 scan_at_region     29         90 36.57063 0.040832532 60741  1727  62468  1857617
 7       0 BTree 1855890  2000      32 scan_at_region     29         90 36.25142 0.015255221 60741  1727  62468  1857617
 8       0 BTree 1855890  2000      64 scan_at_region     29         90 36.48388 0.014496421 60741  1727  62468  1857617
 9       0 BTree 1855890  2000     128 scan_at_region     29         90 36.87419 0.016553445 60741  1727  62468  1857617
10       0 BTree 1855890  2000     256 scan_at_region     29         90 36.47536 0.031228164 60741  1727  62468  1857617
# ... with 15,990 more rows
#+end_example

** DONE Plot                                                        :export:

#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(EltSize == 16) %>%
    mutate(Ratio = FPOS / TotScans) %>%
    mutate(Ratio2 = FPOS / ALLPOS) %>%
    mutate(tgp = Count / avg_ms) %>%
    #filter(queryWidth < 1) %>%
    filter(algo == "BTree") %>%
    filter(Count != TPOS) %>%
    arrange(Count)

#+end_src

*** Plot : ratios
#+RESULTS:
#+begin_example
# A tibble: 493 x 17
   queryId  algo Count     T EltSize          bench Refine queryWidth    avg_ms         stdv  TPOS  FPOS ALLPOS TotScans        Ratio    Ratio2       tgp
     <int> <chr> <int> <int>   <dbl>          <chr>  <int>      <dbl>     <dbl>        <dbl> <dbl> <dbl>  <dbl>    <dbl>        <dbl>     <dbl>     <dbl>
 1      51 BTree  2377  1000      16 scan_at_region     12     2.8125 0.0935207 0.0050950426   800  5221   6021     7598 0.6871545143 0.8671317 25416.833
 2      51 BTree  4537  2000      16 scan_at_region     12     2.8125 0.2101530 0.0128721513  1503 10271  11774    14808 0.6936115613 0.8723458 21589.033
 3      54 BTree  6164  1000      16 scan_at_region     12     2.8125 0.4925632 0.0012249881  4031 39312  43343    45476 0.8644559768 0.9069977 12514.130
 4      15 BTree  8096  1000      16 scan_at_region      6    45.0000 0.1224318 0.0069989957     0     0      0     8096 0.0000000000       NaN 66126.611
 5      51 BTree  9398  4000      16 scan_at_region     12     2.8125 0.4072375 0.0935481622  3507 21023  24530    30421 0.6910686697 0.8570322 23077.443
 6      42 BTree 11446  1000      16 scan_at_region     23     5.6250 0.2468979 0.0145591697   525  7715   8240    19161 0.4026407808 0.9362864 46359.244
 7      45 BTree 11619  1000      16 scan_at_region     23     5.6250 0.1860315 0.0009233704   698  7542   8240    19161 0.3936120244 0.9152913 62457.165
 8      54 BTree 12970  2000      16 scan_at_region     12     2.8125 1.4501400 0.0451758183  8517 82252  90769    95222 0.8637919808 0.9061684  8943.964
 9      15 BTree 13292  2000      16 scan_at_region      9    45.0000 0.2019838 0.0146983636     0     2      2    13294 0.0001504438 1.0000000 65807.258
10      44 BTree 16350  1000      16 scan_at_region     24     5.6250 0.2060321 0.0008137880  4390  3232   7622    19582 0.1650495353 0.4240357 79356.566
# ... with 483 more rows
#+end_example

#+begin_src R :results output :exports both :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
#w = levelsWidth[[1]]
levelsWidth

#+end_src

#+RESULTS:
: [1] "0.703125" "1.40625"  "2.8125"   "5.625"    "11.25"    "22.5"     "45"      
: [8] "90"

#+begin_src R :results output :exports both :session 
plotRatios <- function(dfCR,w){
    dfCR %>% 
    filter(EltSize == 16) %>%
    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(Count), y = Count / avg_ms, fill=(FPOS / TotScans)) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position="dodge",color="black", size =0.2) +
    facet_wrap(~algo, scale="free", ncol = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
   labs(title = paste("Query width = ", w))
}
#+end_src

#+RESULTS:

Facet free 
#+begin_src R :results output graphics :file "./img/tgp-false-positves2.pdf" :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w)
    print(p)
}
#+end_src

#+RESULTS:
[[file:./img/tgp-false-positves2.pdf]]

Facet free_x
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w) + 
    facet_wrap(~algo, scale="free_x", ncol = 1)
    print(p)
}

#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281pOU.pdf]]

*** Speed-Ups of Throughputs                                       :export:

#+begin_src R :results output :exports both :session 
dfCR
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 16,000 x 14
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms        stdv  TPOS  FPOS ALLPOS TotScans
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>       <dbl> <dbl> <dbl>  <dbl>    <dbl>
 1       0 BTree  924827  1000      16 scan_at_region     29         90 17.43356 0.010635078 30329   798  31127   925625
 2       0 BTree  924827  1000      32 scan_at_region     29         90 17.48158 0.022587843 30329   798  31127   925625
 3       0 BTree  924827  1000      64 scan_at_region     29         90 17.50776 0.017892407 30329   798  31127   925625
 4       0 BTree  924827  1000     128 scan_at_region     29         90 17.48583 0.008840569 30329   798  31127   925625
 5       0 BTree  924827  1000     256 scan_at_region     29         90 17.61844 0.032610912 30329   798  31127   925625
 6       0 BTree 1855890  2000      16 scan_at_region     29         90 36.57063 0.040832532 60741  1727  62468  1857617
 7       0 BTree 1855890  2000      32 scan_at_region     29         90 36.25142 0.015255221 60741  1727  62468  1857617
 8       0 BTree 1855890  2000      64 scan_at_region     29         90 36.48388 0.014496421 60741  1727  62468  1857617
 9       0 BTree 1855890  2000     128 scan_at_region     29         90 36.87419 0.016553445 60741  1727  62468  1857617
10       0 BTree 1855890  2000     256 scan_at_region     29         90 36.47536 0.031228164 60741  1727  62468  1857617
# ... with 15,990 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

    dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree")) %>%
    filter(EltSize == 16) %>%
#    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(queryId%%10), y = Count / avg_ms, fill=(FPOS / TotScans), group=T) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position=position_dodge(),color="black", size =0.2) +
    geom_errorbar(aes(ymin = Count / (avg_ms-stdv), ymax = Count / (avg_ms + stdv)), position=position_dodge())+ 
    facet_grid(queryWidth~algo, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
 #  labs(title = paste("Query width = ", w))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281Erb.pdf]]


#+begin_src R :results output :exports both :session :eval never
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree")) %>%
    filter(EltSize == 16) %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up  = RTree_avg_ms / GeoHashBinary_avg_ms ) -> dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up)) %>% arrange(ord)

#+end_src

Compute Speedups
#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree")) %>%
    filter(EltSize == 16) %>%
#Because the Count diverges for Rtree and PMQ , gathering with count 
    #select(queryId, algo, T, queryWidth, avg_ms, stdv, Count, FPOS, ALLPOS, TotScans ) %>% #print() %>%
    #gather(variable, value, Count, avg_ms, stdv) %>% #print() %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up  = RTree_avg_ms / GeoHashBinary_avg_ms ) %>% print() %>%
    inner_join( dfCR %>% 
                filter(algo == "GeoHashBinary", EltSize == 16) %>% 
                select(queryId, T, Count, TotScans)) ->  dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up)) %>% arrange(ord)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 10
   queryId      T queryWidth   FPOS  ALLPOS GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms RTree_stdv speed_up
     <int>  <int>      <dbl>  <dbl>   <dbl>                <dbl>              <dbl>        <dbl>      <dbl>    <dbl>
 1       0   1000         90    798   31127             4.594044        0.008478713     20.79137 0.05379544 4.525723
 2       0   2000         90   1727   62468             8.516582        0.005368902     42.39427 0.05708750 4.977850
 3       0   4000         90   3543  130106            17.061550        0.031491913     87.24963 0.06580046 5.113816
 4       0   8000         90   7505  264277            34.077500        0.025706981    181.07150 0.12255634 5.313521
 5       0  16000         90  16719  513521            68.352870        0.085200614    375.67220 0.22341083 5.496071
 6       0  32000         90  36281 1021940           136.258300        0.046745291    764.90430 0.29928249 5.613635
 7       0  64000         90  72424 1998820           272.577800        0.035209847   1555.83700 0.46595303 5.707864
 8       0 128000         90 149232 3888110           547.478200        0.084006349   3184.72200 1.43768950 5.817075
 9       1   1000         90   2357    6650             4.461463        0.005444802     20.85763 0.02949689 4.675065
10       1   2000         90   4148   12632             8.269311        0.023519216     42.53854 0.05618333 5.144146
# ... with 630 more rows
Joining, by = c("queryId", "T")
# A tibble: 640 x 13
   queryId      T queryWidth    FPOS  ALLPOS GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv  speed_up    Count TotScans   ord
     <int>  <int>      <dbl>   <dbl>   <dbl>                <dbl>              <dbl>        <dbl>       <dbl>     <dbl>    <int>    <dbl> <int>
 1      48   1000   5.625000   16362   46123            1.1788610       0.0390889221    5.3986460 0.006832459 4.5795442   206590   222952     1
 2      56  32000   2.812500  290637  547328            8.6813870       0.0071792433   32.8720300 0.050283221 3.7864952  1106318  1396955     2
 3      58 128000   2.812500 1101600 2262700           35.2006700       0.0260767265  127.5946000 0.154166721 3.6247776  4464643  5566243     3
 4      63   8000   1.406250   68160  111526            0.8815208       0.0024004233    0.6506137 0.004968984 0.7380583    43366   111526     4
 5      66  32000   1.406250  384932 1083070           10.3894700       0.0180665467   19.7171200 0.017889339 1.8977984   698138  1083070     5
 6      68  32000   1.406250  700733  949309            7.9824440       0.0041230065    6.8237150 0.013180009 0.8548403   248576   949309     6
 7      70  32000   0.703125  720294 1675500           15.5850500       0.0102136347   25.8663600 0.027881384 1.6596905   955205  1675500     7
 8      72   1000   0.703125    1454    7951            0.0775408       0.0005726049    0.0870682 0.001104221 1.1228695     6497     7951     8
 9      50  64000   2.812500 2383320 4139970           54.8944000       0.0131379011  158.1154000 0.197790686 2.8803557  5990017  8373337     9
10      59 128000   2.812500 6010120 8158380          103.6845000       0.0313696598  279.3784000 0.292132466 2.6945050 10349418 16359548    10
# ... with 630 more rows
#+end_example

*** Speedup - correlations                                         :export:
:PROPERTIES:
:HEADER-ARGS:R: :exports result
:END:


- Order the results by speedup

- try several mapping to a continuos color map

Count
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up) ) +
    geom_bar(aes(fill=(Count)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure32817kx.pdf]]

QueryWidth
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up) ) +
    geom_bar(aes(fill=factor(queryWidth)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure32816c2.pdf]]

Ratio : FPOS / ALLPOS
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up) ) +
    geom_bar(aes(fill=(FPOS / ALLPOS)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281smF.pdf]]

Ratio : FPOS / TotScans
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281tgY.pdf]]


The color scale show that the speed-up increases according to ratio of False positives.

Mapping to divergent scale:
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) +scale_fill_distiller(limit=c(0,1),type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure32816qe.pdf]]


The divergent scale show that GeoHash is better that the RTree index when the amount of false positives is lower 50% in general.

But when do we have small FPOS Ratio ? 

By grouping queries by queryWidth we can see that FPositive ratio is only a problem on small queries. 
Nevertheless, even on the smallest queries we are able to perform better that the RTree on more than 50% of the cases.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
dfSpeedUp %>% group_by(queryWidth) %>% arrange( speed_up) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up) ) +
    #geom_bar(aes(fill=(FPOS / TotScans)), color="black", stat="identity",width=1) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity") +
    scale_fill_distiller(limit=c(0,1),type="div") +
    facet_wrap(~queryWidth)+ 
    geom_hline(yintercept = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) -> plt
plt
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281VcC.pdf]]

Compute the percentage where PMQ is better than RTree
#+begin_src R :results output  :session 
dfSpeedUp %>% 
    group_by(queryWidth) %>% 
    summarize(tot = length(speed_up), PmqBest = length(speed_up[speed_up>1]), `%` = PmqBest / tot * 100) ->
dfPct 

dfPct
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 8 x 4
  queryWidth   tot PmqBest    `%`
       <dbl> <int>   <int>  <dbl>
1   0.703125    80      37  46.25
2   1.406250    80      44  55.00
3   2.812500    80      67  83.75
4   5.625000    80      80 100.00
5  11.250000    80      80 100.00
6  22.500000    80      80 100.00
7  45.000000    80      80 100.00
8  90.000000    80      80 100.00
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
plt +
    geom_vline(data=dfPct, aes(xintercept = 80-PmqBest)) +  
    geom_text(data=dfPct, aes(x=80-PmqBest + 6, y=2, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281imI.pdf]]

* Final notes 
- The element size doesn't seems to affect the throughput  (in elements / ms). 
  
- PMQ vs BTREE : we always win 

- Bad queries for PMQ VS RTREE
  - queryId = 76 , 78 , 68 , 69

