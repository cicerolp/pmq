# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries - Twitter Dataset
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* DONE Description                                                   :export:

Test the queries on Twitter Dataset and compare the following performances.

- PMQ / GEOHASH
- DENSE Vector + timSort
- BTREE 
- RTREE - quadratic algorithm 
- RTREE - quadratic algorithm with bulk loading

Use the refinement level = 8 

Elements:
- Batch size = 1000
- Variate T = from 1M to 128M
- Variate Element size also = from 16 to 256
 
#+begin_src python :results output :exports both

minitems = 1000
maxitems = 128000

items = minitems
while( items <= maxitems) :
    print(items*1000)
    items *=2
#+end_src

#+RESULTS:
: 1000000
: 2000000
: 4000000
: 8000000
: 16000000
: 32000000
: 64000000
: 128000000

- Total elements = T * 1000  
  
Use the synthetic queries generated by the DoE in [[file:~/Projects/pmq/data/queriesLHS.org::#queries20170923145357][Twitter Data]].

queries Dataset : [[file:~/Projects/pmq/data/queriesTwitter.csv]]


# Results in [[file:exp.pdf]]

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  
  
* DONE Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171016155353

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: [master 79c39e9] LBK: new entry for exp20171016155353
:  1 file changed, 45 insertions(+), 1 deletion(-)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171016155353
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171016155353 08e6c64] Initial commit for exp20171016155353
 1 file changed, 835 insertions(+)
 create mode 100644 data/cicero/exp20171016155353/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 08e6c64 (HEAD -> exp20171016155353) Initial commit for exp20171016155353
: * 79c39e9 (master) LBK: new entry for exp20171016155353
: * d0189bf workaround - performance bugs ?

** DONE Export run script 

#+begin_src sh :results output :exports both

for I in 1 2 4 8 16 32 64 128 ; do
    T=$(($I * 1000))
    echo "$T"
done
#+end_src

#+RESULTS:
: 1000
: 2000
: 4000
: 8000
: 16000
: 32000
: 64000
: 128000

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DELT_SIZE=0 -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=ON -DBENCH_BTREE=ON -DBENCH_RTREE=ON -DBENCH_DENSE=ON -DBENCH_RTREE_BULK=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 

#Run queries
b=1000
#n=$(($t*$b))
ref=8

for EL in 16 32 64 128 256 ; do
    ELTSIZE=$(($EL-16))
    cmake -DELT_SIZE=$ELTSIZE . ; make
    
    for i in 1 2 4 8 16 32 64 128 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dat -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesTwitter.csv >  ${TMPDIR}/bench_queries_region_twitter_${t}_${b}_${ref}_${ELTSIZE}_${EXECID}.log
    done
done

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org
	modified:   run.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	exp.pdf
	exp.tex
	img/

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171012184842 2292431] UPD: run.sh script
:  2 files changed, 2 insertions(+), 2 deletions(-)

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

75 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Fri Oct 13 16:41:34 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 23, done.
(1/20)           remote: Compressing objects:  10% (2/20)           remote: Compressing objects:  15% (3/20)           remote: Compressing objects:  20% (4/20)           remote: Compressing objects:  25% (5/20)           remote: Compressing objects:  30% (6/20)           remote: Compressing objects:  35% (7/20)           remote: Compressing objects:  40% (8/20)           remote: Compressing objects:  45% (9/20)           remote: Compressing objects:  50% (10/20)           remote: Compressing objects:  55% (11/20)           remote: Compressing objects:  60% (12/20)           remote: Compressing objects:  65% (13/20)           remote: Compressing objects:  70% (14/20)           remote: Compressing objects:  75% (15/20)           remote: Compressing objects:  80% (16/20)           remote: Compressing objects:  85% (17/20)           remote: Compressing objects:  90% (18/20)           remote: Compressing objects:  95% (19/20)           remote: Compressing objects: 100% (20/20)           remote: Compressing objects: 100% (20/20), done.        
remote: Total 23 (delta 16), reused 0 (delta 0)
(1/23)   Unpacking objects:   8% (2/23)   Unpacking objects:  13% (3/23)   Unpacking objects:  17% (4/23)   Unpacking objects:  21% (5/23)   Unpacking objects:  26% (6/23)   Unpacking objects:  30% (7/23)   Unpacking objects:  34% (8/23)   Unpacking objects:  39% (9/23)   Unpacking objects:  43% (10/23)   Unpacking objects:  47% (11/23)   Unpacking objects:  52% (12/23)   Unpacking objects:  56% (13/23)   Unpacking objects:  60% (14/23)   Unpacking objects:  65% (15/23)   Unpacking objects:  69% (16/23)   Unpacking objects:  73% (17/23)   Unpacking objects:  78% (18/23)   Unpacking objects:  82% (19/23)   Unpacking objects:  86% (20/23)   Unpacking objects:  91% (21/23)   Unpacking objects:  95% (22/23)   Unpacking objects: 100% (23/23)   Unpacking objects: 100% (23/23), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20171012184842
M	data/cicero/exp20171012184842/run_1507849705
Already on 'exp20171012184842'
Your branch is behind 'origin/exp20171012184842' by 4 commits, and can be fast-forwarded.
  (use "git pull" to update your local branch)
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Updating 6c842e8..2292431
Fast-forward
 benchmarks/bench_queries_region.cpp   |   3 +
 data/cicero/exp20171012184842/exp.org | 724 +++++++++++++++++-----------------
 data/cicero/exp20171012184842/run.sh  |   2 +-
 3 files changed, 376 insertions(+), 353 deletions(-)
commit 229243171c14b0e2c7cc9d9a4b1ffc0d6017cc79
Date:   Fri Oct 13 16:43:23 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ 1507923856

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
: no server running on /tmp/tmux-1001/default
: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
: julio    12367  0.0  0.0  45248  4684 ?        Ss   16:43   0:00 /lib/systemd/sy
: julio    12370  0.0  0.0 145364  2112 ?        S    16:43   0:00 (sd-pam)
: julio    12398  0.0  0.0  97464  3296 ?        R    16:43   0:00 sshd: julio@pts
: julio    12399  0.0  0.0  22688  5132 pts/8    Ss   16:43   0:00 -bash
: julio    12746  0.0  0.0  37368  3296 pts/8    R+   17:11   0:00 ps ux

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
git commit -a -m "wip"
git status
git pull --rebase origin $expId
#+end_src

#+RESULTS:
#+begin_example
[exp20171012184842 37984b2] wip
 1 file changed, 29 insertions(+), 26 deletions(-)
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../exp20170825181747/
	../exp20170830124159/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	../exp20170923144931/
	../exp20170923193058/
	../exp20171009155025/
	exp.pdf
	exp.tex
	img/
	../../queriesLHS.html
	../../queriesLHS_BACKUP_23848.org
	../../queriesLHS_BASE_23848.org
	../../queriesLHS_LOCAL_23848.org
	../../queriesLHS_REMOTE_23848.org
	../../randomLhsQueries.png
	../../../history.txt
	../../../qqqq

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Applying: wip
#+end_example


* DONE Analysis
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tarFile
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tarFile
| log_1508177471.tgz |

#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tarFile
for i in $f; do 
    tar xvzf $i
done
#+end_src

#+RESULTS: logFile
#+begin_example
bench_queries_region_twitter_1000_1000_8_0_1508177471.log
bench_queries_region_twitter_1000_1000_8_112_1508177471.log
bench_queries_region_twitter_1000_1000_8_16_1508177471.log
bench_queries_region_twitter_1000_1000_8_240_1508177471.log
bench_queries_region_twitter_1000_1000_8_48_1508177471.log
bench_queries_region_twitter_128000_1000_8_0_1508177471.log
bench_queries_region_twitter_128000_1000_8_112_1508177471.log
bench_queries_region_twitter_128000_1000_8_16_1508177471.log
bench_queries_region_twitter_128000_1000_8_240_1508177471.log
bench_queries_region_twitter_128000_1000_8_48_1508177471.log
bench_queries_region_twitter_16000_1000_8_0_1508177471.log
bench_queries_region_twitter_16000_1000_8_112_1508177471.log
bench_queries_region_twitter_16000_1000_8_16_1508177471.log
bench_queries_region_twitter_16000_1000_8_240_1508177471.log
bench_queries_region_twitter_16000_1000_8_48_1508177471.log
bench_queries_region_twitter_2000_1000_8_0_1508177471.log
bench_queries_region_twitter_2000_1000_8_112_1508177471.log
bench_queries_region_twitter_2000_1000_8_16_1508177471.log
bench_queries_region_twitter_2000_1000_8_240_1508177471.log
bench_queries_region_twitter_2000_1000_8_48_1508177471.log
bench_queries_region_twitter_32000_1000_8_0_1508177471.log
bench_queries_region_twitter_32000_1000_8_112_1508177471.log
bench_queries_region_twitter_32000_1000_8_16_1508177471.log
bench_queries_region_twitter_32000_1000_8_240_1508177471.log
bench_queries_region_twitter_32000_1000_8_48_1508177471.log
bench_queries_region_twitter_4000_1000_8_0_1508177471.log
bench_queries_region_twitter_4000_1000_8_112_1508177471.log
bench_queries_region_twitter_4000_1000_8_16_1508177471.log
bench_queries_region_twitter_4000_1000_8_240_1508177471.log
bench_queries_region_twitter_4000_1000_8_48_1508177471.log
bench_queries_region_twitter_64000_1000_8_0_1508177471.log
bench_queries_region_twitter_64000_1000_8_112_1508177471.log
bench_queries_region_twitter_64000_1000_8_16_1508177471.log
bench_queries_region_twitter_64000_1000_8_240_1508177471.log
bench_queries_region_twitter_64000_1000_8_48_1508177471.log
bench_queries_region_twitter_8000_1000_8_0_1508177471.log
bench_queries_region_twitter_8000_1000_8_112_1508177471.log
bench_queries_region_twitter_8000_1000_8_16_1508177471.log
bench_queries_region_twitter_8000_1000_8_240_1508177471.log
bench_queries_region_twitter_8000_1000_8_48_1508177471.log
#+end_example

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile
for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
#+begin_example
bench_queries_region_twitter_1000_1000_8_0_1508177471.csv
bench_queries_region_twitter_1000_1000_8_112_1508177471.csv
bench_queries_region_twitter_1000_1000_8_16_1508177471.csv
bench_queries_region_twitter_1000_1000_8_240_1508177471.csv
bench_queries_region_twitter_1000_1000_8_48_1508177471.csv
bench_queries_region_twitter_128000_1000_8_0_1508177471.csv
bench_queries_region_twitter_128000_1000_8_112_1508177471.csv
bench_queries_region_twitter_128000_1000_8_16_1508177471.csv
bench_queries_region_twitter_128000_1000_8_240_1508177471.csv
bench_queries_region_twitter_128000_1000_8_48_1508177471.csv
bench_queries_region_twitter_16000_1000_8_0_1508177471.csv
bench_queries_region_twitter_16000_1000_8_112_1508177471.csv
bench_queries_region_twitter_16000_1000_8_16_1508177471.csv
bench_queries_region_twitter_16000_1000_8_240_1508177471.csv
bench_queries_region_twitter_16000_1000_8_48_1508177471.csv
bench_queries_region_twitter_2000_1000_8_0_1508177471.csv
bench_queries_region_twitter_2000_1000_8_112_1508177471.csv
bench_queries_region_twitter_2000_1000_8_16_1508177471.csv
bench_queries_region_twitter_2000_1000_8_240_1508177471.csv
bench_queries_region_twitter_2000_1000_8_48_1508177471.csv
bench_queries_region_twitter_32000_1000_8_0_1508177471.csv
bench_queries_region_twitter_32000_1000_8_112_1508177471.csv
bench_queries_region_twitter_32000_1000_8_16_1508177471.csv
bench_queries_region_twitter_32000_1000_8_240_1508177471.csv
bench_queries_region_twitter_32000_1000_8_48_1508177471.csv
bench_queries_region_twitter_4000_1000_8_0_1508177471.csv
bench_queries_region_twitter_4000_1000_8_112_1508177471.csv
bench_queries_region_twitter_4000_1000_8_16_1508177471.csv
bench_queries_region_twitter_4000_1000_8_240_1508177471.csv
bench_queries_region_twitter_4000_1000_8_48_1508177471.csv
bench_queries_region_twitter_64000_1000_8_0_1508177471.csv
bench_queries_region_twitter_64000_1000_8_112_1508177471.csv
bench_queries_region_twitter_64000_1000_8_16_1508177471.csv
bench_queries_region_twitter_64000_1000_8_240_1508177471.csv
bench_queries_region_twitter_64000_1000_8_48_1508177471.csv
bench_queries_region_twitter_8000_1000_8_0_1508177471.csv
bench_queries_region_twitter_8000_1000_8_112_1508177471.csv
bench_queries_region_twitter_8000_1000_8_16_1508177471.csv
bench_queries_region_twitter_8000_1000_8_240_1508177471.csv
bench_queries_region_twitter_8000_1000_8_48_1508177471.csv
#+end_example

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

#+LATEX_HEADER:  \usepackage[a4paper,includeheadfoot,margin=2cm]{geometry}
 
*** Prepare

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep=""), col_types="ccicicdcici" ) %>%
         mutate (
            EltSize = as.factor(
                 gsub("bench_queries_region_twitter_[[:digit:]]+_1000_8_([[:digit:]]+)_[[:digit:]]+.csv","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
Loading tidyverse: ggplot2
Loading tidyverse: tibble
Loading tidyverse: tidyr
Loading tidyverse: readr
Loading tidyverse: purrr
Loading tidyverse: dplyr
Conflicts with tidy packages ---------------------------------------------------
filter(): dplyr, stats
lag():    dplyr, stats
 [1] "bench_queries_region_twitter_1000_1000_8_0_1508177471.csv"    
 [2] "bench_queries_region_twitter_1000_1000_8_112_1508177471.csv"  
 [3] "bench_queries_region_twitter_1000_1000_8_16_1508177471.csv"   
 [4] "bench_queries_region_twitter_1000_1000_8_240_1508177471.csv"  
 [5] "bench_queries_region_twitter_1000_1000_8_48_1508177471.csv"   
 [6] "bench_queries_region_twitter_128000_1000_8_0_1508177471.csv"  
 [7] "bench_queries_region_twitter_128000_1000_8_112_1508177471.csv"
 [8] "bench_queries_region_twitter_128000_1000_8_16_1508177471.csv" 
 [9] "bench_queries_region_twitter_128000_1000_8_240_1508177471.csv"
[10] "bench_queries_region_twitter_128000_1000_8_48_1508177471.csv" 
[11] "bench_queries_region_twitter_16000_1000_8_0_1508177471.csv"   
[12] "bench_queries_region_twitter_16000_1000_8_112_1508177471.csv" 
[13] "bench_queries_region_twitter_16000_1000_8_16_1508177471.csv"  
[14] "bench_queries_region_twitter_16000_1000_8_240_1508177471.csv" 
[15] "bench_queries_region_twitter_16000_1000_8_48_1508177471.csv"  
[16] "bench_queries_region_twitter_2000_1000_8_0_1508177471.csv"    
[17] "bench_queries_region_twitter_2000_1000_8_112_1508177471.csv"  
[18] "bench_queries_region_twitter_2000_1000_8_16_1508177471.csv"   
[19] "bench_queries_region_twitter_2000_1000_8_240_1508177471.csv"  
[20] "bench_queries_region_twitter_2000_1000_8_48_1508177471.csv"   
[21] "bench_queries_region_twitter_32000_1000_8_0_1508177471.csv"   
[22] "bench_queries_region_twitter_32000_1000_8_112_1508177471.csv" 
[23] "bench_queries_region_twitter_32000_1000_8_16_1508177471.csv"  
[24] "bench_queries_region_twitter_32000_1000_8_240_1508177471.csv" 
[25] "bench_queries_region_twitter_32000_1000_8_48_1508177471.csv"  
[26] "bench_queries_region_twitter_4000_1000_8_0_1508177471.csv"    
[27] "bench_queries_region_twitter_4000_1000_8_112_1508177471.csv"  
[28] "bench_queries_region_twitter_4000_1000_8_16_1508177471.csv"   
[29] "bench_queries_region_twitter_4000_1000_8_240_1508177471.csv"  
[30] "bench_queries_region_twitter_4000_1000_8_48_1508177471.csv"   
[31] "bench_queries_region_twitter_64000_1000_8_0_1508177471.csv"   
[32] "bench_queries_region_twitter_64000_1000_8_112_1508177471.csv" 
[33] "bench_queries_region_twitter_64000_1000_8_16_1508177471.csv"  
[34] "bench_queries_region_twitter_64000_1000_8_240_1508177471.csv" 
[35] "bench_queries_region_twitter_64000_1000_8_48_1508177471.csv"  
[36] "bench_queries_region_twitter_8000_1000_8_0_1508177471.csv"    
[37] "bench_queries_region_twitter_8000_1000_8_112_1508177471.csv"  
[38] "bench_queries_region_twitter_8000_1000_8_16_1508177471.csv"   
[39] "bench_queries_region_twitter_8000_1000_8_240_1508177471.csv"  
[40] "bench_queries_region_twitter_8000_1000_8_48_1508177471.csv"
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                            file expected   <int> <chr>      <chr>      <chr>                                                           <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................... ........ .............................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_16_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                            file expected   <int> <chr>      <chr>      <chr>                                                           <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................... ........ .............................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_48_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_112_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_240_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_112_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_240_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_0_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_112_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                           file expected   <int> <chr>      <chr>      <chr>                                                          <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_240_1508177471.csv'
... ................. ... .................................................................................................. ........ ...................................................................................... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_0_1508177471.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_112_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_16_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_240_1508177471.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_48_1508177471.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
There were 40 warnings (use warnings() to see them)
#+end_example


Remove useless columns
#+begin_src R :results output :exports both :session 
names(df) <- c("algo" , "V2" , "queryId", "V4", "T", "bench" , "ms" , "V8", "Refine","V10","Count","EltSize")

df <- select(df, -V2, -V4, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 320,000 x 8
            algo queryId     T           bench       ms Refine  Count EltSize
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>  <fctr>
 1 GeoHashBinary       0  1000 apply_at_region 0.419478     29 924827       0
 2 GeoHashBinary       0  1000 apply_at_region 0.410266     29 924827       0
 3 GeoHashBinary       0  1000 apply_at_region 0.408498     29 924827       0
 4 GeoHashBinary       0  1000 apply_at_region 0.408071     29 924827       0
 5 GeoHashBinary       0  1000 apply_at_region 0.407314     29 924827       0
 6 GeoHashBinary       0  1000 apply_at_region 0.408054     29 924827       0
 7 GeoHashBinary       0  1000 apply_at_region 0.407821     29 924827       0
 8 GeoHashBinary       0  1000 apply_at_region 0.407836     29 924827       0
 9 GeoHashBinary       0  1000 apply_at_region 0.406955     29 924827       0
10 GeoHashBinary       0  1000 apply_at_region 0.421066     29 924827       0
# ... with 319,990 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both :session 
df <- 
    df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) %>%
    mutate(queryWidth = 90 / 2**(queryId %/% 10))  %>%   # comput info about query width
    mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> df # adjust the actual size of the elements
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :session :exports both
df 

dfAvg <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfAvg
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 320,000 x 9
            algo queryId     T           bench       ms Refine  Count EltSize queryWidth
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>   <dbl>      <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 0.419478     29 924827      16         90
 2 GeoHashBinary       0  1000 apply_at_region 0.410266     29 924827      16         90
 3 GeoHashBinary       0  1000 apply_at_region 0.408498     29 924827      16         90
 4 GeoHashBinary       0  1000 apply_at_region 0.408071     29 924827      16         90
 5 GeoHashBinary       0  1000 apply_at_region 0.407314     29 924827      16         90
 6 GeoHashBinary       0  1000 apply_at_region 0.408054     29 924827      16         90
 7 GeoHashBinary       0  1000 apply_at_region 0.407821     29 924827      16         90
 8 GeoHashBinary       0  1000 apply_at_region 0.407836     29 924827      16         90
 9 GeoHashBinary       0  1000 apply_at_region 0.406955     29 924827      16         90
10 GeoHashBinary       0  1000 apply_at_region 0.421066     29 924827      16         90
# ... with 319,990 more rows
# A tibble: 32,000 x 10
# Groups:   algo, queryId, T, bench, Refine, Count, EltSize [?]
    algo queryId     T           bench Refine  Count EltSize queryWidth   avg_ms        stdv
   <chr>   <int> <int>           <chr>  <int>  <int>   <dbl>      <dbl>    <dbl>       <dbl>
 1 BTree       0  1000 apply_at_region     27 924827      16         90 10.92556 0.839933454
 2 BTree       0  1000 apply_at_region     27 924827      32         90 10.60835 0.056736491
 3 BTree       0  1000 apply_at_region     27 924827      64         90 10.85630 0.851608473
 4 BTree       0  1000 apply_at_region     27 924827     128         90 10.97631 0.844460953
 5 BTree       0  1000 apply_at_region     27 924827     256         90 11.22784 0.832504941
 6 BTree       0  1000  scan_at_region     29     NA      16         90 17.43356 0.010635078
 7 BTree       0  1000  scan_at_region     29     NA      32         90 17.48158 0.022587843
 8 BTree       0  1000  scan_at_region     29     NA      64         90 17.50776 0.017892407
 9 BTree       0  1000  scan_at_region     29     NA     128         90 17.48583 0.008840569
10 BTree       0  1000  scan_at_region     29     NA     256         90 17.61844 0.032610912
# ... with 31,990 more rows
#+end_example

Set the Count values on scan_at_region lines
#+begin_src R :results output :exports both :session 
dfAvg %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,T,EltSize) %>%
    left_join( 
        filter(ungroup(dfAvg), bench == "scan_at_region") %>% select(-Count)
   ) -> dfCount
dfCount
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "algo", "T", "EltSize")
# A tibble: 16,000 x 10
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms        stdv
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>       <dbl>
 1       0 BTree  924827  1000      16 scan_at_region     29         90 17.43356 0.010635078
 2       0 BTree  924827  1000      32 scan_at_region     29         90 17.48158 0.022587843
 3       0 BTree  924827  1000      64 scan_at_region     29         90 17.50776 0.017892407
 4       0 BTree  924827  1000     128 scan_at_region     29         90 17.48583 0.008840569
 5       0 BTree  924827  1000     256 scan_at_region     29         90 17.61844 0.032610912
 6       0 BTree 1855890  2000      16 scan_at_region     29         90 36.57063 0.040832532
 7       0 BTree 1855890  2000      32 scan_at_region     29         90 36.25142 0.015255221
 8       0 BTree 1855890  2000      64 scan_at_region     29         90 36.48388 0.014496421
 9       0 BTree 1855890  2000     128 scan_at_region     29         90 36.87419 0.016553445
10       0 BTree 1855890  2000     256 scan_at_region     29         90 36.47536 0.031228164
# ... with 15,990 more rows
#+end_example

*** Plot: change of query count with size of T.                    :export:
#+begin_src R :results output graphics :file "./img/count_by_T.png" :exports both :width 600 :height 400 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region") %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId%%10))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryWidth, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:./img/count_by_T.png]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(EltSize == 16) %>%
    filter(algo == "BTree" & bench == "apply_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryId, scale = "free_y", labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980T9Z.pdf]]

*** Plot: Scan Query Time by T facet by queryId                    :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
#    filter(EltSize == 0) %>%
    filter(bench == "scan_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free_y",ncol=10,labeller=label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980ExF.pdf]]

*** Plot: Scan query time by Query Count faceted by QueryId        :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980L42.pdf]]

*** Plot: Scan query throughput by Query Count faceted by QueryId  :export:
:PROPERTIES: 
:HEADER-ARGS:R: :exports results
:END:      

#+begin_src R :results output :exports both :session 

tgpPlot <- function(dfCount){
    dfCount %>% 
        filter(queryWidth == w) %>%
        arrange(Count,T) %>%
        mutate(lbls = paste(Count," (",T/1000,")",sep="")) %>%
        ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +  
                                        #ggplot(aes(x = factor(Count), group=algo, y = Count / avg_ms, color = algo)) +  
        geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
        geom_line() +
        facet_wrap(EltSize~queryId, scale = "free",ncol=10, labeller = label_both) + 
        theme(legend.position = "bottom",
              axis.text.x = element_text(angle = 45, hjust = 1)) +
        labs(x = "Query Count ( Dataset size x 10^{6} )",
             title = paste("Query width = ", w)) 

}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure139809BG.pdf]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot(dfCount)

#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980KMM.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980XWS.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980kgY.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980xqe.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[6]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980-0k.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[7]]

tgpPlot(dfCount)
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980L_q.pdf]]


*** Plot: Element Size influence on Throughput Analysis by QueryId

#+begin_src R :results output :exports both :session 

tgpPlot2 <- function(dfCount){

dfCount %>% 
    filter(queryWidth == w) %>%
    #filter(queryId == 70) %>%
    arrange(T,Count) %>%
    mutate(lbls = paste(T/1000,"M (",Count,")",sep="")) %>%
    mutate(`T (Count)` = factor(lbls,levels=unique(lbls))) %>%
    #ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +
    ggplot(aes(x = factor(EltSize), group=algo, y = Count / avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
    geom_line() +
    facet_wrap(queryId~ `T (Count)` , scale = "fixed", ncol = 8,  labeller = label_both) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = paste("Query width = ", w)) 
}

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w1.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[1]]

tgpPlot2(dfCount)

#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w1.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w2.pdf"  :width 14 :height 40 :session 

levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[2]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w2.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w3.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[3]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w3.pdf]]

#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w4.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[4]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w4.pdf]]


#+begin_src R :results output graphics :file "./img/tgpGridEltsizeQueryId_w5.pdf"  :width 14 :height 40 :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
w = levelsWidth[[5]]

tgpPlot2(dfCount)
#+end_src

#+RESULTS:
[[file:./img/tgpGridEltsizeQueryId_w5.pdf]]

*** Plot: Histogram of Queries Return Size

- Count histogram
- Aggregation histogram

#+begin_src R :results output :exports both :session 
dfHist <- 
dfCount %>% 
#    filter(algo=="BTree") %>%
#    filter(queryWidth == levelsWidth[[3]] ) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) 

#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 800 :height 400 :session 
dfHist %>% 
    ggplot(aes(x = factor(EltsBin) , y = n/length(levels(factor(dfCount$EltSize)))  , fill=(T))) +
    geom_bar(stat="identity") +
    facet_grid(algo~queryWidth, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980OeQ.png]]


We should separate the "Ts"

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 


dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
#    filter(T == 1000) %>% 
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge") +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-139803ef/figure13980o5Q.pdf]]


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
    mutate(EltsBin = Count %/% 100) %>%
    group_by(algo,EltsBin, queryWidth,T) %>%
    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2

dfH2 %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(EltsBin) , y = avgCount / ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black",size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281oNN.pdf]]

**** Plot: Througput barchart by T for a small query 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(EltSize == 16) %>%
#    mutate(EltsBin = Count %/% 100) %>%
#    group_by(algo,EltsBin, queryWidth,T) %>%
#    summarize(n = length(EltsBin), avgCount = mean(Count), ms = mean(avg_ms), std = sd(avg_ms)) -> dfH2
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count) , y = Count / avg_ms, fill=factor(T)) ) +
    geom_bar(stat="identity",position="dodge",color="black", size=0.2) +
    #geom_errorbar(aes(ymin = ms - std , ymax = ms + std) ) +
    facet_wrap(algo~queryWidth, scale="free", nrow = 5) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281O5A.pdf]]

* DONE Analysis of "False positive" ratios

** Running

Running src from master branch.

#+begin_src sh :session teste :results output :exports both 
cd ~/Projects/pmq/build-release/
 
    for i in 1 2 4 8 16 32 64 128 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dmp -x 1 -rate 1000 -min_t ${t} -max_t ${t} -ref 8 -bf ../data/queriesTwitter.csv >> bench_queries_region_twitter_ratio_analysis_1_to_128.log
    done
#+end_src

#+begin_src sh :results output :exports both
cd ~/Projects/pmq/build-release/
git log -1 master --oneline

git add bench_queries_region_twitter_ratio_analysis_1_to_128.log

tail bench_queries_region_twitter_ratio_analysis_1_to_128.log
#+end_src

#+RESULTS:
#+begin_example
a181d8c pmq: return true positives
QueryBench GeoHashBinary ; query ; 76 ; T ; 128000 ; scan_at_region ; 3.10143 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 30670 ; scan_at_region_false_positives ; 243768 ; scan_at_region_true_and_false ; 274438 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 20.4222 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 77 ; T ; 128000 ; apply_at_region ; 26.8968 ; apply_at_region_refinements ; 2 ; count ; 1521534 ; 
QueryBench GeoHashBinary ; query ; 77 ; T ; 128000 ; scan_at_region ; 29.137 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 1.52153e+06 ; scan_at_region_false_positives ; 1.11056e+06 ; scan_at_region_true_and_false ; 2.6321e+06 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 19.7861 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 78 ; T ; 128000 ; apply_at_region ; 31.5211 ; apply_at_region_refinements ; 2 ; count ; 868311 ; 
QueryBench GeoHashBinary ; query ; 78 ; T ; 128000 ; scan_at_region ; 37.4016 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 868311 ; scan_at_region_false_positives ; 2.69751e+06 ; scan_at_region_true_and_false ; 3.56582e+06 ; 
QueryBench GeoHashBinary ; init ; apply_at_region ; 19.6703 ; apply_at_region_refinements ; 252 ; count ; 127999992 ; 
QueryBench GeoHashBinary ; query ; 79 ; T ; 128000 ; apply_at_region ; 9.45635 ; apply_at_region_refinements ; 2 ; count ; 822726 ; 
QueryBench GeoHashBinary ; query ; 79 ; T ; 128000 ; scan_at_region ; 10.5603 ; scan_at_region_refinements ; 2 ; scan_at_region_true_positives ; 822726 ; scan_at_region_false_positives ; 137430 ; scan_at_region_true_and_false ; 960156 ; 
#+end_example


#+begin_src sh :results output :exports both
cp ~/Projects/pmq/build-release/bench_queries_region_twitter_ratio_analysis_1_to_128.log .
git add -f bench_queries_region_twitter_ratio_analysis_1_to_128.log
#+end_src

#+RESULTS:



** Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList="bench_queries_region_twitter_ratio_analysis_1_to_128.log"

for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
: bench_queries_region_twitter_ratio_analysis_1_to_128.csv


** Prepare

Load the CSV into R

Remove useless columns
- Milliseconds and Refine are note relevant in this test
- FPOS FPOS and ALLPOS must be float

#+begin_src R :results output :exports both :var path=(print default-directory) :session
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

f="bench_queries_region_twitter_ratio_analysis_1_to_128.csv"

# Warining: make sure we read the values as doudle because for csv format number issues
df2 <- read_delim(f,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:15),sep=""), col_types="ccicicdcicdcdcd" )
# df2

names(df2) <- c("algo" , "V2" , "queryId", "V4", 
                "T", "bench" , "ms" , "V8","Refine","V10","TPOS","V12","FPOS","V14","ALLPOS")
df2 <- select(df2, -V2, -V4, -V8, -V10, -V12, -V14, -ms, -Refine)
df2

#df2 %>% filter(queryId == 55, T == 16000) 
#+end_src

#+RESULTS:
#+begin_example
Warning: 1280 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' file 2     2  <NA> 15 columns 16 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' col 4     4  <NA> 15 columns 16 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_ratio_analysis_1_to_128.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning message:
In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
# A tibble: 1,280 x 7
            algo queryId     T           bench   TPOS  FPOS ALLPOS
           <chr>   <int> <int>           <chr>  <dbl> <dbl>  <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 924827    NA     NA
 2 GeoHashBinary       0  1000  scan_at_region  30329   798  31127
 3 GeoHashBinary       1  1000 apply_at_region 929918    NA     NA
 4 GeoHashBinary       1  1000  scan_at_region   4293  2357   6650
 5 GeoHashBinary       2  1000 apply_at_region 753921    NA     NA
 6 GeoHashBinary       2  1000  scan_at_region   6017 65222  71239
 7 GeoHashBinary       3  1000 apply_at_region 989228    NA     NA
 8 GeoHashBinary       3  1000  scan_at_region      0     1      1
 9 GeoHashBinary       4  1000 apply_at_region 929320    NA     NA
10 GeoHashBinary       4  1000  scan_at_region   3695  2955   6650
# ... with 1,270 more rows
#+end_example

#+begin_src R :results output :exports both :session 
df2 %>% 
    filter(bench=="scan_at_region") %>%
    select(-algo,-bench) -> dfRatios
    
dfRatios
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 5
   queryId     T  TPOS  FPOS ALLPOS
     <int> <int> <dbl> <dbl>  <dbl>
 1       0  1000 30329   798  31127
 2       1  1000  4293  2357   6650
 3       2  1000  6017 65222  71239
 4       3  1000     0     1      1
 5       4  1000  3695  2955   6650
 6       5  1000 28635  2492  31127
 7       6  1000     0     1      1
 8       7  1000 79127   773  79900
 9       8  1000     1     0      1
10       9  1000 76054  3846  79900
# ... with 630 more rows
#+end_example

Join with dataframe with real performance measurements to get the false-positives
#+begin_src R :results output :exports both :session 
dfCount %>% 
    left_join( dfRatios ) %>%
    mutate(TotScans = ALLPOS + (Count - TPOS)) -> dfCR  # to compute the ratio based on all the elements scanned. 

dfCR
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "T")
# A tibble: 16,000 x 14
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms        stdv  TPOS  FPOS ALLPOS TotScans
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>       <dbl> <dbl> <dbl>  <dbl>    <dbl>
 1       0 BTree  924827  1000      16 scan_at_region     29         90 17.43356 0.010635078 30329   798  31127   925625
 2       0 BTree  924827  1000      32 scan_at_region     29         90 17.48158 0.022587843 30329   798  31127   925625
 3       0 BTree  924827  1000      64 scan_at_region     29         90 17.50776 0.017892407 30329   798  31127   925625
 4       0 BTree  924827  1000     128 scan_at_region     29         90 17.48583 0.008840569 30329   798  31127   925625
 5       0 BTree  924827  1000     256 scan_at_region     29         90 17.61844 0.032610912 30329   798  31127   925625
 6       0 BTree 1855890  2000      16 scan_at_region     29         90 36.57063 0.040832532 60741  1727  62468  1857617
 7       0 BTree 1855890  2000      32 scan_at_region     29         90 36.25142 0.015255221 60741  1727  62468  1857617
 8       0 BTree 1855890  2000      64 scan_at_region     29         90 36.48388 0.014496421 60741  1727  62468  1857617
 9       0 BTree 1855890  2000     128 scan_at_region     29         90 36.87419 0.016553445 60741  1727  62468  1857617
10       0 BTree 1855890  2000     256 scan_at_region     29         90 36.47536 0.031228164 60741  1727  62468  1857617
# ... with 15,990 more rows
#+end_example


+Compute Speedups+ (issue with mismatched count)
#+begin_src R :results output :exports both :session :eval never
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree")) %>%
    filter(EltSize == 16) %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up_RTree  = RTree_avg_ms / GeoHashBinary_avg_ms ) -> dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up_RTree)) %>% arrange(ord)

#+end_src

Compute Speedups
#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree","BTree")) %>%
    filter(EltSize == 16) %>%
#Because the Count diverges for Rtree and PMQ , gathering with count 
    #select(queryId, algo, T, queryWidth, avg_ms, stdv, Count, FPOS, ALLPOS, TotScans ) %>% #print() %>%
    #gather(variable, value, Count, avg_ms, stdv) %>% #print() %>%
    select(queryId, algo, T, queryWidth, avg_ms, stdv,  FPOS, ALLPOS) %>% #print() %>%
    gather(variable, value, avg_ms, stdv) %>% #print() %>%
    unite(temp, algo, variable) %>% #print() %>%
    spread(temp,value) %>%
    mutate(speed_up_RTree  = RTree_avg_ms / GeoHashBinary_avg_ms, speed_up_BTree  = BTree_avg_ms / GeoHashBinary_avg_ms  ) %>% print() %>%
    inner_join( dfCR %>% 
                filter(algo == "GeoHashBinary", EltSize == 16) %>% 
                select(queryId, T, Count, TotScans)) ->  dfSpeedUp 

dfSpeedUp %>% mutate( ord = order(speed_up_RTree)) %>% arrange(ord)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 640 x 13
   queryId      T queryWidth   FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms RTree_stdv speed_up_RTree speed_up_BTree
     <int>  <int>      <dbl>  <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>      <dbl>          <dbl>          <dbl>
 1       0   1000         90    798   31127     17.43356 0.01063508             4.594044        0.008478713     20.79137 0.05379544       4.525723       3.794818
 2       0   2000         90   1727   62468     36.57063 0.04083253             8.516582        0.005368902     42.39427 0.05708750       4.977850       4.294050
 3       0   4000         90   3543  130106     74.03437 0.04002619            17.061550        0.031491913     87.24963 0.06580046       5.113816       4.339252
 4       0   8000         90   7505  264277    152.95590 6.30575808            34.077500        0.025706981    181.07150 0.12255634       5.313521       4.488472
 5       0  16000         90  16719  513521    309.07210 0.08495286            68.352870        0.085200614    375.67220 0.22341083       5.496071       4.521714
 6       0  32000         90  36281 1021940    695.07640 0.67523431           136.258300        0.046745291    764.90430 0.29928249       5.613635       5.101167
 7       0  64000         90  72424 1998820   1678.86400 0.73925187           272.577800        0.035209847   1555.83700 0.46595303       5.707864       6.159210
 8       0 128000         90 149232 3888110   3682.15800 4.60360921           547.478200        0.084006349   3184.72200 1.43768950       5.817075       6.725671
 9       1   1000         90   2357    6650     17.40374 0.01760847             4.461463        0.005444802     20.85763 0.02949689       4.675065       3.900904
10       1   2000         90   4148   12632     36.46787 0.03059394             8.269311        0.023519216     42.53854 0.05618333       5.144146       4.410025
# ... with 630 more rows
Joining, by = c("queryId", "T")
# A tibble: 640 x 16
   queryId      T queryWidth    FPOS  ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv speed_up_RTree speed_up_BTree    Count TotScans   ord
     <int>  <int>      <dbl>   <dbl>   <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>       <dbl>          <dbl>          <dbl>    <int>    <dbl> <int>
 1      48   1000   5.625000   16362   46123    3.8086340 0.336621987            1.1788610       0.0390889221    5.3986460 0.006832459      4.5795442       3.230774   206590   222952     1
 2      56  32000   2.812500  290637  547328   33.5274000 2.337961891            8.6813870       0.0071792433   32.8720300 0.050283221      3.7864952       3.861987  1106318  1396955     2
 3      58 128000   2.812500 1101600 2262700  155.2003000 0.155589238           35.2006700       0.0260767265  127.5946000 0.154166721      3.6247776       4.409016  4464643  5566243     3
 4      63   8000   1.406250   68160  111526    2.4039550 0.310091215            0.8815208       0.0024004233    0.6506137 0.004968984      0.7380583       2.727054    43366   111526     4
 5      66  32000   1.406250  384932 1083070   27.7573000 0.120622552           10.3894700       0.0180665467   19.7171200 0.017889339      1.8977984       2.671676   698138  1083070     5
 6      68  32000   1.406250  700733  949309   22.5063200 0.046844395            7.9824440       0.0041230065    6.8237150 0.013180009      0.8548403       2.819477   248576   949309     6
 7      70  32000   0.703125  720294 1675500   42.4584100 0.106250014           15.5850500       0.0102136347   25.8663600 0.027881384      1.6596905       2.724304   955205  1675500     7
 8      72   1000   0.703125    1454    7951    0.1248068 0.006812684            0.0775408       0.0005726049    0.0870682 0.001104221      1.1228695       1.609563     6497     7951     8
 9      50  64000   2.812500 2383320 4139970  226.2990000 2.251553883           54.8944000       0.0131379011  158.1154000 0.197790686      2.8803557       4.122442  5990017  8373337     9
10      59 128000   2.812500 6010120 8158380  452.1548000 0.139693951          103.6845000       0.0313696598  279.3784000 0.292132466      2.6945050       4.360872 10349418 16359548    10
# ... with 630 more rows
#+end_example

** DONE Analysis                                                    :export:

#+begin_src R :results output :exports both :session 
dfCR %>% 
    filter(EltSize == 16) %>%
    mutate(Ratio = FPOS / TotScans) %>%
    mutate(Ratio2 = FPOS / ALLPOS) %>%
    mutate(tgp = Count / avg_ms) %>%
    #filter(queryWidth < 1) %>%
    filter(algo == "BTree") %>%
    filter(Count != TPOS) %>%
    arrange(Count)

#+end_src

*** Plot : ratios
#+RESULTS:
#+begin_example
# A tibble: 493 x 17
   queryId  algo Count     T EltSize          bench Refine queryWidth    avg_ms         stdv  TPOS  FPOS ALLPOS TotScans        Ratio    Ratio2       tgp
     <int> <chr> <int> <int>   <dbl>          <chr>  <int>      <dbl>     <dbl>        <dbl> <dbl> <dbl>  <dbl>    <dbl>        <dbl>     <dbl>     <dbl>
 1      51 BTree  2377  1000      16 scan_at_region     12     2.8125 0.0935207 0.0050950426   800  5221   6021     7598 0.6871545143 0.8671317 25416.833
 2      51 BTree  4537  2000      16 scan_at_region     12     2.8125 0.2101530 0.0128721513  1503 10271  11774    14808 0.6936115613 0.8723458 21589.033
 3      54 BTree  6164  1000      16 scan_at_region     12     2.8125 0.4925632 0.0012249881  4031 39312  43343    45476 0.8644559768 0.9069977 12514.130
 4      15 BTree  8096  1000      16 scan_at_region      6    45.0000 0.1224318 0.0069989957     0     0      0     8096 0.0000000000       NaN 66126.611
 5      51 BTree  9398  4000      16 scan_at_region     12     2.8125 0.4072375 0.0935481622  3507 21023  24530    30421 0.6910686697 0.8570322 23077.443
 6      42 BTree 11446  1000      16 scan_at_region     23     5.6250 0.2468979 0.0145591697   525  7715   8240    19161 0.4026407808 0.9362864 46359.244
 7      45 BTree 11619  1000      16 scan_at_region     23     5.6250 0.1860315 0.0009233704   698  7542   8240    19161 0.3936120244 0.9152913 62457.165
 8      54 BTree 12970  2000      16 scan_at_region     12     2.8125 1.4501400 0.0451758183  8517 82252  90769    95222 0.8637919808 0.9061684  8943.964
 9      15 BTree 13292  2000      16 scan_at_region      9    45.0000 0.2019838 0.0146983636     0     2      2    13294 0.0001504438 1.0000000 65807.258
10      44 BTree 16350  1000      16 scan_at_region     24     5.6250 0.2060321 0.0008137880  4390  3232   7622    19582 0.1650495353 0.4240357 79356.566
# ... with 483 more rows
#+end_example

#+begin_src R :results output :exports both :session 
levelsWidth = levels(as.factor(dfCount$queryWidth) )
#w = levelsWidth[[1]]
levelsWidth

#+end_src

#+RESULTS:
: [1] "0.703125" "1.40625"  "2.8125"   "5.625"    "11.25"    "22.5"     "45"      
: [8] "90"

#+begin_src R :results output :exports both :session 
plotRatios <- function(dfCR,w){
    dfCR %>% 
    filter(EltSize == 16) %>%
    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(Count), y = Count / avg_ms, fill=(FPOS / TotScans)) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position="dodge",color="black", size =0.2) +
    facet_wrap(~algo, scale="free", ncol = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
   labs(title = paste("Query width = ", w))
}
#+end_src

#+RESULTS:

Facet free 
#+begin_src R :results output graphics :file "./img/tgp-false-positves2.pdf" :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w)
    print(p)
}
#+end_src

#+RESULTS:
[[file:./img/tgp-false-positves2.pdf]]

Facet free_x
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

levelsWidth = levels(as.factor(dfCR$queryWidth) )

for (w in levelsWidth ){
    p <- plotRatios(dfCR, w) + 
    facet_wrap(~algo, scale="free_x", ncol = 1)
    print(p)
}

#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281pOU.pdf]]

*** Speed-Ups of Throughputs                                       :export:

#+begin_src R :results output :exports both :session 
dfCR
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 16,000 x 14
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms        stdv  TPOS  FPOS ALLPOS TotScans
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>       <dbl> <dbl> <dbl>  <dbl>    <dbl>
 1       0 BTree  924827  1000      16 scan_at_region     29         90 17.43356 0.010635078 30329   798  31127   925625
 2       0 BTree  924827  1000      32 scan_at_region     29         90 17.48158 0.022587843 30329   798  31127   925625
 3       0 BTree  924827  1000      64 scan_at_region     29         90 17.50776 0.017892407 30329   798  31127   925625
 4       0 BTree  924827  1000     128 scan_at_region     29         90 17.48583 0.008840569 30329   798  31127   925625
 5       0 BTree  924827  1000     256 scan_at_region     29         90 17.61844 0.032610912 30329   798  31127   925625
 6       0 BTree 1855890  2000      16 scan_at_region     29         90 36.57063 0.040832532 60741  1727  62468  1857617
 7       0 BTree 1855890  2000      32 scan_at_region     29         90 36.25142 0.015255221 60741  1727  62468  1857617
 8       0 BTree 1855890  2000      64 scan_at_region     29         90 36.48388 0.014496421 60741  1727  62468  1857617
 9       0 BTree 1855890  2000     128 scan_at_region     29         90 36.87419 0.016553445 60741  1727  62468  1857617
10       0 BTree 1855890  2000     256 scan_at_region     29         90 36.47536 0.031228164 60741  1727  62468  1857617
# ... with 15,990 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCR %>% 
    filter(algo %in% c("GeoHashBinary","RTree","BTree")) %>%
    filter(EltSize == 16) %>%
                                        #    filter(queryWidth == w) %>%
    ggplot(aes(x = factor(queryId%%10), y = Count / avg_ms, fill=(FPOS / TotScans), group=T) ) +
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_bar(stat="identity",position=position_dodge(),color="black", size =0.2) +
    geom_errorbar(aes(ymin = Count / (avg_ms-stdv), ymax = Count / (avg_ms + stdv)), position=position_dodge())+ 
    facet_grid(queryWidth~algo, scale="free") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
                                        #  labs(title = paste("Query width = ", w))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:/tmp/babel-23558m-M/figure23558WAG.pdf]]


*** Speedup - correlations                                         :export:
:PROPERTIES:
:HEADER-ARGS:R: :exports result
:END:


- Order the results by speedup

- try several mapping to a continuos color map

Count
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(Count)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure32817kx.pdf]]

QueryWidth
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=factor(queryWidth)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure32816c2.pdf]]

Ratio : FPOS / ALLPOS
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / ALLPOS)), stat="identity",position="dodge") + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281smF.pdf]]

Ratio : FPOS / TotScans
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + #  scale_fill_distiller(type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-15175dlJ/figure15175n7O.pdf]]


The color scale show that the speed-up increases according to ratio of False positives.

Mapping to divergent scale:
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
    dfSpeedUp %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) +scale_fill_distiller(limit=c(0,1),type="div") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure32816qe.pdf]]


The divergent scale show that GeoHash is better that the RTree index when the amount of false positives is lower 50% in general.

But when do we have small FPOS Ratio ? 

By grouping queries by queryWidth we can see that FPositive ratio is only a problem on small queries. 
Nevertheless, even on the smallest queries we are able to perform better that the RTree on more than 50% of the cases.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
dfSpeedUp %>% group_by(queryWidth) %>% arrange( speed_up_RTree) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = speed_up_RTree) ) +
    #geom_bar(aes(fill=(FPOS / TotScans)), color="black", stat="identity",width=1) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity") +
    scale_fill_distiller(limit=c(0,1),type="div") +
    facet_wrap(~queryWidth)+ 
    geom_hline(yintercept = 1) + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) -> plt
plt
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281VcC.pdf]]

Compute the percentage where PMQ is better than RTree
#+begin_src R :results output  :session 
dfSpeedUp %>% 
    group_by(queryWidth) %>% 
    summarize(tot = length(speed_up_RTree), PmqBest = length(speed_up_RTree[speed_up_RTree>1]), `%` = PmqBest / tot * 100) ->
dfPct 

dfPct
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 8 x 4
  queryWidth   tot PmqBest    `%`
       <dbl> <int>   <int>  <dbl>
1   0.703125    80      37  46.25
2   1.406250    80      44  55.00
3   2.812500    80      67  83.75
4   5.625000    80      80 100.00
5  11.250000    80      80 100.00
6  22.500000    80      80 100.00
7  45.000000    80      80 100.00
8  90.000000    80      80 100.00
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf")  :width 14 :height 10 :session 
plt +
    geom_vline(data=dfPct, aes(xintercept = 80-PmqBest)) +  
    geom_text(data=dfPct, aes(x=80-PmqBest + 6, y=2, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3281Prz/figure3281imI.pdf]]


*** Speedup comparison with BTree and RTree                        :export:
:PROPERTIES:
:EXPORT_FILE_NAME: speedup_comparison_BTree_RTree
:HEADER-ARGS:R: :exports results
:END:

These results show the query's throughput speed-up for PMQ over BTree and RTree.

#+begin_src R :results output :exports none :session 
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 15
   queryId      T queryWidth   FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms RTree_stdv     Count  TotScans   algo_speedup val_speedup
     <int>  <int>      <dbl>  <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>      <dbl>     <int>     <dbl>          <chr>       <dbl>
 1       0   1000         90    798   31127     17.43356 0.01063508             4.594044        0.008478713     20.79137 0.05379544    924827    925625 speed_up_RTree    4.525723
 2       0   2000         90   1727   62468     36.57063 0.04083253             8.516582        0.005368902     42.39427 0.05708750   1855890   1857617 speed_up_RTree    4.977850
 3       0   4000         90   3543  130106     74.03437 0.04002619            17.061550        0.031491913     87.24963 0.06580046   3706387   3709930 speed_up_RTree    5.113816
 4       0   8000         90   7505  264277    152.95590 6.30575808            34.077500        0.025706981    181.07150 0.12255634   7417949   7425454 speed_up_RTree    5.313521
 5       0  16000         90  16719  513521    309.07210 0.08495286            68.352870        0.085200614    375.67220 0.22341083  14876686  14893405 speed_up_RTree    5.496071
 6       0  32000         90  36281 1021940    695.07640 0.67523431           136.258300        0.046745291    764.90430 0.29928249  29764961  29801243 speed_up_RTree    5.613635
 7       0  64000         90  72424 1998820   1678.86400 0.73925187           272.577800        0.035209847   1555.83700 0.46595303  59715461  59787881 speed_up_RTree    5.707864
 8       0 128000         90 149232 3888110   3682.15800 4.60360921           547.478200        0.084006349   3184.72200 1.43768950 119931295 120080525 speed_up_RTree    5.817075
 9       1   1000         90   2357    6650     17.40374 0.01760847             4.461463        0.005444802     20.85763 0.02949689    929918    932275 speed_up_RTree    4.675065
10       1   2000         90   4148   12632     36.46787 0.03059394             8.269311        0.023519216     42.53854 0.05618333   1866101   1870249 speed_up_RTree    5.144146
# ... with 1,270 more rows
#+end_example

All the results were sorted by increasing speedUp value. 
The colors show the ratio between amount of false positive elements over the total amount of elements scanned by the query. 
#+begin_src R :results output graphics :file "./img/speedup_BTREE_RTREE.svg"  :width 14 :height 10 :session 
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree) %>%
    arrange( val_speedup) %>% group_by(algo_speedup) %>% mutate(ord = row_number()) %>% 
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(limits=c(0,1),type="div") +
    geom_hline(yintercept = 1) + 
    facet_grid(~algo_speedup) +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_BTREE_RTREE.svg]]

#+CAPTION: Compute the percentages 
#+begin_src R :results output :session :exports none
dfSpeedUp %>% 
    gather(algo_speedup,val_speedup,speed_up_RTree,speed_up_BTree) %>%
    arrange( val_speedup) %>% 
    group_by(algo_speedup,queryWidth) %>% 
    mutate(ord = row_number()) -> dfSpeedUp2

dfSpeedUp2

dfSpeedUp2 %>% 
    summarize(tot = length(val_speedup), PmqBest = length( val_speedup[val_speedup>1]), `%` = PmqBest / tot * 100) ->
dfPct2 

dfPct2
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 16
# Groups:   algo_speedup, queryWidth [16]
   queryId      T queryWidth   FPOS ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms   RTree_stdv Count TotScans   algo_speedup val_speedup   ord
     <int>  <int>      <dbl>  <dbl>  <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>        <dbl> <int>    <dbl>          <chr>       <dbl> <int>
 1      76 128000   0.703125 243768 274438    6.7264120 0.059425979            2.2655230       0.0065489983    0.4087136 0.0009851632 30670   274438 speed_up_RTree   0.1804058     1
 2      76   8000   0.703125  14409  16086    0.2450226 0.014603683            0.1235638       0.0015787749    0.0254306 0.0034877059  1677    16086 speed_up_RTree   0.2058095     2
 3      54   4000   2.812500 165087 183103    3.5871080 0.013635533            1.5248560       0.0052561735    0.3208266 0.0012677555 26914   192001 speed_up_RTree   0.2103980     1
 4      76  16000   0.703125  27147  30395    0.3594563 0.001228705            0.2337451       0.0038910547    0.0502536 0.0039408447  3248    30395 speed_up_RTree   0.2149932     3
 5      76  64000   0.703125 115063 129740    2.3170460 0.028394162            1.0357000       0.0399796543    0.2253266 0.0371831162 14677   129740 speed_up_RTree   0.2175597     4
 6      76   4000   0.703125   7193   7970    0.1060996 0.004819485            0.0633969       0.0015886084    0.0140851 0.0023626772   777     7970 speed_up_RTree   0.2221733     5
 7      54   2000   2.812500  82252  90769    1.4501400 0.045175818            0.6684977       0.0014885204    0.1644758 0.0215614793 12970    95222 speed_up_RTree   0.2460379     2
 8      74   4000   0.703125 103847 124438    2.3150610 0.321853102            0.9475128       0.0045015212    0.2519637 0.0273541813 20591   124438 speed_up_RTree   0.2659212     6
 9      74   2000   0.703125  49381  59968    0.7117453 0.003806669            0.4325339       0.0019736333    0.1163358 0.0012908473 10587    59968 speed_up_RTree   0.2689634     7
10      76  32000   0.703125  54424  61633    0.7685108 0.013016140            0.4521312       0.0005826477    0.1249283 0.0084581335  7209    61633 speed_up_RTree   0.2763098     8
# ... with 1,270 more rows
# A tibble: 16 x 5
# Groups:   algo_speedup [?]
     algo_speedup queryWidth   tot PmqBest    `%`
            <chr>      <dbl> <int>   <int>  <dbl>
 1 speed_up_BTree   0.703125    80      80 100.00
 2 speed_up_BTree   1.406250    80      80 100.00
 3 speed_up_BTree   2.812500    80      80 100.00
 4 speed_up_BTree   5.625000    80      80 100.00
 5 speed_up_BTree  11.250000    80      80 100.00
 6 speed_up_BTree  22.500000    80      80 100.00
 7 speed_up_BTree  45.000000    80      80 100.00
 8 speed_up_BTree  90.000000    80      80 100.00
 9 speed_up_RTree   0.703125    80      37  46.25
10 speed_up_RTree   1.406250    80      44  55.00
11 speed_up_RTree   2.812500    80      67  83.75
12 speed_up_RTree   5.625000    80      80 100.00
13 speed_up_RTree  11.250000    80      80 100.00
14 speed_up_RTree  22.500000    80      80 100.00
15 speed_up_RTree  45.000000    80      80 100.00
16 speed_up_RTree  90.000000    80      80 100.00
#+end_example

We can see a correlation between the amount false positives and the Speedup over *RTrees* on small queries. 
This is due to the discontinuities caused by the Z-ordering scheme, used in the GeoHash algorithm. 
We can see that this correlation doesn't appear in the *BTree* speedups.
#+begin_src R :results output graphics :file "./img/speedup_BTREE_RTREE_facet.svg"  :width 14 :height 10 :session 

dfSpeedUp2 %>%
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=(FPOS / TotScans)), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(limits=c(0,1),type="div") +
    facet_grid(queryWidth~algo_speedup) +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))

#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_BTREE_RTREE_facet.svg]]


We normalized the element count within each querySize to check it the BTree's Speedup increases with the number os elements queried.
# +However, there is not a clear correlation to explain increase of speedup for the BTree case.+
#+begin_src R :results output :exports none :session 
dfSpeedUp2 %>% mutate(max = max(Count)) %>%
    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count)))  %>% arrange(algo_speedup,queryWidth,-ord)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 1,280 x 18
# Groups:   algo_speedup, queryWidth [16]
   queryId      T queryWidth    FPOS  ALLPOS BTree_avg_ms BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms RTree_stdv   Count TotScans   algo_speedup val_speedup   ord     max  normCount
     <int>  <int>      <dbl>   <dbl>   <dbl>        <dbl>      <dbl>                <dbl>              <dbl>        <dbl>      <dbl>   <int>    <dbl>          <chr>       <dbl> <int>   <dbl>      <dbl>
 1      74 128000   0.703125 3212550 3810030    108.06000 0.18138970            30.478030        0.017599814    18.065090 0.03650612  597480  3810030 speed_up_BTree    3.545505    80 3682101 0.16220075
 2      78 128000   0.703125 2697510 3565820    103.05640 1.36977786            29.559790        0.020411514    23.361700 0.02374316  868311  3565820 speed_up_BTree    3.486371    79 3682101 0.23575987
 3      73 128000   0.703125 3330990 6079850    183.66040 0.10845706            54.026540        0.028831472    78.458460 0.07448939 2748860  6079850 speed_up_BTree    3.399448    78 3682101 0.74652685
 4      70 128000   0.703125 2888540 6570640    202.28340 0.12448936            60.869830        0.015346230    97.171030 0.09411546 3682101  6570641 speed_up_BTree    3.323213    77 3682101 1.00000000
 5      75 128000   0.703125 1436760 4270330    131.85610 0.20509589            40.597530        0.041563580    76.066900 0.08953893 2833578  4270328 speed_up_BTree    3.247885    76 3682101 0.76953670
 6      74  64000   0.703125 1610240 1918290     49.79501 0.24653883            15.379660        0.007778203     9.592362 0.01510888  308049  1918290 speed_up_BTree    3.237719    75 3682101 0.08358977
 7      77 128000   0.703125 1110560 2632100     79.14142 0.22104596            24.627350        0.017682776    41.078850 0.04656957 1521534  2632104 speed_up_BTree    3.213558    74 3682101 0.41317867
 8      71 128000   0.703125  502071  663138     17.84301 0.03295001             5.598997        0.014980574     4.340049 0.01827141  161067   663138 speed_up_BTree    3.186823    73 3682101 0.04366869
 9      78  64000   0.703125 1348500 1792150     47.20611 0.22410900            14.905460        0.007824917    12.096510 0.01356904  443642  1792150 speed_up_BTree    3.167035    72 3682101 0.12041754
10      73  64000   0.703125 1708650 3087000     86.64781 0.19097865            27.365440        0.021003926    39.678410 0.06132791 1378343  3087003 speed_up_BTree    3.166323    71 3682101 0.37428724
# ... with 1,270 more rows
#+end_example

The color scale is normalized between the min and max count of the elements count in each facet.
Note that Count increases as power of 2 . Therefore we take the logarithm to map it to linear color scale. (see Extra sections on org file)
#+begin_src R :results output graphics :file "./img/speedup_color_count.svg" :exports both :width 14 :height 10 :session 
dfSpeedUp2 %>%
    mutate(Count = log2(Count)) %>%
    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count))) %>%
    ggplot(aes(x = ord, y = val_speedup) ) +
    geom_bar(aes(fill=normCount), stat="identity",position="dodge",width=1) + 
    scale_fill_distiller(type="seq") +
    facet_grid(queryWidth~algo_speedup) +
    labs(fill="Normalized log2(Count)") + 
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))
#+end_src

#+ATTR_SRC: :width 600
#+RESULTS:
[[file:./img/speedup_color_count.svg]]

**** EXTRA: verify if relation between SpeedUp and Ratio is linear. :noexport:

To verify that SpeedUp linearly decreases with SpeedUp we plot orderded by ratio.
Although for RTree is not a clear straight line , there is a neat inverse relation between SpeedUp and False Positive ratio. 
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session 
dfSpeedUp2 %>% filter(queryWidth < 2) %>%
    ggplot(aes(x = FPOS/TotScans, y = val_speedup) ) +
    geom_line() + geom_point() + 
    facet_wrap(queryWidth~algo_speedup,scale="free",ncol=2) +
    theme(legend.position = "bottom",
#          strip.text = element_blank() , 
          axis.text.x = element_text(angle = 0, hjust = 1)) 
#+end_src

#+RESULTS:
[[file:/tmp/babel-24355lQN/figure24355KKO.png]]

**** EXTRA: Logarithmic interpolation of colors ?               :noexport:

Note that the the dataset doubles the size at each experiments. 
The counts should double in the same way.

So to have a linear X scale we must take the log of Count on x axis.
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".svg") :exports both :width 14 :height 10 :session 
dfSpeedUp2 %>%
#    mutate(Count = log2(Count)) %>%
#    mutate(normCount = (Count - min(Count)) / ( max(Count) - min(Count))) %>%
    ggplot(aes(x = log2(Count), y = val_speedup) ) +
#    geom_bar(aes(fill=normCount), stat="identity",position="dodge",width=1) + 
    geom_line() + geom_point() + 
#    scale_fill_distiller(type="seq") +
    facet_wrap(queryWidth~algo_speedup,scale="free_y",ncol=2) +
    theme(legend.position = "bottom",
          strip.text = element_blank() , 
          axis.text.x = element_text(angle = 0, hjust = 1)) 
#    geom_hline(yintercept = 1, size = 0.5, linetype="dashed") +
#    geom_vline(data=dfPct2, aes(xintercept = 80-PmqBest+0.5), size = 0.5 , linetype="dashed") +
#    geom_text(data=dfPct2, aes(x=80-PmqBest + 4.5, y=4, label = paste(`%`,"%")))
#+end_src

#+RESULTS:
[[file:/tmp/babel-24355lQN/figure24355L9s.svg]]


#+begin_src R :results output :exports both :session 
s = 2**seq(1,10)
si = (s - min(s)) / (max(s) - min(s))
s
si

log2(s)
#+end_src

#+RESULTS:
:  [1]    2    4    8   16   32   64  128  256  512 1024
:  [1] 0.000000000 0.001956947 0.005870841 0.013698630 0.029354207 0.060665362
:  [7] 0.123287671 0.248532290 0.499021526 1.000000000
:  [1]  1  2  3  4  5  6  7  8  9 10

* Final notes 
- The element size doesn't seems to affect the throughput  (in elements / ms). 
  
- PMQ vs BTREE : we always win 

- Bad queries for PMQ VS RTREE
  - queryId = 76 , 78 , 68 , 69

