# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Experiment Diary
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4
#+PROPERTY: header-args :cache no :eval no-export 


* Description 
Test to check better refinemnet levels. 

- PMQ / GEOHASH
- BTREE 
  
- test with both datasets

** TODO Standalone script 
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  

* Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20170904153555

Set up git branch
#+begin_src sh :results output :exports both
git checkout master
#+end_src

#+RESULTS:
: M	LabBook.org

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:
: M	LabBook.org

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20170904153555
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20170904153555 3b07765] Initial commit for exp20170904153555
 1 file changed, 558 insertions(+)
 create mode 100644 data/cicero/exp20170904153555/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 3b07765 (HEAD -> exp20170904153555) Initial commit for exp20170904153555
: * 5409ff9 (origin/master, master) minor fix
: * d574560 fix main

** Export run script 

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=ON -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DCMAKE_BUILD_TYPE="Release" ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 
# Queries insert remove count
t=10000
b=100
#n=$(($t*$b))
#ref=8
listRef=$(seq 1 14)
for ref in $listRef
do
stdbuf -oL ./benchmarks/bench_queries_region -seed 123 -rate 100 -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesLHS.csv >  ${TMPDIR}/bench_queries_region_random_${t}_${b}_${ref}_${EXECID}.log
done 

# stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dmp  -rate 100 -x 10 -rate $b -min_t $t -max_t $t -ref $ref -bf ../data/queriesLHS.csv >  $TMPDIR/bench_queries_region_twitter_$t_$b_$EXECID.log

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
: On branch exp20170904153555
: Changes not staged for commit:
:   (use "git add <file>..." to update what will be committed)
:   (use "git checkout -- <file>..." to discard changes in working directory)
: 
: 	modified:   exp.org
: 
: no changes added to commit (use "git add" and/or "git commit -a")

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20170904153555 fffe070] UPD: run.sh script
:  1 file changed, 4 insertions(+), 5 deletions(-)

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** DONE Local Execution                                              :local:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

25 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Wed Aug 30 12:25:44 2017 from 143.54.13.218
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 132, done.
(1/128)           remote: Compressing objects:   1% (2/128)           remote: Compressing objects:   2% (3/128)           remote: Compressing objects:   3% (4/128)           remote: Compressing objects:   4% (6/128)           remote: Compressing objects:   5% (7/128)           remote: Compressing objects:   6% (8/128)           remote: Compressing objects:   7% (9/128)           remote: Compressing objects:   8% (11/128)           remote: Compressing objects:   9% (12/128)           remote: Compressing objects:  10% (13/128)           remote: Compressing objects:  11% (15/128)           remote: Compressing objects:  12% (16/128)           remote: Compressing objects:  13% (17/128)           remote: Compressing objects:  14% (18/128)           remote: Compressing objects:  15% (20/128)           remote: Compressing objects:  16% (21/128)           remote: Compressing objects:  17% (22/128)           remote: Compressing objects:  18% (24/128)           remote: Compressing objects:  19% (25/128)           remote: Compressing objects:  20% (26/128)           remote: Compressing objects:  21% (27/128)           remote: Compressing objects:  22% (29/128)           remote: Compressing objects:  23% (30/128)           remote: Compressing objects:  24% (31/128)           remote: Compressing objects:  25% (32/128)           remote: Compressing objects:  26% (34/128)           remote: Compressing objects:  27% (35/128)           remote: Compressing objects:  28% (36/128)           remote: Compressing objects:  29% (38/128)           remote: Compressing objects:  30% (39/128)           remote: Compressing objects:  31% (40/128)           remote: Compressing objects:  32% (41/128)           remote: Compressing objects:  33% (43/128)           remote: Compressing objects:  34% (44/128)           remote: Compressing objects:  35% (45/128)           remote: Compressing objects:  36% (47/128)           remote: Compressing objects:  37% (48/128)           remote: Compressing objects:  38% (49/128)           remote: Compressing objects:  39% (50/128)           remote: Compressing objects:  40% (52/128)           remote: Compressing objects:  41% (53/128)           remote: Compressing objects:  42% (54/128)           remote: Compressing objects:  43% (56/128)           remote: Compressing objects:  44% (57/128)           remote: Compressing objects:  45% (58/128)           remote: Compressing objects:  46% (59/128)           remote: Compressing objects:  47% (61/128)           remote: Compressing objects:  48% (62/128)           remote: Compressing objects:  49% (63/128)           remote: Compressing objects:  50% (64/128)           remote: Compressing objects:  51% (66/128)           remote: Compressing objects:  52% (67/128)           remote: Compressing objects:  53% (68/128)           remote: Compressing objects:  54% (70/128)           remote: Compressing objects:  55% (71/128)           remote: Compressing objects:  56% (72/128)           remote: Compressing objects:  57% (73/128)           remote: Compressing objects:  58% (75/128)           remote: Compressing objects:  59% (76/128)           remote: Compressing objects:  60% (77/128)           remote: Compressing objects:  61% (79/128)           remote: Compressing objects:  62% (80/128)           remote: Compressing objects:  63% (81/128)           remote: Compressing objects:  64% (82/128)           remote: Compressing objects:  65% (84/128)           remote: Compressing objects:  66% (85/128)           remote: Compressing objects:  67% (86/128)           remote: Compressing objects:  68% (88/128)           remote: Compressing objects:  69% (89/128)           remote: Compressing objects:  70% (90/128)           remote: Compressing objects:  71% (91/128)           remote: Compressing objects:  72% (93/128)           remote: Compressing objects:  73% (94/128)           remote: Compressing objects:  74% (95/128)           remote: Compressing objects:  75% (96/128)           remote: Compressing objects:  76% (98/128)           remote: Compressing objects:  77% (99/128)           remote: Compressing objects:  78% (100/128)           remote: Compressing objects:  79% (102/128)           remote: Compressing objects:  80% (103/128)           remote: Compressing objects:  81% (104/128)           remote: Compressing objects:  82% (105/128)           remote: Compressing objects:  83% (107/128)           remote: Compressing objects:  84% (108/128)           remote: Compressing objects:  85% (109/128)           remote: Compressing objects:  86% (111/128)           remote: Compressing objects:  87% (112/128)           remote: Compressing objects:  88% (113/128)           remote: Compressing objects:  89% (114/128)           remote: Compressing objects:  90% (116/128)           remote: Compressing objects:  91% (117/128)           remote: Compressing objects:  92% (118/128)           remote: Compressing objects:  93% (120/128)           remote: Compressing objects:  94% (121/128)           remote: Compressing objects:  95% (122/128)           remote: Compressing objects:  96% (123/128)           remote: Compressing objects:  97% (125/128)           remote: Compressing objects:  98% (126/128)           remote: Compressing objects:  99% (127/128)           remote: Compressing objects: 100% (128/128)           remote: Compressing objects: 100% (128/128), done.
(1/132)   Receiving objects:   1% (2/132)   Receiving objects:   2% (3/132)   Receiving objects:   3% (4/132)   Receiving objects:   4% (6/132)   Receiving objects:   5% (7/132)   Receiving objects:   6% (8/132)   Receiving objects:   7% (10/132)   Receiving objects:   8% (11/132)   Receiving objects:   9% (12/132)   Receiving objects:  10% (14/132)   Receiving objects:  11% (15/132)   Receiving objects:  12% (16/132)   Receiving objects:  13% (18/132)   Receiving objects:  14% (19/132)   Receiving objects:  15% (20/132)   Receiving objects:  16% (22/132)   Receiving objects:  17% (23/132)   Receiving objects:  18% (24/132)   Receiving objects:  19% (26/132)   Receiving objects:  20% (27/132)   Receiving objects:  21% (28/132)   Receiving objects:  22% (30/132)   Receiving objects:  23% (31/132)   Receiving objects:  24% (32/132)   Receiving objects:  25% (33/132)   Receiving objects:  26% (35/132)   Receiving objects:  27% (36/132)   Receiving objects:  28% (37/132)   Receiving objects:  29% (39/132)   Receiving objects:  30% (40/132)   Receiving objects:  31% (41/132)   Receiving objects:  32% (43/132)   Receiving objects:  33% (44/132)   Receiving objects:  34% (45/132)   Receiving objects:  35% (47/132)   Receiving objects:  36% (48/132)   remote: Total 132 (delta 97), reused 0 (delta 0)
(49/132)   Receiving objects:  38% (51/132)   Receiving objects:  39% (52/132)   Receiving objects:  40% (53/132)   Receiving objects:  41% (55/132)   Receiving objects:  42% (56/132)   Receiving objects:  43% (57/132)   Receiving objects:  44% (59/132)   Receiving objects:  45% (60/132)   Receiving objects:  46% (61/132)   Receiving objects:  47% (63/132)   Receiving objects:  48% (64/132)   Receiving objects:  49% (65/132)   Receiving objects:  50% (66/132)   Receiving objects:  51% (68/132)   Receiving objects:  52% (69/132)   Receiving objects:  53% (70/132)   Receiving objects:  54% (72/132)   Receiving objects:  55% (73/132)   Receiving objects:  56% (74/132)   Receiving objects:  57% (76/132)   Receiving objects:  58% (77/132)   Receiving objects:  59% (78/132)   Receiving objects:  60% (80/132)   Receiving objects:  61% (81/132)   Receiving objects:  62% (82/132)   Receiving objects:  63% (84/132)   Receiving objects:  64% (85/132)   Receiving objects:  65% (86/132)   Receiving objects:  66% (88/132)   Receiving objects:  67% (89/132)   Receiving objects:  68% (90/132)   Receiving objects:  69% (92/132)   Receiving objects:  70% (93/132)   Receiving objects:  71% (94/132)   Receiving objects:  72% (96/132)   Receiving objects:  73% (97/132)   Receiving objects:  74% (98/132)   Receiving objects:  75% (99/132)   Receiving objects:  76% (101/132)   Receiving objects:  77% (102/132)   Receiving objects:  78% (103/132)   Receiving objects:  79% (105/132)   Receiving objects:  80% (106/132)   Receiving objects:  81% (107/132)   Receiving objects:  82% (109/132)   Receiving objects:  83% (110/132)   Receiving objects:  84% (111/132)   Receiving objects:  85% (113/132)   Receiving objects:  86% (114/132)   Receiving objects:  87% (115/132)   Receiving objects:  88% (117/132)   Receiving objects:  89% (118/132)   Receiving objects:  90% (119/132)   Receiving objects:  91% (121/132)   Receiving objects:  92% (122/132)   Receiving objects:  93% (123/132)   Receiving objects:  94% (125/132)   Receiving objects:  95% (126/132)   Receiving objects:  96% (127/132)   Receiving objects:  97% (129/132)   Receiving objects:  98% (130/132)   Receiving objects:  99% (131/132)   Receiving objects: 100% (132/132)   Receiving objects: 100% (132/132), 25.59 KiB | 0 bytes/s, done.
(0/97)   Resolving deltas:  13% (13/97)   Resolving deltas:  16% (16/97)   Resolving deltas:  24% (24/97)   Resolving deltas:  25% (25/97)   Resolving deltas:  43% (42/97)   Resolving deltas:  44% (43/97)   Resolving deltas:  50% (49/97)   Resolving deltas:  56% (55/97)   Resolving deltas:  59% (58/97)   Resolving deltas:  63% (62/97)   Resolving deltas:  65% (64/97)   Resolving deltas:  69% (67/97)   Resolving deltas:  70% (68/97)   Resolving deltas:  80% (78/97)   Resolving deltas:  85% (83/97)   Resolving deltas:  88% (86/97)   Resolving deltas:  89% (87/97)   Resolving deltas:  93% (91/97)   Resolving deltas:  97% (95/97)   Resolving deltas: 100% (97/97)   Resolving deltas: 100% (97/97), completed with 16 local objects.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20170904153555
Branch exp20170904153555 set up to track remote branch exp20170904153555 from origin.
Switched to a new branch 'exp20170904153555'
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Already up-to-date.
commit fffe0707dff5c630de0017a454dc46025231d18d
Date:   Mon Sep 4 16:36:37 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 011775f5fdeaeeff330da7df39751d9c5323b570
: Date:   Mon Feb 13 12:20:46 2017 -0200
: 
:     Bugfix: corrected pointer casts

*** Execute Remotely                                               :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20170904153555$ julio@cicero:~/Projects/pmq/data/cicero/exp20170904153555$ julio@cicero:~/Projects/pmq/data/cicero/exp20170904153555$ julio@cicero:~/Projects/pmq/data/cicero/exp20170904153555$ 1504554073

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
#+begin_example
runExp: 1 windows (created Mon Sep  4 16:41:13 2017) [80x23]
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
julio    11347  0.0  0.0  45248  4640 ?        Ss   16:36   0:00 /lib/systemd/sy
julio    11348  0.0  0.0 145408  2160 ?        S    16:36   0:00 (sd-pam)
julio    11438  0.0  0.0  97464  3336 ?        R    16:36   0:00 sshd: julio@pts
julio    11439  0.0  0.0  22764  5244 pts/18   Ss   16:36   0:00 -bash
julio    12153  0.0  0.0  29420  2888 ?        Ss   16:41   0:00 tmux new -d -s 
julio    12154  0.0  0.0  12532  3028 pts/19   Ss+  16:41   0:00 bash -c cd ~/Pr
julio    12156  0.0  0.0  12544  2972 pts/19   S+   16:41   0:00 /bin/bash ./run
julio    12383  115  0.4 303148 158064 pts/19  R+   16:41   0:03 ./benchmarks/be
julio    12385  0.0  0.0  37368  3288 pts/18   R+   16:41   0:00 ps ux
#+end_example

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
#git commit -a -m "wip"
git status
git pull origin $expId
#+end_src

#+RESULTS:
#+begin_example
On branch exp20170904153555
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   ../../../LabBook.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.org.bkp
	../../../build-debug/
	../exp20170830124159/
	../exp20170904152622/
	.#exp.org
	../../../include/types.h.orig

no changes added to commit (use "git add" and/or "git commit -a")
Updating fffe070..656ab64
Fast-forward
 data/cicero/exp20170904153555/info.org           | 696 +++++++++++++++++++++++
 data/cicero/exp20170904153555/log_1504554073.tgz | Bin 0 -> 275926 bytes
 data/cicero/exp20170904153555/run_1504554073     |  55 ++
 3 files changed, 751 insertions(+)
 create mode 100644 data/cicero/exp20170904153555/info.org
 create mode 100644 data/cicero/exp20170904153555/log_1504554073.tgz
 create mode 100644 data/cicero/exp20170904153555/run_1504554073
#+end_example


* Analisys
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+begin_src sh :results table :exports both
ls -htl *tgz
#+end_src

#+RESULTS:
| -rw-rw-r-- 1 julio julio 270K Set  6 19:32 log_1504554073.tgz |

#+NAME: logFile
#+begin_src sh :results table :exports both 
tar xvzf log_*.tgz
#+end_src

#+RESULTS: logFile
| bench_queries_region_random_10000_100_10_1504554073.log |
| bench_queries_region_random_10000_100_11_1504554073.log |
| bench_queries_region_random_10000_100_1_1504554073.log  |
| bench_queries_region_random_10000_100_12_1504554073.log |
| bench_queries_region_random_10000_100_13_1504554073.log |
| bench_queries_region_random_10000_100_14_1504554073.log |
| bench_queries_region_random_10000_100_2_1504554073.log  |
| bench_queries_region_random_10000_100_3_1504554073.log  |
| bench_queries_region_random_10000_100_4_1504554073.log  |
| bench_queries_region_random_10000_100_5_1504554073.log  |
| bench_queries_region_random_10000_100_6_1504554073.log  |
| bench_queries_region_random_10000_100_7_1504554073.log  |
| bench_queries_region_random_10000_100_8_1504554073.log  |
| bench_queries_region_random_10000_100_9_1504554073.log  |

Create CSV using logFile 

#+NAME: csvFile
#+begin_src sh :results output :exports both :var logFileList=logFile

#echo $logFile | sed 's/bench_queries_region_random_10000_100_\([[:digit:]]\)//g'
for logFile in $logFileList
do
echo $(basename -s .log $logFile ).csv
grep "GeoHashBinary\|BTree" $logFile | grep "query" | sed "s/QueryBench//g" >  $(basename -s .log $logFile ).csv
done

#+end_src

#+RESULTS: csvFile
#+begin_example
bench_queries_region_random_10000_100_10_1504554073.csv
bench_queries_region_random_10000_100_11_1504554073.csv
bench_queries_region_random_10000_100_1_1504554073.csv
bench_queries_region_random_10000_100_12_1504554073.csv
bench_queries_region_random_10000_100_13_1504554073.csv
bench_queries_region_random_10000_100_14_1504554073.csv
bench_queries_region_random_10000_100_2_1504554073.csv
bench_queries_region_random_10000_100_3_1504554073.csv
bench_queries_region_random_10000_100_4_1504554073.csv
bench_queries_region_random_10000_100_5_1504554073.csv
bench_queries_region_random_10000_100_6_1504554073.csv
bench_queries_region_random_10000_100_7_1504554073.csv
bench_queries_region_random_10000_100_8_1504554073.csv
bench_queries_region_random_10000_100_9_1504554073.csv
#+end_example


Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile
library(tidyverse)

                                        # Reads a csv file and add a column identifying the csv by parsin its name
readAdd <- function(input){

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep="") ) %>%
         mutate (
             ref = as.factor(
                 gsub("bench_queries_region_random_10000_100_([[:digit:]]+)_.*","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]

df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
Loading tidyverse: ggplot2
Loading tidyverse: tibble
Loading tidyverse: tidyr
Loading tidyverse: readr
Loading tidyverse: purrr
Loading tidyverse: dplyr
Conflicts with tidy packages ---------------------------------------------------
filter(): dplyr, stats
lag():    dplyr, stats
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual expected   <int> <chr>      <chr>      <chr> actual 1     1  <NA> 11 columns 10 columns file 2     2  <NA> 11 columns 10 columns row 3     3  <NA> 11 columns 10 columns col 4     4  <NA> 11 columns 10 columns expected 5     5  <NA> 11 columns 10 columns actual # ... with 1 more variables: file <chr>
... ................. ... ................................... ........ ................................... ...... ................................... .... ................................... ... ................................... ... ................................... ........ ................................... ...... .......................................
See problems(...) for more details.

There were 14 warnings (use warnings() to see them)
#+end_example

Remove useless columns
#+begin_src R :results output :exports both :session 

names(df) <- c("algo" , "V2" , "queryId",  "V4" , "T"  ,"bench" , "ms" , "V8", "refinements" , "V10", "Count", "refLevel")

df <- select(df, -V2, -V4, -V8, -V10)

str(df)
#+end_src

#+RESULTS:
: Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	44800 obs. of  8 variables:
:  $ algo       : chr  "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" ...
:  $ queryId    : int  0 0 0 0 0 0 0 0 0 0 ...
:  $ T          : int  10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 ...
:  $ bench      : chr  "scan_at_region" "scan_at_region" "scan_at_region" "scan_at_region" ...
:  $ ms         : num  0.9 0.857 0.846 0.853 0.856 ...
:  $ refinements: int  1911 1911 1911 1911 1911 1911 1911 1911 1911 1911 ...
:  $ Count      : int  NA NA NA NA NA NA NA NA NA NA ...
:  $ refLevel   : Factor w/ 14 levels "10","11","1",..: 1 1 1 1 1 1 1 1 1 1 ...

Summarize the averages
#+begin_src R :results output :session :exports both
dfplot <- 
    df %>% 
    group_by_at(vars(-ms, -refinements)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms)) %>%
    mutate(refLevel = as.integer(as.character(refLevel))) %>%
    arrange(refLevel)

dfplot
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 4,480 x 8
# Groups:   algo, queryId, T, bench, Count [320]
    algo queryId     T           bench  Count refLevel    avg_ms       stdv
   <chr>   <int> <int>           <chr>  <int>    <int>     <dbl>      <dbl>
 1 BTree       0 10000 apply_at_region 132363        1  7.250580 0.05396851
 2 BTree       0 10000  scan_at_region     NA        1  9.220791 0.07743079
 3 BTree       1 10000 apply_at_region 132280        1  7.366771 0.10338661
 4 BTree       1 10000  scan_at_region     NA        1  9.267250 0.09813547
 5 BTree       2 10000 apply_at_region 132084        1 15.231120 0.08558480
 6 BTree       2 10000  scan_at_region     NA        1 17.420730 0.01589550
 7 BTree       3 10000 apply_at_region 132291        1 15.229560 0.07000530
 8 BTree       3 10000  scan_at_region     NA        1 17.442760 0.06526832
 9 BTree       4 10000 apply_at_region 132312        1 15.262720 0.12851341
10 BTree       4 10000  scan_at_region     NA        1 17.415560 0.01674171
# ... with 4,470 more rows
#+end_example


PLot time Vs RefLevel
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session 

dfplot %>% 
#    mutate(queryW = queryId %/% 10) 
#    filter(queryId < 10) %>%
    ggplot(aes(x = refLevel,avg_ms,color=as.factor(queryId))) + 
 #   geom_errorbar(aes(ymin = avg_ms - stdv, ymax = avg_ms + stdv) ) +
    geom_line() +
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
    facet_grid(bench~algo) 
  #  facet_wrap(algo~bench,nrow=2)

#+end_src

#+RESULTS:
[[file:/tmp/babel-40865RF/figure40861pU.png]]

Note the the smaller queryIDs have a larger area of selection 
[[file:~/Projects/pmq/data/queriesLHS.org::*Coordinates%20LHS%20To%20avoid%20out-of-bound%20queries%20+%20LAPPLY%20+%20RBIND][Coordinates LHS To avoid out-of-bound queries + LAPPLY + RBIND]]


Group by query width
#+begin_src R :results output :exports both :session 

dfplot %>% 
    mutate(queryW = queryId %/% 10) %>%
    group_by(algo,queryW,bench,refLevel) %>%
    summarize(time = mean(avg_ms), sdtdv = sd(avg_ms))

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 448 x 6
# Groups:   algo, queryW, bench [?]
    algo queryW           bench refLevel       time      sdtdv
   <chr>  <dbl>           <chr>    <int>      <dbl>      <dbl>
 1 BTree      0 apply_at_region        1 10.5206426 4.06853961
 2 BTree      0 apply_at_region        2  5.8119632 0.52092073
 3 BTree      0 apply_at_region        3  3.1141515 0.29458023
 4 BTree      0 apply_at_region        4  1.8906055 0.12699758
 5 BTree      0 apply_at_region        5  1.2007414 0.09021508
 6 BTree      0 apply_at_region        6  0.9936434 0.08033271
 7 BTree      0 apply_at_region        7  0.8089490 0.10404456
 8 BTree      0 apply_at_region        8  1.0047601 0.08612890
 9 BTree      0 apply_at_region        9  1.0476115 0.07974362
10 BTree      0 apply_at_region       10  1.0911424 0.07507010
# ... with 438 more rows
#+end_example


#+begin_src R :results output graphics :file "./img/Reflevel.png" :exports both :width 600 :height 400 :session 

dfplot %>% 
    mutate(queryW = 90 / 2**(queryId %/% 10)) %>%
    group_by(algo,queryW,bench,refLevel) %>%
    summarize(time = mean(avg_ms), stdv = sd(avg_ms)) %>%
    ggplot(aes(x = refLevel,time,color=as.factor(queryW))) + 
 #   geom_errorbar(aes(ymin = time - stdv, ymax = time + stdv) ) +
    geom_line() +
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
    scale_y_continuous(trans = scales::log2_trans()) +
    labs(colour="Query Width\n (Degrees)", y = "times ms (log2 scale)") +
    facet_grid(bench~algo)
  #  facet_wrap(algo~bench,nrow=2)

#+end_src

#+RESULTS:
[[file:./img/Reflevel.png]]


** Conclusions

We can justify that we verify in our experiments that Z = 8 gives in general the best performance. 

Z  = number of quadtree refinements of the query algorithm
