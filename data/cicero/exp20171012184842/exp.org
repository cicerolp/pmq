# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries - Twitter Dataset
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* DONE Description                                                   :export:

Test the queries on Twitter Dataset. 
And compare the following performances.

- PMQ / GEOHASH
- BTREE 
- RTREE - quadratic algorithm 
- RTREE - quadratic algorithm with bulk loading

Use the refinement level = 8 

Elements:
- Batch size = 1000
- Variate T = 1M to 128M
 
#+begin_src python :results output :exports both

minitems = 1000
maxitems = 128000

items = minitems
while( items <= maxitems) :
    print(items*1000)
    items *=2
#+end_src

#+RESULTS:
: 1000000
: 2000000
: 4000000
: 8000000
: 16000000
: 32000000
: 64000000
: 128000000

- Total elements = T * 1000  
  
Use the synthetic queries generated by the DoE in [[file:~/Projects/pmq/data/queriesLHS.org::#queries20170923145357][Twitter Data]].

queries Dataset : [[file:~/Projects/pmq/data/queriesTwitter.csv]]

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  

* DONE Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171012184842

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: Your branch is up-to-date with 'origin/master'.
: [master eb6f25a] LBK: new entry for exp20171012184842
:  1 file changed, 43 insertions(+)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171012184842 d820aca] Initial commit for exp20171012184842
 1 file changed, 885 insertions(+)
 create mode 100644 data/cicero/exp20171012184842/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * d820aca (HEAD -> exp20171012184842) Initial commit for exp20171012184842
: * eb6f25a (master) LBK: new entry for exp20171012184842
: | * cf1ae77 (exp20171009155025) wip

** DONE Export run script 

#+begin_src sh :results output :exports both

for I in 1 2 4 8 16 32 64 128 ; do
    T=$(($I * 1000))
    echo "$T"
done
#+end_src

#+RESULTS:
: 1000
: 2000
: 4000
: 8000
: 16000
: 32000
: 64000
: 128000

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DELT_SIZE=0 -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=OFF -DBENCH_BTREE=OFF -DBENCH_RTREE=OFF -DBENCH_DENSE=OFF -DBENCH_RTREE_BULK=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 

#Run queries
#t=$((10**6))
#t=26000
b=1000
#n=$(($t*$b))
ref=8

for i in 1 2 4 8 16 32 64 128 ; do
    t=$(($i * 1000))
    stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dat -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesTwitter.csv >  ${TMPDIR}/bench_queries_region_twitter_${t}_${b}_${ref}_${EXECID}.log
done

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	run.sh

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171012184842 65adf2d] UPD: run.sh script
:  2 files changed, 114 insertions(+), 17 deletions(-)
:  create mode 100755 data/cicero/exp20171012184842/run.sh

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

78 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Thu Oct 12 19:23:33 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 2, done.
(1/2)           remote: Compressing objects: 100% (2/2)           remote: Compressing objects: 100% (2/2), done.        
remote: Total 2 (delta 1), reused 0 (delta 0)
(1/2)   Unpacking objects: 100% (2/2)   Unpacking objects: 100% (2/2), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20171012184842
A	data/cicero/exp20171012184842/run_1507848678
Already on 'exp20171012184842'
Your branch is behind 'origin/exp20171012184842' by 1 commit, and can be fast-forwarded.
  (use "git pull" to update your local branch)
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Updating 65adf2d..fa246c7
Fast-forward
 benchmarks/bench_queries_region.cpp | 19 ++++++++++---------
 1 file changed, 10 insertions(+), 9 deletions(-)
commit fa246c7e0aa9fdc2a213118804bebb999225adf3
Date:   Thu Oct 12 20:05:04 2017 -0300

    workaround - performance bugs ?
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ 1507849705

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
: no server running on /tmp/tmux-1001/default
: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
: julio     8753  0.0  0.0  45248  4604 ?        Ss   Out12   0:00 /lib/systemd/sy
: julio     8755  0.0  0.0 145364  2112 ?        S    Out12   0:00 (sd-pam)
: julio     8784  0.0  0.0  97464  3384 ?        R    Out12   0:00 sshd: julio@pts
: julio     8785  0.0  0.0  22684  5136 pts/8    Ss   Out12   0:00 -bash
: julio     9654  0.0  0.0  37368  3276 pts/8    R+   00:04   0:00 ps ux

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
git commit -a -m "wip"
git status
git pull --rebase origin $expId
#+end_src

#+RESULTS:
#+begin_example
On branch exp20170923144931
Untracked files:
	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.org.orig
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../../../benchmarks/bench_insert_and_scan.cpp.orig
	../../../benchmarks/bench_queries_region.cpp.orig
	../exp20170825181747/
	../exp20170830124159/
	../exp20170904153555/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	.#exp.org
	../../queriesLHS.html
	../../randomLhsQueries.png

nothing added to commit but untracked files present
On branch exp20170923144931
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.org.orig
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../../../benchmarks/bench_insert_and_scan.cpp.orig
	../../../benchmarks/bench_queries_region.cpp.orig
	../exp20170825181747/
	../exp20170830124159/
	../exp20170904153555/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	.#exp.org
	../../queriesLHS.html
	../../randomLhsQueries.png

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Applying: wip
#+end_example


* INPROGRESS Analysis
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tarFile
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tarFile
| log_1507849705.tgz |

#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tarFile
tar xvzf $f
#+end_src

#+RESULTS: logFile
: bench_queries_region_twitter_1000_1000_8_1507849705.log
: bench_queries_region_twitter_128000_1000_8_1507849705.log
: bench_queries_region_twitter_16000_1000_8_1507849705.log
: bench_queries_region_twitter_2000_1000_8_1507849705.log
: bench_queries_region_twitter_32000_1000_8_1507849705.log
: bench_queries_region_twitter_4000_1000_8_1507849705.log
: bench_queries_region_twitter_64000_1000_8_1507849705.log
: bench_queries_region_twitter_8000_1000_8_1507849705.log

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile
for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
: bench_queries_region_twitter_1000_1000_8_1507849705.csv
: bench_queries_region_twitter_128000_1000_8_1507849705.csv
: bench_queries_region_twitter_16000_1000_8_1507849705.csv
: bench_queries_region_twitter_2000_1000_8_1507849705.csv
: bench_queries_region_twitter_32000_1000_8_1507849705.csv
: bench_queries_region_twitter_4000_1000_8_1507849705.csv
: bench_queries_region_twitter_64000_1000_8_1507849705.csv
: bench_queries_region_twitter_8000_1000_8_1507849705.csv

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

#+LATEX_HEADER:  \usepackage[a4paper,includeheadfoot,margin=2cm]{geometry}
 
*** Prepare

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep="") ))# %>%
        # mutate (
         #    tSize = as.factor(
          #       gsub("bench_queries_region_twitter_([[:digit:]]+)_.*","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
[1] "bench_queries_region_twitter_1000_1000_8_1507849705.csv"  
[2] "bench_queries_region_twitter_128000_1000_8_1507849705.csv"
[3] "bench_queries_region_twitter_16000_1000_8_1507849705.csv" 
[4] "bench_queries_region_twitter_2000_1000_8_1507849705.csv"  
[5] "bench_queries_region_twitter_32000_1000_8_1507849705.csv" 
[6] "bench_queries_region_twitter_4000_1000_8_1507849705.csv"  
[7] "bench_queries_region_twitter_64000_1000_8_1507849705.csv" 
[8] "bench_queries_region_twitter_8000_1000_8_1507849705.csv"
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning messages:
1: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
2: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
3: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
4: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
5: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
6: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
7: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
8: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
#+end_example


Remove useless columns
#+begin_src R :results output :exports both :session 
names(df) <- c("algo" , "V2" , "queryId", "V4", "T", "bench" , "ms" , "V8", "Refine","V10","Count")

df <- select(df, -V2, -V4, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 51,200 x 7
            algo queryId     T           bench       ms Refine  Count
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>
 1 GeoHashBinary       0  1000 apply_at_region 0.414447     29 924827
 2 GeoHashBinary       0  1000 apply_at_region 0.412729     29 924827
 3 GeoHashBinary       0  1000 apply_at_region 0.410752     29 924827
 4 GeoHashBinary       0  1000 apply_at_region 0.417607     29 924827
 5 GeoHashBinary       0  1000 apply_at_region 0.409624     29 924827
 6 GeoHashBinary       0  1000 apply_at_region 0.409441     29 924827
 7 GeoHashBinary       0  1000 apply_at_region 0.408944     29 924827
 8 GeoHashBinary       0  1000 apply_at_region 0.409712     29 924827
 9 GeoHashBinary       0  1000 apply_at_region 0.409174     29 924827
10 GeoHashBinary       0  1000 apply_at_region 0.408876     29 924827
# ... with 51,190 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both :session 
df <- 
    df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) %>%
    mutate(queryWidth = 90 / 2**(queryId %/% 10))  #%>%   # comput info about query width
    #mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> df # adjust the actual size of the elements
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :session :exports both
df 

dfAvg <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfAvg
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 51,200 x 8
            algo queryId     T           bench       ms Refine  Count queryWidth
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>      <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 0.414447     29 924827         90
 2 GeoHashBinary       0  1000 apply_at_region 0.412729     29 924827         90
 3 GeoHashBinary       0  1000 apply_at_region 0.410752     29 924827         90
 4 GeoHashBinary       0  1000 apply_at_region 0.417607     29 924827         90
 5 GeoHashBinary       0  1000 apply_at_region 0.409624     29 924827         90
 6 GeoHashBinary       0  1000 apply_at_region 0.409441     29 924827         90
 7 GeoHashBinary       0  1000 apply_at_region 0.408944     29 924827         90
 8 GeoHashBinary       0  1000 apply_at_region 0.409712     29 924827         90
 9 GeoHashBinary       0  1000 apply_at_region 0.409174     29 924827         90
10 GeoHashBinary       0  1000 apply_at_region 0.408876     29 924827         90
# ... with 51,190 more rows
# A tibble: 5,120 x 9
# Groups:   algo, queryId, T, bench, Refine, Count [?]
    algo queryId     T           bench Refine    Count queryWidth    avg_ms       stdv
   <chr>   <int> <int>           <chr>  <int>    <int>      <dbl>     <dbl>      <dbl>
 1 BTree       0  1000 apply_at_region     27   924827         90  10.64469 0.04540806
 2 BTree       0  1000  scan_at_region     29       NA         90  17.44709 0.03102001
 3 BTree       0  2000 apply_at_region     28  1855890         90  25.77742 0.67939316
 4 BTree       0  2000  scan_at_region     29       NA         90  36.30205 0.03723738
 5 BTree       0  4000 apply_at_region     32  3706387         90  56.49481 0.66904755
 6 BTree       0  4000  scan_at_region     33       NA         90  74.14725 0.02328167
 7 BTree       0  8000 apply_at_region     44  7417949         90 116.41330 0.11938737
 8 BTree       0  8000  scan_at_region     45       NA         90 151.37550 0.03572192
 9 BTree       0 16000 apply_at_region     50 14876686         90 238.53760 0.06671865
10 BTree       0 16000  scan_at_region     52       NA         90 307.56140 0.09391154
# ... with 5,110 more rows
#+end_example

*** Plot: change of query count with size of T.                    :export:

#+begin_src R :results output graphics :file "./img/count_by_T.pdf" :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(algo == "BTree" & bench == "apply_at_region") %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId%%10))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryWidth, scale = "free_y") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:./img/count_by_T.pdf]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(algo == "BTree" & bench == "apply_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryId, scale = "free_y") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-3289gSP/figure3289z5y.pdf]]

*** Plot: Scan Query Time by T facet by queryId                    :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(bench == "scan_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(~queryId, scale = "free_y") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:/tmp/babel-3289gSP/figure3289XqM.pdf]]

*** Plot: Scan query time by Query Count faceted by QueryId        :export:

#+begin_src R :results output :exports both :session 
dfAvg %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,T) %>%
    left_join( 
        filter(ungroup(dfAvg), bench == "scan_at_region") %>% select(-Count)
   ) -> dfCount
dfCount
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "algo", "T")
# A tibble: 2,560 x 9
   queryId  algo     Count      T          bench Refine queryWidth     avg_ms        stdv
     <int> <chr>     <int>  <int>          <chr>  <int>      <dbl>      <dbl>       <dbl>
 1       0 BTree    924827   1000 scan_at_region     29         90   17.44709  0.03102001
 2       0 BTree   1855890   2000 scan_at_region     29         90   36.30205  0.03723738
 3       0 BTree   3706387   4000 scan_at_region     33         90   74.14725  0.02328167
 4       0 BTree   7417949   8000 scan_at_region     45         90  151.37550  0.03572192
 5       0 BTree  14876686  16000 scan_at_region     52         90  307.56140  0.09391154
 6       0 BTree  29764961  32000 scan_at_region     64         90  691.87770  0.59023687
 7       0 BTree  59715461  64000 scan_at_region     89         90 1679.86400  1.45107010
 8       0 BTree 119931295 128000 scan_at_region    118         90 3746.40000 14.13264542
 9       1 BTree    929918   1000 scan_at_region     46         90   17.40384  0.03499172
10       1 BTree   1866101   2000 scan_at_region     46         90   36.21215  0.03681069
# ... with 2,550 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(~queryId, scale = "free") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-3289gSP/figure3289lnx.pdf]]

*** Plot: Scan query throughput by Query Count faceted by QueryId  :export:


#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 
dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), group=algo, y = Count / avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
    geom_line() +
    facet_wrap(~queryId,scale="free") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, vjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-3289gSP/figure3289l1Z.pdf]]

