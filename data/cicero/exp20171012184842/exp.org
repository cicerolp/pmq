# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries - Twitter Dataset
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* DONE Description                                                   :export:

Test the queries on Twitter Dataset. 
And compare the following performances.

- PMQ / GEOHASH
- BTREE 
- RTREE - quadratic algorithm 
- RTREE - quadratic algorithm with bulk loading

Use the refinement level = 8 

Elements:
- Batch size = 1000
- Variate T = 1M to 128M
 
#+begin_src python :results output :exports both

minitems = 1000
maxitems = 128000

items = minitems
while( items <= maxitems) :
    print(items*1000)
    items *=2
#+end_src

#+RESULTS:
: 1000000
: 2000000
: 4000000
: 8000000
: 16000000
: 32000000
: 64000000
: 128000000

- Total elements = T * 1000  
  
Use the synthetic queries generated by the DoE in [[file:~/Projects/pmq/data/queriesLHS.org::#queries20170923145357][Twitter Data]].

queries Dataset : [[file:~/Projects/pmq/data/queriesTwitter.csv]]

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  

* DONE Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171012184842

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: Your branch is up-to-date with 'origin/master'.
: [master eb6f25a] LBK: new entry for exp20171012184842
:  1 file changed, 43 insertions(+)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171012184842 d820aca] Initial commit for exp20171012184842
 1 file changed, 885 insertions(+)
 create mode 100644 data/cicero/exp20171012184842/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * d820aca (HEAD -> exp20171012184842) Initial commit for exp20171012184842
: * eb6f25a (master) LBK: new entry for exp20171012184842
: | * cf1ae77 (exp20171009155025) wip

** DONE Export run script 

#+begin_src sh :results output :exports both

for I in 1 2 4 8 16 32 64 128 ; do
    T=$(($I * 1000))
    echo "$T"
done
#+end_src

#+RESULTS:
: 1000
: 2000
: 4000
: 8000
: 16000
: 32000
: 64000
: 128000

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DELT_SIZE=0 -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=OFF -DBENCH_BTREE=OFF -DBENCH_RTREE=OFF -DBENCH_DENSE=OFF -DBENCH_RTREE_BULK=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 

#Run queries
#t=$((10**6))
#t=26000
b=1000
#n=$(($t*$b))
ref=8

for i in 1 2 4 8 16 32 64 128 ; do
    t=$(($i * 1000))
    stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dat -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesTwitter.csv >  ${TMPDIR}/bench_queries_region_twitter_${t}_${b}_${ref}_${EXECID}.log
done

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171012184842
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org
	modified:   run.sh

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	exp.pdf
	exp.tex
	img/

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171012184842 2292431] UPD: run.sh script
:  2 files changed, 2 insertions(+), 2 deletions(-)

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

75 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Fri Oct 13 16:41:34 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 23, done.
(1/20)           
remote: Compressing objects:  10% (2/20)           
remote: Compressing objects:  15% (3/20)           
remote: Compressing objects:  20% (4/20)           
remote: Compressing objects:  25% (5/20)           
remote: Compressing objects:  30% (6/20)           
remote: Compressing objects:  35% (7/20)           
remote: Compressing objects:  40% (8/20)           
remote: Compressing objects:  45% (9/20)           
remote: Compressing objects:  50% (10/20)           
remote: Compressing objects:  55% (11/20)           
remote: Compressing objects:  60% (12/20)           
remote: Compressing objects:  65% (13/20)           
remote: Compressing objects:  70% (14/20)           
remote: Compressing objects:  75% (15/20)           
remote: Compressing objects:  80% (16/20)           
remote: Compressing objects:  85% (17/20)           
remote: Compressing objects:  90% (18/20)           
remote: Compressing objects:  95% (19/20)           
remote: Compressing objects: 100% (20/20)           
remote: Compressing objects: 100% (20/20), done.        
remote: Total 23 (delta 16), reused 0 (delta 0)
(1/23)   
Unpacking objects:   8% (2/23)   
Unpacking objects:  13% (3/23)   
Unpacking objects:  17% (4/23)   
Unpacking objects:  21% (5/23)   
Unpacking objects:  26% (6/23)   
Unpacking objects:  30% (7/23)   
Unpacking objects:  34% (8/23)   
Unpacking objects:  39% (9/23)   
Unpacking objects:  43% (10/23)   
Unpacking objects:  47% (11/23)   
Unpacking objects:  52% (12/23)   
Unpacking objects:  56% (13/23)   
Unpacking objects:  60% (14/23)   
Unpacking objects:  65% (15/23)   
Unpacking objects:  69% (16/23)   
Unpacking objects:  73% (17/23)   
Unpacking objects:  78% (18/23)   
Unpacking objects:  82% (19/23)   
Unpacking objects:  86% (20/23)   
Unpacking objects:  91% (21/23)   
Unpacking objects:  95% (22/23)   
Unpacking objects: 100% (23/23)   
Unpacking objects: 100% (23/23), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20171012184842
M	data/cicero/exp20171012184842/run_1507849705
Already on 'exp20171012184842'
Your branch is behind 'origin/exp20171012184842' by 4 commits, and can be fast-forwarded.
  (use "git pull" to update your local branch)
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Updating 6c842e8..2292431
Fast-forward
 benchmarks/bench_queries_region.cpp   |   3 +
 data/cicero/exp20171012184842/exp.org | 724 +++++++++++++++++-----------------
 data/cicero/exp20171012184842/run.sh  |   2 +-
 3 files changed, 376 insertions(+), 353 deletions(-)
commit 229243171c14b0e2c7cc9d9a4b1ffc0d6017cc79
Date:   Fri Oct 13 16:43:23 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ julio@cicero:~/Projects/pmq/data/cicero/exp20171012184842$ 1507923856

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
: no server running on /tmp/tmux-1001/default
: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
: julio    12367  0.0  0.0  45248  4684 ?        Ss   16:43   0:00 /lib/systemd/sy
: julio    12370  0.0  0.0 145364  2112 ?        S    16:43   0:00 (sd-pam)
: julio    12398  0.0  0.0  97464  3296 ?        R    16:43   0:00 sshd: julio@pts
: julio    12399  0.0  0.0  22688  5132 pts/8    Ss   16:43   0:00 -bash
: julio    12746  0.0  0.0  37368  3296 pts/8    R+   17:11   0:00 ps ux

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
git commit -a -m "wip"
git status
git pull --rebase origin $expId
#+end_src

#+RESULTS:
#+begin_example
[exp20171012184842 37984b2] wip
 1 file changed, 29 insertions(+), 26 deletions(-)
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../exp20170825181747/
	../exp20170830124159/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	../exp20170923144931/
	../exp20170923193058/
	../exp20171009155025/
	exp.pdf
	exp.tex
	img/
	../../queriesLHS.html
	../../queriesLHS_BACKUP_23848.org
	../../queriesLHS_BASE_23848.org
	../../queriesLHS_LOCAL_23848.org
	../../queriesLHS_REMOTE_23848.org
	../../randomLhsQueries.png
	../../../history.txt
	../../../qqqq

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Applying: wip
#+end_example


* INPROGRESS Analysis
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tarFile
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tarFile
| log_1507849705.tgz |
| log_1507923856.tgz |

#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tarFile
for i in $f; do 
    tar xvzf $i
done
#+end_src

#+RESULTS: logFile
#+begin_example
bench_queries_region_twitter_1000_1000_8_1507849705.log
bench_queries_region_twitter_128000_1000_8_1507849705.log
bench_queries_region_twitter_16000_1000_8_1507849705.log
bench_queries_region_twitter_2000_1000_8_1507849705.log
bench_queries_region_twitter_32000_1000_8_1507849705.log
bench_queries_region_twitter_4000_1000_8_1507849705.log
bench_queries_region_twitter_64000_1000_8_1507849705.log
bench_queries_region_twitter_8000_1000_8_1507849705.log
bench_queries_region_twitter_1000_1000_8_1507923856.log
bench_queries_region_twitter_128000_1000_8_1507923856.log
bench_queries_region_twitter_16000_1000_8_1507923856.log
bench_queries_region_twitter_2000_1000_8_1507923856.log
bench_queries_region_twitter_32000_1000_8_1507923856.log
bench_queries_region_twitter_4000_1000_8_1507923856.log
bench_queries_region_twitter_64000_1000_8_1507923856.log
bench_queries_region_twitter_8000_1000_8_1507923856.log
#+end_example

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile
for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep "; query ;" $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
#+begin_example
bench_queries_region_twitter_1000_1000_8_1507849705.csv
bench_queries_region_twitter_128000_1000_8_1507849705.csv
bench_queries_region_twitter_16000_1000_8_1507849705.csv
bench_queries_region_twitter_2000_1000_8_1507849705.csv
bench_queries_region_twitter_32000_1000_8_1507849705.csv
bench_queries_region_twitter_4000_1000_8_1507849705.csv
bench_queries_region_twitter_64000_1000_8_1507849705.csv
bench_queries_region_twitter_8000_1000_8_1507849705.csv
bench_queries_region_twitter_1000_1000_8_1507923856.csv
bench_queries_region_twitter_128000_1000_8_1507923856.csv
bench_queries_region_twitter_16000_1000_8_1507923856.csv
bench_queries_region_twitter_2000_1000_8_1507923856.csv
bench_queries_region_twitter_32000_1000_8_1507923856.csv
bench_queries_region_twitter_4000_1000_8_1507923856.csv
bench_queries_region_twitter_64000_1000_8_1507923856.csv
bench_queries_region_twitter_8000_1000_8_1507923856.csv
#+end_example

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      

#+LATEX_HEADER:  \usepackage[a4paper,includeheadfoot,margin=2cm]{geometry}
 
*** Prepare

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
library(tidyverse)
options(tibble.width = Inf)
setwd(path)

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep=""), col_types="ccicicdcici" ))# %>%
        # mutate (
         #    tSize = as.factor(
          #       gsub("bench_queries_region_twitter_([[:digit:]]+)_.*","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
 [1] "bench_queries_region_twitter_1000_1000_8_1507849705.csv"  
 [2] "bench_queries_region_twitter_128000_1000_8_1507849705.csv"
 [3] "bench_queries_region_twitter_16000_1000_8_1507849705.csv" 
 [4] "bench_queries_region_twitter_2000_1000_8_1507849705.csv"  
 [5] "bench_queries_region_twitter_32000_1000_8_1507849705.csv" 
 [6] "bench_queries_region_twitter_4000_1000_8_1507849705.csv"  
 [7] "bench_queries_region_twitter_64000_1000_8_1507849705.csv" 
 [8] "bench_queries_region_twitter_8000_1000_8_1507849705.csv"  
 [9] "bench_queries_region_twitter_1000_1000_8_1507923856.csv"  
[10] "bench_queries_region_twitter_128000_1000_8_1507923856.csv"
[11] "bench_queries_region_twitter_16000_1000_8_1507923856.csv" 
[12] "bench_queries_region_twitter_2000_1000_8_1507923856.csv"  
[13] "bench_queries_region_twitter_32000_1000_8_1507923856.csv" 
[14] "bench_queries_region_twitter_4000_1000_8_1507923856.csv"  
[15] "bench_queries_region_twitter_64000_1000_8_1507923856.csv" 
[16] "bench_queries_region_twitter_8000_1000_8_1507923856.csv"
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_128000_1000_8_1507849705.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_16000_1000_8_1507849705.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_32000_1000_8_1507849705.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_4000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_64000_1000_8_1507849705.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 6400 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_8000_1000_8_1507849705.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_1000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_1000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_1000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_1000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_1000_1000_8_1507923856.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_128000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_128000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_128000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_128000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_128000_1000_8_1507923856.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_16000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_16000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_16000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_16000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_16000_1000_8_1507923856.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_2000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_2000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_2000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_2000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_2000_1000_8_1507923856.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_32000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_32000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_32000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_32000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_32000_1000_8_1507923856.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_4000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_4000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_4000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_4000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_4000_1000_8_1507923856.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_64000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_64000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_64000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_64000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_64000_1000_8_1507923856.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 1600 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_twitter_8000_1000_8_1507923856.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_twitter_8000_1000_8_1507923856.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_twitter_8000_1000_8_1507923856.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_twitter_8000_1000_8_1507923856.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_twitter_8000_1000_8_1507923856.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
There were 16 warnings (use warnings() to see them)
#+end_example


Remove useless columns
#+begin_src R :results output :exports both :session 
names(df) <- c("algo" , "V2" , "queryId", "V4", "T", "bench" , "ms" , "V8", "Refine","V10","Count")

df <- select(df, -V2, -V4, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 64,000 x 7
            algo queryId     T           bench       ms Refine  Count
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>
 1 GeoHashBinary       0  1000 apply_at_region 0.414447     29 924827
 2 GeoHashBinary       0  1000 apply_at_region 0.412729     29 924827
 3 GeoHashBinary       0  1000 apply_at_region 0.410752     29 924827
 4 GeoHashBinary       0  1000 apply_at_region 0.417607     29 924827
 5 GeoHashBinary       0  1000 apply_at_region 0.409624     29 924827
 6 GeoHashBinary       0  1000 apply_at_region 0.409441     29 924827
 7 GeoHashBinary       0  1000 apply_at_region 0.408944     29 924827
 8 GeoHashBinary       0  1000 apply_at_region 0.409712     29 924827
 9 GeoHashBinary       0  1000 apply_at_region 0.409174     29 924827
10 GeoHashBinary       0  1000 apply_at_region 0.408876     29 924827
# ... with 63,990 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both :session 
df <- 
    df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) %>%
    mutate(queryWidth = 90 / 2**(queryId %/% 10))  #%>%   # comput info about query width
    #mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> df # adjust the actual size of the elements
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :session :exports both
df 

dfAvg <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfAvg
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 64,000 x 8
            algo queryId     T           bench       ms Refine  Count queryWidth
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <int>      <dbl>
 1 GeoHashBinary       0  1000 apply_at_region 0.414447     29 924827         90
 2 GeoHashBinary       0  1000 apply_at_region 0.412729     29 924827         90
 3 GeoHashBinary       0  1000 apply_at_region 0.410752     29 924827         90
 4 GeoHashBinary       0  1000 apply_at_region 0.417607     29 924827         90
 5 GeoHashBinary       0  1000 apply_at_region 0.409624     29 924827         90
 6 GeoHashBinary       0  1000 apply_at_region 0.409441     29 924827         90
 7 GeoHashBinary       0  1000 apply_at_region 0.408944     29 924827         90
 8 GeoHashBinary       0  1000 apply_at_region 0.409712     29 924827         90
 9 GeoHashBinary       0  1000 apply_at_region 0.409174     29 924827         90
10 GeoHashBinary       0  1000 apply_at_region 0.408876     29 924827         90
# ... with 63,990 more rows
# A tibble: 6,400 x 9
# Groups:   algo, queryId, T, bench, Refine, Count [?]
    algo queryId     T           bench Refine    Count queryWidth    avg_ms       stdv
   <chr>   <int> <int>           <chr>  <int>    <int>      <dbl>     <dbl>      <dbl>
 1 BTree       0  1000 apply_at_region     27   924827         90  10.64469 0.04540806
 2 BTree       0  1000  scan_at_region     29       NA         90  17.44709 0.03102001
 3 BTree       0  2000 apply_at_region     28  1855890         90  25.77742 0.67939316
 4 BTree       0  2000  scan_at_region     29       NA         90  36.30205 0.03723738
 5 BTree       0  4000 apply_at_region     32  3706387         90  56.49481 0.66904755
 6 BTree       0  4000  scan_at_region     33       NA         90  74.14725 0.02328167
 7 BTree       0  8000 apply_at_region     44  7417949         90 116.41330 0.11938737
 8 BTree       0  8000  scan_at_region     45       NA         90 151.37550 0.03572192
 9 BTree       0 16000 apply_at_region     50 14876686         90 238.53760 0.06671865
10 BTree       0 16000  scan_at_region     52       NA         90 307.56140 0.09391154
# ... with 6,390 more rows
#+end_example

*** Plot: change of query count with size of T.                    :export:
#+begin_src R :results output graphics :file "./img/count_by_T.pdf" :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(algo == "BTree" & bench == "apply_at_region") %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId%%10))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryWidth, scale = "free_y") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:./img/count_by_T.pdf]]

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(algo == "BTree" & bench == "apply_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = Count, color = factor(queryId))) +
    geom_line(aes(group=queryId))+
    facet_wrap(~queryId, scale = "free_y") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-19237zwJ/figure19237AuT.pdf]]

*** Plot: Scan Query Time by T facet by queryId                    :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfAvg %>% 
    filter(bench == "scan_at_region" & queryWidth < 1) %>%
    ggplot(aes(x = factor(T), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(~queryId, scale = "free_y") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))
#+end_src

#+RESULTS:
[[file:/tmp/babel-19237zwJ/figure19237N4Z.pdf]]

*** Plot: Scan query time by Query Count faceted by QueryId        :export:

#+begin_src R :results output :exports both :session 
dfAvg %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,T) %>%
    left_join( 
        filter(ungroup(dfAvg), bench == "scan_at_region") %>% select(-Count)
   ) -> dfCount
dfCount
#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("queryId", "algo", "T")
# A tibble: 3,200 x 9
   queryId  algo     Count      T          bench Refine queryWidth     avg_ms        stdv
     <int> <chr>     <int>  <int>          <chr>  <int>      <dbl>      <dbl>       <dbl>
 1       0 BTree    924827   1000 scan_at_region     29         90   17.44709  0.03102001
 2       0 BTree   1855890   2000 scan_at_region     29         90   36.30205  0.03723738
 3       0 BTree   3706387   4000 scan_at_region     33         90   74.14725  0.02328167
 4       0 BTree   7417949   8000 scan_at_region     45         90  151.37550  0.03572192
 5       0 BTree  14876686  16000 scan_at_region     52         90  307.56140  0.09391154
 6       0 BTree  29764961  32000 scan_at_region     64         90  691.87770  0.59023687
 7       0 BTree  59715461  64000 scan_at_region     89         90 1679.86400  1.45107010
 8       0 BTree 119931295 128000 scan_at_region    118         90 3746.40000 14.13264542
 9       1 BTree    929918   1000 scan_at_region     46         90   17.40384  0.03499172
10       1 BTree   1866101   2000 scan_at_region     46         90   36.21215  0.03681069
# ... with 3,190 more rows
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 

dfCount %>% 
    filter(queryWidth < 1) %>%
    ggplot(aes(x = factor(Count), y = avg_ms, color = algo)) +
    geom_line(aes(group=algo))+
    geom_errorbar(aes(ymin = avg_ms - stdv , ymax = avg_ms + stdv)) + 
    facet_wrap(~queryId, scale = "free") + 
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 0.75))

#+end_src

#+RESULTS:
[[file:/tmp/babel-19237zwJ/figure19237aCg.pdf]]

*** Plot: Scan query throughput by Query Count faceted by QueryId  :export:

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".pdf") :exports both :width 14 :height 10 :session 
dfCount %>% 
    filter(queryWidth < 1) %>%
    arrange(Count,T) %>%
    mutate(lbls = paste(Count," (",T/1000,")",sep="")) %>%
    ggplot(aes(x = factor(lbls,levels=unique(lbls)), group=algo, y = Count / avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = Count / (avg_ms - stdv) , ymax = Count / (avg_ms + stdv)) ) +
    geom_line() +
    facet_wrap(~queryId,scale="free") +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1))+
    labs(x = "Query Count ( Dataset size x 10^{6} )")

#+end_src

#+RESULTS:
[[file:/tmp/babel-19237zwJ/figure192374vk.pdf]]

