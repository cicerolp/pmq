# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries 
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* DONE Description                                                   :export:

Test the queries on uniform data. 
And compare the following performances.


- PMQ / GEOHASH
- BTREE 
- RTREE - quadratic algorithm 
- RTREE - quadratic algorithm with bulk loading
- DENSE

Use the refinement level = 8 

Elements:
- Timewindow = 26000
- Batch size = 1000

- Total elements = 26.000.000 

- Fixed query width on the map
- Uniform dataset (to get the same amount of elements on the average)

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  

* TODO Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171009155025

Set up git branch

#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: [master bb12441] LBK: new entry for exp20171009155025
:  1 file changed, 39 insertions(+), 2 deletions(-)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:
: M	LabBook.org

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171009155025
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171009155025 85b4e29] Initial commit for exp20171009155025
 1 file changed, 867 insertions(+)
 create mode 100644 data/cicero/exp20171009155025/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 85b4e29 (HEAD -> exp20171009155025) Initial commit for exp20171009155025
: * 678a50e (master) LBK: new entry for exp20171009155025
: * 706cb9a upd: ELT_SIZE on CMAKE variables

** DONE Export run script 

#+begin_src sh :results output :exports both

for EL in 16 32 64 128 256 ; do
    ELTSIZE=$(($EL-16))
    echo "cmake -DELT_SIZE=$ELTSIZE . ; make"
done
#+end_src

#+RESULTS:
: cmake -DELT_SIZE=0 . ; make
: cmake -DELT_SIZE=16 . ; make
: cmake -DELT_SIZE=48 . ; make
: cmake -DELT_SIZE=112 . ; make
: cmake -DELT_SIZE=240 . ; make

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF  ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=ON -DBENCH_BTREE=ON -DBENCH_RTREE=ON -DBENCH_DENSE=ON -DBENCH_RTREE_BULK=ON ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 


for EL in 16 32 64 128 256 ; do
    ELTSIZE=$(($EL-16))
    cmake -DELT_SIZE=$ELTSIZE . ; make

    #Run queries
    #t=$((10**6))
    t=26000
    b=1000
    #n=$(($t*$b))
    ref=8
    stdbuf -oL ./benchmarks/bench_queries_region -seed 123 -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesLHS.csv >  ${TMPDIR}/bench_queries_region_random_${t}_${b}_${ref}_${ELTSIZE}_${EXECID}.log

done
set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171009155025
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	run.sh

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171009155025 202321f] UPD: run.sh script
:  2 files changed, 115 insertions(+), 20 deletions(-)
:  create mode 100755 data/cicero/exp20171009155025/run.sh

Push to remote
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
git push origin $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** DONE Get new changes on remote                                 :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example

Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

67 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Fri Oct  6 15:34:03 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
git fetch origin $expId
git checkout $expId
git pull origin $expId
git log -1 | cat 
#+end_src

#+RESULTS:
#+begin_example

julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ remote: Counting objects: 22, done.
(1/19)           remote: Compressing objects:  10% (2/19)           remote: Compressing objects:  15% (3/19)           remote: Compressing objects:  21% (4/19)           remote: Compressing objects:  26% (5/19)           remote: Compressing objects:  31% (6/19)           remote: Compressing objects:  36% (7/19)           remote: Compressing objects:  42% (8/19)           remote: Compressing objects:  47% (9/19)           remote: Compressing objects:  52% (10/19)           remote: Compressing objects:  57% (11/19)           remote: Compressing objects:  63% (12/19)           remote: Compressing objects:  68% (13/19)           remote: Compressing objects:  73% (14/19)           remote: Compressing objects:  78% (15/19)           remote: Compressing objects:  84% (16/19)           remote: Compressing objects:  89% (17/19)           remote: Compressing objects:  94% (18/19)           remote: Compressing objects: 100% (19/19)           remote: Compressing objects: 100% (19/19), done.        
remote: Total 22 (delta 13), reused 0 (delta 0)
(1/22)   Unpacking objects:   9% (2/22)   Unpacking objects:  13% (3/22)   Unpacking objects:  18% (4/22)   Unpacking objects:  22% (5/22)   Unpacking objects:  27% (6/22)   Unpacking objects:  31% (7/22)   Unpacking objects:  36% (8/22)   Unpacking objects:  40% (9/22)   Unpacking objects:  45% (10/22)   Unpacking objects:  50% (11/22)   Unpacking objects:  54% (12/22)   Unpacking objects:  59% (13/22)   Unpacking objects:  63% (14/22)   Unpacking objects:  68% (15/22)   Unpacking objects:  72% (16/22)   Unpacking objects:  77% (17/22)   Unpacking objects:  81% (18/22)   Unpacking objects:  86% (19/22)   Unpacking objects:  90% (20/22)   Unpacking objects:  95% (21/22)   Unpacking objects: 100% (22/22)   Unpacking objects: 100% (22/22), done.
From bitbucket.org:jtoss/pmq
FETCH_HEAD
origin/exp20171009155025
Branch exp20171009155025 set up to track remote branch exp20171009155025 from origin.
Switched to a new branch 'exp20171009155025'
From bitbucket.org:jtoss/pmq
FETCH_HEAD
Already up-to-date.
commit 202321fa0b13a6268b22e29124a548653539ac41
Date:   Mon Oct 9 17:41:22 2017 -0300

    UPD: run.sh script
#+end_example

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 011775f5fdeaeeff330da7df39751d9c5323b570
: Date:   Mon Feb 13 12:20:46 2017 -0200
: 
:     Bugfix: corrected pointer casts

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171009155025$ julio@cicero:~/Projects/pmq/data/cicero/exp20171009155025$ julio@cicero:~/Projects/pmq/data/cicero/exp20171009155025$ julio@cicero:~/Projects/pmq/data/cicero/exp20171009155025$ 1507581698

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
: no server running on /tmp/tmux-1001/default
: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
: julio     2021  0.0  0.0  45248   708 ?        Ss   17:41   0:00 /lib/systemd/sy
: julio     2023  0.0  0.0 145364  2112 ?        S    17:41   0:00 (sd-pam)
: julio     2076  0.0  0.0  97464  1272 ?        R    17:41   0:00 sshd: julio@pts
: julio     2077  0.0  0.0  22684  3916 pts/8    Ss   17:41   0:00 -bash
: julio     3146  0.0  0.0  37368  3300 pts/8    R+   18:07   0:00 ps ux

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
#git commit -a -m "wip"
git status
git pull origin $expId
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171009155025
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   ../../../LabBook.org
	modified:   exp.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.org.bkp
	../exp20170830124159/
	../exp20170904152622/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170925155952/
	.#exp.org
	../../../pprVLDB2018/

no changes added to commit (use "git add" and/or "git commit -a")
Updating 202321f..7460cea
Fast-forward
 data/cicero/exp20171009155025/info.org           | 691 +++++++++++++++++++
 data/cicero/exp20171009155025/log_1507581698.tgz | Bin 0 -> 210336 bytes
 data/cicero/exp20171009155025/run_1507581698     | 809 +++++++++++++++++++++++
 3 files changed, 1500 insertions(+)
 create mode 100644 data/cicero/exp20171009155025/info.org
 create mode 100644 data/cicero/exp20171009155025/log_1507581698.tgz
 create mode 100644 data/cicero/exp20171009155025/run_1507581698
#+end_example


* TODO Analysis
** Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+NAME: tgzFiles
#+begin_src sh :results table :exports both
ls *tgz
#+end_src

#+RESULTS: tgzFiles
| log_1507581698.tgz |


Take the last archive from the list above:
#+NAME: logFile
#+begin_src sh :results output :exports both :var f=tgzFiles[-1]
tar xvzf $f
#+end_src

#+RESULTS: logFile
: bench_queries_region_random_26000_1000_8_0_1507581698.log
: bench_queries_region_random_26000_1000_8_112_1507581698.log
: bench_queries_region_random_26000_1000_8_16_1507581698.log
: bench_queries_region_random_26000_1000_8_240_1507581698.log
: bench_queries_region_random_26000_1000_8_48_1507581698.log

Create CSV using logFile 
#+begin_src sh :results output :exports both :var logFileList=logFile

#f=$(echo $logFileList | cut -d" " -f1)

#output=$( basename -s .log $f | sed "s/_[[:digit:]]\{5\}_/_/g").csv
#echo $output
#rm $output
#touch $output

for logFile in $logFileList ; 
do
output=$( basename -s .log $logFile).csv
echo $output 
grep " ; query ; " $logFile | sed "s/QueryBench//g" >  $output
done
#+end_src

#+NAME: csvFile
#+RESULTS:
: bench_queries_region_random_26000_1000_8_0_1507581698.csv
: bench_queries_region_random_26000_1000_8_112_1507581698.csv
: bench_queries_region_random_26000_1000_8_16_1507581698.csv
: bench_queries_region_random_26000_1000_8_240_1507581698.csv
: bench_queries_region_random_26000_1000_8_48_1507581698.csv

Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      
*** Prepare

Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
library(tidyverse)
setwd(path)

#df <- f %>% read_delim(delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep="") )
#df

readAdd <- function(input){  # Reads a csv file and add a column identifying the csv by parsing its name

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep="") ) %>%
         mutate (
             tSize = as.factor(
                 gsub("bench_queries_region_random_26000_1000_8_([[:digit:]]+)_.*","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]
files
df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
[1] "bench_queries_region_random_26000_1000_8_0_1507581698.csv"  
[2] "bench_queries_region_random_26000_1000_8_112_1507581698.csv"
[3] "bench_queries_region_random_26000_1000_8_16_1507581698.csv" 
[4] "bench_queries_region_random_26000_1000_8_240_1507581698.csv"
[5] "bench_queries_region_random_26000_1000_8_48_1507581698.csv"
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_0_1507581698.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_0_1507581698.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_0_1507581698.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_0_1507581698.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_0_1507581698.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                          file expected   <int> <chr>      <chr>      <chr>                                                         <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_112_1507581698.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_112_1507581698.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_112_1507581698.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_112_1507581698.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_112_1507581698.csv'
... ................. ... ................................................................................................. ........ .............................................................................................. [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_16_1507581698.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_16_1507581698.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_16_1507581698.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_16_1507581698.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_16_1507581698.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 8000 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_48_1507581698.csv' file 2     2  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_48_1507581698.csv' row 3     3  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_48_1507581698.csv' col 4     4  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_48_1507581698.csv' expected 5     5  <NA> 11 columns 10 columns 'bench_queries_region_random_26000_1000_8_48_1507581698.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning messages:
1: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
2: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
3: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
4: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
#+end_example

Remove useless columns
#+begin_src R :results output :exports both :session 
names(df) <- c("algo" , "V2" , "queryId", "V4", "V5", "bench" , "ms" , "V8", "Refine","V10","Count","EltSize")

df <- select(df, -V2, -V4, -V5, -V8, -V10)
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 32,000 x 7
            algo queryId          bench      ms Refine Count EltSize
           <chr>   <int>          <chr>   <dbl>  <int> <int>  <fctr>
 1 GeoHashBinary       0 scan_at_region 17.0167    482    NA       0
 2 GeoHashBinary       0 scan_at_region 16.9375    482    NA       0
 3 GeoHashBinary       0 scan_at_region 16.9164    482    NA       0
 4 GeoHashBinary       0 scan_at_region 16.9788    482    NA       0
 5 GeoHashBinary       0 scan_at_region 16.9578    482    NA       0
 6 GeoHashBinary       0 scan_at_region 16.9516    482    NA       0
 7 GeoHashBinary       0 scan_at_region 16.9453    482    NA       0
 8 GeoHashBinary       0 scan_at_region 16.9122    482    NA       0
 9 GeoHashBinary       0 scan_at_region 16.8876    482    NA       0
10 GeoHashBinary       0 scan_at_region 16.9266    482    NA       0
# ... with 31,990 more rows
#+end_example

Fix the count for Rtrees
#+begin_src R :results output :exports both :session 
df %>%  
    mutate(Count = if_else(bench=="apply_at_region" & is.na(Count) , Refine, Count), # fix the count an Refine columns for Rtrees
           Refine = ifelse(grepl("RTree",algo), NA, Refine)) -> df
#+end_src

#+RESULTS:

Summarize the averages
#+begin_src R :results output :session :exports both
dfplot <- 
    df %>% 
    group_by_at(vars(-ms)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms))

dfplot %>% filter(queryId == 20, EltSize==0)
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 10 x 8
# Groups:   algo, queryId, bench, Refine, Count [10]
                  algo queryId           bench Refine  Count EltSize    avg_ms        stdv
                 <chr>   <int>           <chr>  <int>  <int>  <fctr>     <dbl>       <dbl>
 1               BTree      20 apply_at_region    108 214775       0 2.3666330 0.134292831
 2               BTree      20  scan_at_region    108     NA       0 4.3428940 0.082706779
 3       GeoHashBinary      20 apply_at_region    108 214775       0 0.5255329 0.024832940
 4       GeoHashBinary      20  scan_at_region    108     NA       0 1.3787500 0.026629616
 5 ImplicitDenseVector      20 apply_at_region    108 214775       0 0.3310479 0.003156074
 6 ImplicitDenseVector      20  scan_at_region    108     NA       0 0.8996339 0.029089231
 7               RTree      20 apply_at_region     NA 214775       0 3.0876000 0.297250249
 8               RTree      20  scan_at_region     NA     NA       0 5.4553120 0.033865678
 9           RTreeBulk      20 apply_at_region     NA 214775       0 0.4209257 0.025744708
10           RTreeBulk      20  scan_at_region     NA     NA       0 2.2189200 0.024514321
#+end_example

#+begin_src R :results output :exports both :session 
dfplot %>% filter(queryId == 10, bench == "scan_at_region", algo=="BTree") 
#+end_src

#+RESULTS:
: # A tibble: 4 x 8
: # Groups:   algo, queryId, bench, Refine, Count [1]
:    algo queryId          bench Refine Count EltSize   avg_ms       stdv
:   <chr>   <int>          <chr>  <int> <int>  <fctr>    <dbl>      <dbl>
: 1 BTree      10 scan_at_region    255    NA       0 16.54239 1.23092993
: 2 BTree      10 scan_at_region    255    NA     112 42.09688 0.05387862
: 3 BTree      10 scan_at_region    255    NA      16 19.67193 0.15769480
: 4 BTree      10 scan_at_region    255    NA      48 28.10382 0.11951415


*** Plot overview  - scan at region                                :export:

#+begin_src R :results output :exports both :session 
dfplot %>% 
    mutate( queryWidth = 90 / 2**(queryId %/% 10))  %>% 
    mutate(EltSize = as.numeric(as.character(EltSize)) + 16 ) -> dfplot

dfplot
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 3,200 x 9
# Groups:   algo, queryId, bench, Refine, Count [800]
    algo queryId           bench Refine   Count EltSize    avg_ms       stdv queryWidth
   <chr>   <int>           <chr>  <int>   <int>   <dbl>     <dbl>      <dbl>      <dbl>
 1 BTree       0 apply_at_region    482 3440580      16  40.82089 0.03144415         90
 2 BTree       0 apply_at_region    482 3440580     128  69.57817 1.63002373         90
 3 BTree       0 apply_at_region    482 3440580      32  52.45735 0.32611210         90
 4 BTree       0 apply_at_region    482 3440580      64  58.46813 1.31351381         90
 5 BTree       0  scan_at_region    482      NA      16  60.88873 0.03706559         90
 6 BTree       0  scan_at_region    482      NA     128 163.88020 0.05121805         90
 7 BTree       0  scan_at_region    482      NA      32  77.04593 0.14423432         90
 8 BTree       0  scan_at_region    482      NA      64 115.03220 0.25991529         90
 9 BTree       1 apply_at_region    519 3440446      16  41.48212 0.03550549         90
10 BTree       1 apply_at_region    519 3440446     128  70.75012 1.64405762         90
# ... with 3,190 more rows
#+end_example

#+begin_src R :results output graphics :file "./img/overview_query_region.png" :exports results :width 800 :height 600 :session 

myplot <- function(data) {
    data %>%
    ggplot(aes(x = as.factor(queryId), y = avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = avg_ms - stdv, ymax = avg_ms + stdv) ) +
    geom_point() +
    #acet_wrap(EltSize ~ queryWidth,scale="free", labeller = labeller(bench=c(apply_at_region="Count Query", scan_at_region="Scan Query"), `Query Width`=label_both)) + 
#    facet_wrap(~queryW,scale="free", labeller = "label_both") + 
    facet_wrap(EltSize~queryWidth,scale="free",nrow=4, labeller = "label_both") + 
    theme(legend.position = "bottom",)
}

dfplot %>% 
    filter(bench == "scan_at_region") %>% 
    myplot() -> p

ggsave("./img/overview_query_region.pdf")
p
#+end_src

#+RESULTS:
[[file:./img/overview_query_region.png]]


*** Scan Queries ordered by count


#+begin_src R :results output graphics :file "./img/scan_queries_by_size.png"  :exports results :width 1000 :height 600 :session 

myplot <- function(data) {
    data %>%
    ggplot(aes(x = as.factor(Count), group=algo, y = avg_ms, color = algo)) +  
    geom_errorbar(aes(ymin = avg_ms - stdv, ymax = avg_ms + stdv) ) +
    geom_line() +
    # geom_text(aes(label=queryId),color="black") +
    labs(title="Queries ordered by size of the result", x = "Element count of the query" ) +     
    facet_wrap( EltSize~queryWidth,
               scale="free", 
               nrow = 4 , 
               labeller = label_both)+
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 0.75))
        
}


dfplot %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,EltSize) %>%
    left_join( 
        filter(ungroup(dfplot), bench == "scan_at_region") %>% select(-Count)
   ) -> dfCount


dfCount %>% myplot() -> p1 
ggsave("./img/scan_queries_by_size.pdf")
p1

#+end_src

#+RESULTS:
[[file:./img/scan_queries_by_size.png]]




*** Scan Queries ordered by count
#+begin_src R :results output :exports both :session 
dfplot %>% 
    ungroup() %>%
    filter(bench == "apply_at_region") %>%
    select(queryId,algo,Count,EltSize) %>%
    left_join( 
        filter(ungroup(dfplot), bench == "scan_at_region") %>% select(-Count)
   ) -> dfCount

#+end_src


#+begin_src R :results output graphics :file "./img/tgp_queries_by_size.png"  :exports results :width 1000 :height 600 :session 

myplot <- function(data) {
    data %>%
    ggplot(aes(x = as.factor(Count), group=algo, y = AvgTgp, color = algo)) +  
#    geom_errorbar(aes(ymin = avg_ms - stdv, ymax = avg_ms + stdv) ) +
    geom_line() +
    # geom_text(aes(label=queryId),color="black") +
    labs(title="Queries ordered by size of the result", x = "Element count of the query" ) +     
    facet_wrap( EltSize~queryWidth,
               scale="free", 
               nrow = 4 , 
               labeller = label_both)+
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 0.75))
        
}


dfCount %>% mutate(AvgTgp = (Count/avg_ms)) %>%
myplot() -> p1 

ggsave("./img/tgp_queries_by_size.pdf")
#p1

#+end_src

#+RESULTS:
[[file:./img/tgp_queries_by_size.png]]



** What is the actual count of elements per query ?


*** Table                                                          :export:

Variance shows that some counts differ between algorithms:
#+begin_src R :results output :exports none :session :colnames yes

dfplot %>% 
    filter( bench== "apply_at_region") %>% 
    group_by(queryId) %>%                     #group to see if every algo has same coubts
    summarize(Var = round(var(Count),3)  ) -> 
    countVariation

options(dplyr.width = Inf)
dfplot %>% 
    filter( bench == "apply_at_region") %>%
    ungroup( bench) %>% # must ungroup to drop the column
    select( -bench, -stdv, -Refine) %>%
    gather(measure, value, Count, avg_ms) %>%
    unite(temp, algo, measure) %>%
    spread( temp, value) %>% 
    #select(queryId,ends_with("Count") , ends_with("ms")) %>%
    select(queryId,ends_with("Count") ) %>%
 #   filter( !(BTree_Count == GeoHashBinary_Count & RTreeBulk_Count == RTree_Count & BTree_Count == RTree_Count)) %>% 
    inner_join(countVariation) -> wideTable

#+end_src

#+RESULTS:
: Joining, by = "queryId"

#+CAPTION: Number of elements returned in each query
#+begin_src R :results table :exports results :session :colnames yes
wideTable %>%
    as_tibble() %>%
    print(n = nrow(.))
#+end_src

#+RESULTS:
| queryId | BTree_Count | GeoHashBinary_Count | RTreeBulk_Count | RTree_Count |   Var |
|---------+-------------+---------------------+-----------------+-------------+-------|
|       0 |     3440580 |             3440580 |         3440580 |     3440580 |     0 |
|       1 |     3440446 |             3440446 |         3440447 |     3440447 | 0.333 |
|       2 |     3438884 |             3438884 |         3438884 |     3438884 |     0 |
|       3 |     3440915 |             3440915 |         3440916 |     3440916 | 0.333 |
|       4 |     3442356 |             3442356 |         3442356 |     3442356 |     0 |
|       5 |     3439224 |             3439224 |         3439224 |     3439224 |     0 |
|       6 |     3438953 |             3438953 |         3438953 |     3438953 |     0 |
|       7 |     3442233 |             3442233 |         3442234 |     3442234 | 0.333 |
|       8 |     3441859 |             3441859 |         3441859 |     3441859 |     0 |
|       9 |     3443858 |             3443858 |         3443858 |     3443858 |     0 |
|      10 |      859819 |              859819 |          859819 |      859819 |     0 |
|      11 |      860304 |              860304 |          860304 |      860304 |     0 |
|      12 |      862004 |              862004 |          862004 |      862004 |     0 |
|      13 |      859895 |              859895 |          859895 |      859895 |     0 |
|      14 |      862262 |              862262 |          862263 |      862263 | 0.333 |
|      15 |      859189 |              859189 |          859189 |      859189 |     0 |
|      16 |      859264 |              859264 |          859266 |      859266 | 1.333 |
|      17 |      861935 |              861935 |          861935 |      861935 |     0 |
|      18 |      861341 |              861341 |          861341 |      861341 |     0 |
|      19 |      859799 |              859799 |          859799 |      859799 |     0 |
|      20 |      214775 |              214775 |          214776 |      214776 | 0.333 |
|      21 |      214220 |              214220 |          214220 |      214220 |     0 |
|      22 |      215543 |              215543 |          215543 |      215543 |     0 |
|      23 |      214932 |              214932 |          214932 |      214932 |     0 |
|      24 |      215726 |              215726 |          215726 |      215726 |     0 |
|      25 |      214526 |              214526 |          214526 |      214526 |     0 |
|      26 |      215502 |              215502 |          215502 |      215502 |     0 |
|      27 |      214199 |              214199 |          214199 |      214199 |     0 |
|      28 |      215471 |              215471 |          215471 |      215471 |     0 |
|      29 |      214738 |              214738 |          214738 |      214738 |     0 |
|      30 |       53488 |               53488 |           53488 |       53488 |     0 |
|      31 |       54129 |               54129 |           54129 |       54129 |     0 |
|      32 |       53212 |               53212 |           53212 |       53212 |     0 |
|      33 |       53584 |               53584 |           53584 |       53584 |     0 |
|      34 |       53724 |               53724 |           53724 |       53724 |     0 |
|      35 |       53825 |               53825 |           53825 |       53825 |     0 |
|      36 |       53856 |               53856 |           53856 |       53856 |     0 |
|      37 |       53236 |               53236 |           53236 |       53236 |     0 |
|      38 |       53837 |               53837 |           53837 |       53837 |     0 |
|      39 |       53767 |               53767 |           53767 |       53767 |     0 |
|      40 |       13230 |               13230 |           13230 |       13230 |     0 |
|      41 |       13399 |               13399 |           13400 |       13400 | 0.333 |
|      42 |       13513 |               13513 |           13514 |       13514 | 0.333 |
|      43 |       13251 |               13251 |           13251 |       13251 |     0 |
|      44 |       13524 |               13524 |           13524 |       13524 |     0 |
|      45 |       13356 |               13356 |           13356 |       13356 |     0 |
|      46 |       13401 |               13401 |           13401 |       13401 |     0 |
|      47 |       13530 |               13530 |           13530 |       13530 |     0 |
|      48 |       13417 |               13417 |           13417 |       13417 |     0 |
|      49 |       13298 |               13298 |           13298 |       13298 |     0 |
|      50 |        3358 |                3358 |            3358 |        3358 |     0 |
|      51 |        3304 |                3304 |            3304 |        3304 |     0 |
|      52 |        3517 |                3517 |            3517 |        3517 |     0 |
|      53 |        3338 |                3338 |            3338 |        3338 |     0 |
|      54 |        3394 |                3394 |            3394 |        3394 |     0 |
|      55 |        3353 |                3353 |            3353 |        3353 |     0 |
|      56 |        3356 |                3356 |            3357 |        3357 | 0.333 |
|      57 |        3440 |                3440 |            3440 |        3440 |     0 |
|      58 |        3455 |                3455 |            3455 |        3455 |     0 |
|      59 |        3461 |                3461 |            3461 |        3461 |     0 |
|      60 |         842 |                 842 |             842 |         842 |     0 |
|      61 |         808 |                 808 |             808 |         808 |     0 |
|      62 |         840 |                 840 |             840 |         840 |     0 |
|      63 |         834 |                 834 |             834 |         834 |     0 |
|      64 |         839 |                 839 |             839 |         839 |     0 |
|      65 |         852 |                 852 |             852 |         852 |     0 |
|      66 |         797 |                 797 |             797 |         797 |     0 |
|      67 |         843 |                 843 |             843 |         843 |     0 |
|      68 |         813 |                 813 |             813 |         813 |     0 |
|      69 |         895 |                 895 |             895 |         895 |     0 |
|      70 |         225 |                 225 |             225 |         225 |     0 |
|      71 |         184 |                 184 |             184 |         184 |     0 |
|      72 |         209 |                 209 |             209 |         209 |     0 |
|      73 |         199 |                 199 |             199 |         199 |     0 |
|      74 |         212 |                 212 |             212 |         212 |     0 |
|      75 |         222 |                 222 |             222 |         222 |     0 |
|      76 |         213 |                 213 |             213 |         213 |     0 |
|      77 |         192 |                 192 |             192 |         192 |     0 |
|      78 |         196 |                 196 |             196 |         196 |     0 |
|      79 |         188 |                 188 |             188 |         188 |     0 |
#+TBLFM: $6=$0;%0.3f



Just the diverging queries : 
#+begin_src R :results table :exports results :session :colnames yes

wideTable %>%
    filter ( Var > 0) %>%            #get only the queryIds with variance greater that zero 
    as_tibble() %>%
    print(n = nrow(.))

#+end_src

#+CAPTION: Queries that returned different result depending on the algorithm 
#+RESULTS:
| queryId | BTree_Count | GeoHashBinary_Count | RTreeBulk_Count | RTree_Count |   Var |
|---------+-------------+---------------------+-----------------+-------------+-------|
|       1 |     3440446 |             3440446 |         3440447 |     3440447 | 0.333 |
|       3 |     3440915 |             3440915 |         3440916 |     3440916 | 0.333 |
|       7 |     3442233 |             3442233 |         3442234 |     3442234 | 0.333 |
|      14 |      862262 |              862262 |          862263 |      862263 | 0.333 |
|      16 |      859264 |              859264 |          859266 |      859266 | 1.333 |
|      20 |      214775 |              214775 |          214776 |      214776 | 0.333 |
|      41 |       13399 |               13399 |           13400 |       13400 | 0.333 |
|      42 |       13513 |               13513 |           13514 |       13514 | 0.333 |
|      56 |        3356 |                3356 |            3357 |        3357 | 0.333 |


*** Plot                                                           :export:

There are some queries where the count differs for Rtree by a small amount of elements.

Counts have some differences :
#+begin_src R :results output :exports none :session 
options(dplyr.width = Inf)
dfplot %>% 
    filter( bench== "apply_at_region") %>% 
    group_by(queryId, bench) %>% #group to see if every algo has same counts
    summarize(c = mean(Count), s = sd(Count)  ) %>% 
    filter ( s > 0) %>% 
    select(queryId, bench) %>% 
    left_join(dfplot) -> dfWrongCounts

#+end_src

#+RESULTS:
: Joining, by = c("queryId", "bench")


These are the queries that for some misterious reason resulted in different counts.
#+begin_src R :results output graphics :file "./img/differing_counts.png" :exports results :width 600 :height 400 :session 

myplot <- function(data) {
    data %>%
   #     mutate(`Query Width` = 90 / 2**(queryId %/% 10)) %>%
        ggplot(aes(x = as.factor(algo), y = Count, color = algo))+
# as.numeric(labels(as.factor(unique(algo))))), y = Count, color = algo)) +  
        #geom_jitter( width=0.1, height=0) +
        geom_point( ) +
        facet_wrap(~queryId,scale="free", labeller = "label_both") + 
        theme(legend.position = "bottom",) + 
#        labs(x = "Query width (degrees)") +
        #scale_y_continuous(breaks=c(3440446,3440447) )
        scale_y_continuous(breaks=seq(min(data$Count),max(data$Count) ))
    
}

#dfWrongCounts %>% myplot() 

dfWrongCounts %>% myplot()

#dfWrongCounts %>% 
#group_by(queryId) %>% filter(queryId == 1 ) %>%
#mutate(y_min = min(Count), y_max = max(Count)) %>% myplot()
#+end_src

#+RESULTS:
[[file:./img/differing_counts.png]]

