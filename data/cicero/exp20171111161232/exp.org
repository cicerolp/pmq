# -*- org-export-babel-evaluate: t; -*-
#+TITLE: Benchmark Queries - Twitter Dataset
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n) ignore(n) export(e)
#+CATEGORY: exp
#+OPTIONS: ^:{} todo:nil H:4 toc:t tags:nil author:nil
#+PROPERTY: header-args :cache no :eval never-export 


* TODO Description                                                   :export:

Test to check better refinemnet levels. 

- PMQ / GEOHASH
- BTREE 
  
- test with twitter dataset


- Total elements = T * 1000  
  
Use the synthetic queries generated by the DoE in [[file:~/Projects/pmq/data/queriesLHS.org::#queries20170923145357][Twitter Data]].

queries Dataset : [[file:~/Projects/pmq/data/queriesTwitter.csv]]

** Standalone script                                              :noexport:
To generate the results outside emacs and orgmode you can use the standalone scripts, generated from the tangled source blocks in this file

- parse.sh : parse the results to CSV
- plotResults.R : generate the plots 
  

* TODO Experiment Script
** DONE Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp20171111161232

Set up git branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout master
git commit ../../../LabBook.org -m "LBK: new entry for ${expId}"
#+end_src

#+RESULTS:
: M	LabBook.org
: Your branch is ahead of 'origin/master' by 2 commits.
:   (use "git push" to publish your local commits)
: [master bd2ed0d] LBK: new entry for exp20171111161232
:  1 file changed, 35 insertions(+)

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171111161232
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.#exp.org
	exp.org

nothing added to commit but untracked files present (use "git add" to track)
[exp20171111161232 d06e7c2] Initial commit for exp20171111161232
 1 file changed, 1068 insertions(+)
 create mode 100644 data/cicero/exp20171111161232/exp.org
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * d06e7c2 (HEAD -> exp20171111161232) Initial commit for exp20171111161232
: * bd2ed0d (master) LBK: new entry for exp20171111161232
: | * 8a4aa51 (temp) Replicate test with random tweets on my machine

** DONE Export run script 

- Test refinements levels on twitter dataset 

#+begin_src sh :results output :exports both

for I in 1 2 4 8 16 32 64 128 ; do
    T=$(($I * 1000))
    echo "$T"
done
#+end_src

#+RESULTS:
: 1000
: 2000
: 4000
: 8000
: 16000
: 32000
: 64000
: 128000

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/pmq/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
# workaround as :var arguments are not been correctly tangled by my orgmode
#expId=$(basename $(pwd) | sed 's/exp//g')
expId=$(basename $(pwd))
TMPDIR=/dev/shm/$expId

# generate output name
if [ $1 ] ; then 
    EXECID=$1
else
    EXECID=$(date +%s)
fi

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# make pma
mkdir -p $PMABUILD_DIR
cd $PMABUILD_DIR
cmake -DCMAKE_BUILD_TYPE="Release" -DTWITTERVIS=OFF -DRHO_INIT=OFF ../pma_cd
make 

# make twitterVis
mkdir -p $BUILDIR
cd $BUILDIR 
cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DELT_SIZE=0 -DCMAKE_BUILD_TYPE="Release" -DBENCH_PMQ=ON -DBENCH_BTREE=ON -DBENCH_RTREE=OFF -DBENCH_DENSE=OFF -DBENCH_RTREE_BULK=OFF ..
make

#get machine configuration
echo "" > $DATADIR/info.org
~/Projects/pmq/scripts/g5k_get_info.sh $DATADIR/info.org 

# EXECUTE BENCHMARK

#Continue execution even if one these fails
set +e 

#Run queries
b=1000
#n=$(($t*$b))
#ref=8

    
listRef=$(seq 1 14)
for ref in $listRef
    for i in 1 2 ; do
        t=$(($i * 1000))
        stdbuf -oL ./benchmarks/bench_queries_region -f ../data/geo-tweets.dat -x 10 -rate ${b} -min_t ${t} -max_t ${t} -ref ${ref} -bf ../data/queriesTwitter.csv >  ${TMPDIR}/bench_queries_region_twitter_${t}_${b}_${ref}_${EXECID}.log
    done
done

set -e

cd $TMPDIR
tar -cvzf log_$EXECID.tgz *_$EXECID.log

cd $DATADIR
cp $TMPDIR/log_$EXECID.tgz .

git checkout $expId

git add info.org log_$EXECID.tgz run.sh 
git add -u
git commit -m "Finish execution $EXECID"
#git push origin $expId
#+end_src 


** DONE Commit local changes
#+begin_src sh :results output :exports both
git status .
#+end_src

#+RESULTS:
#+begin_example
On branch exp20171111161232
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	run.sh

no changes added to commit (use "git add" and/or "git commit -a")
#+end_example

#+begin_src sh :results output :exports both
git add run.sh exp.org
git commit -m "UPD: run.sh script"
#git commit --amend -m "UPD: run.sh script"
#+end_src

#+RESULTS:
: [exp20171111161232 ebb4318] UPD: run.sh script
:  2 files changed, 102 insertions(+), 21 deletions(-)
:  create mode 100755 data/cicero/exp20171111161232/run.sh

Push to remote (directly to the experiment machine)
#+begin_src sh :results output :exports both :var expId=expId
#git push bitbucket $expId
#git push origin $expId
git push cicero $expId
#+end_src

#+RESULTS:

** CANCELED Local Execution                                          :local:
:LOGBOOK:
- State "CANCELED"   from "TODO"       [2017-09-05 Ter 19:00]
:END:

#+begin_src sh :results output :exports both :session local :var expId=expId
cd ~/Projects/pmq/data/$(hostname)/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/$(hostname)/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

** DONE Remote Execution                                            :remote:

*** Get new changes on remote                                      :remote:
#+begin_src sh :session remote :results output :exports both 
ssh -A cicero
#+end_src

#+RESULTS:
#+begin_example
julio@cicero's password: 
Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 ,* Documentation:  https://help.ubuntu.com
 ,* Management:     https://landscape.canonical.com
 ,* Support:        https://ubuntu.com/advantage

115 packages can be updated.
0 updates are security updates.

,*** System restart required ***
Last login: Sat Nov 11 16:21:42 2017 from 143.54.11.6
#+end_example

Get the last script on the remote machine (require entering a password
for bitbucket)
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/pmq/
#git config --add remote.origin.fetch refs/heads/$expId:refs/remotes/origin/$expId
#git fetch origin $expId
git checkout $expId
#git pull origin $expId
#git log -1 | cat 
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ julio@cicero:~/Projects/pmq$ Switched to branch 'exp20171111161232'

Update PMA repository on exp machine
#+begin_src sh :session remote :results output :exports both :var expId=expId
cd ~/Projects/hppsimulations/
git pull origin PMA_2016
git log -1 | cat
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/hppsimulations$ From bitbucket.org:joaocomba/pma
: FETCH_HEAD
: Already up-to-date.
: commit 6931408d8b9c109f3f2a9543374cfd712791b1e7
: Date:   Tue Sep 19 16:58:38 2017 -0300
: 
:     error ouput on pma initialization

*** DONE Execute Remotely                                          :remote:

Opens ssh connection and a tmux session

#+begin_src sh :results output :exports both :session remote :var expId=expId
cd ~/Projects/pmq/data/cicero/$expId
runid=$(date +%s)
tmux new -d -s runExp "cd ~/Projects/pmq/data/cicero/$expId; ./run.sh ${runid} &> run_${runid}"
git add run_$runid
echo $runid
#+end_src

#+RESULTS:
: 
: julio@cicero:~/Projects/pmq/data/cicero/exp20171111161232$ julio@cicero:~/Projects/pmq/data/cicero/exp20171111161232$ julio@cicero:~/Projects/pmq/data/cicero/exp20171111161232$ julio@cicero:~/Projects/pmq/data/cicero/exp20171111161232$ 1510783350

Check process running
#+begin_src sh :results output :exports both :session remote
tmux ls
ps ux
#+end_src

#+RESULTS:
#+begin_example
no server running on /tmp/tmux-1001/default
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
julio     1893  0.0  0.0  45248  4464 ?        Ss   16:47   0:00 /lib/systemd/systemd --user
julio     1895  0.0  0.0 145352  2100 ?        S    16:47   0:00 (sd-pam)
julio     2042  0.0  0.0  97496  3284 ?        R    16:49   0:00 sshd: julio@pts/9
julio     2043  0.0  0.0  22676  4876 pts/9    Ss   16:49   0:00 -bash
julio     2063  0.0  0.0  44920  5156 pts/9    R+   16:49   0:00 ssh -A cicero
julio     2091  0.0  0.0  97576  3264 ?        S    16:49   0:00 sshd: julio@pts/10
julio     2092  0.0  0.0  22716  5204 pts/10   Ss   16:49   0:00 -bash
julio     3154  0.0  0.0  97576  3384 ?        S    19:58   0:00 sshd: julio@pts/8
julio     3155  0.0  0.0  22824  5516 pts/8    Ss+  19:58   0:00 -bash
julio     3997  0.0  0.0  37368  3216 pts/10   R+   22:32   0:00 ps ux
#+end_example

**** DONE Pull local 
#+begin_src sh :results output :exports both :var expId=expId
#git commit -a -m "wip"
#git status
#git pull --rebase origin $expId
git pull cicero $expId
#+end_src

#+RESULTS:
#+begin_example
[exp20171012184842 37984b2] wip
 1 file changed, 29 insertions(+), 26 deletions(-)
On branch exp20171012184842
Untracked files:
  (use "git add <file>..." to include in what will be committed)

	../../../LabBook.man
	../../../LabBook.markdown_phpextra
	../../../LabBook.md
	../../../LabBook.rst
	../../../LabBook.rtf
	../../../LabBook.txt
	../../../LabBook_BACKUP_19287.md
	../../../LabBook_BACKUP_19287.org
	../../../LabBook_BASE_19287.org
	../../../LabBook_LOCAL_19287.org
	../../../LabBook_REMOTE_19287.org
	../../../README.html
	../exp20170825181747/
	../exp20170830124159/
	../exp20170907105314/
	../exp20170907105804/
	../exp20170907112116/
	../exp20170907145711/
	../exp20170914091842/
	../exp20170915143003/
	../exp20170919161448/
	../exp20170923144931/
	../exp20170923193058/
	../exp20171009155025/
	exp.pdf
	exp.tex
	img/
	../../queriesLHS.html
	../../queriesLHS_BACKUP_23848.org
	../../queriesLHS_BASE_23848.org
	../../queriesLHS_LOCAL_23848.org
	../../queriesLHS_REMOTE_23848.org
	../../randomLhsQueries.png
	../../../history.txt
	../../../qqqq

nothing added to commit but untracked files present (use "git add" to track)
First, rewinding head to replay your work on top of it...
Applying: wip
#+end_example

* Refinement Level Analysis
** DONE Generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+begin_src sh :results table :exports both
ls -htl *tgz
#+end_src

#+RESULTS:
| -rw-r--r-- 1 julio julio 669K nov 11 16:51 log_1510425176.tgz |

#+NAME: logFile
#+begin_src sh :results table :exports both 
tar xvzf log_*.tgz
#+end_src

#+RESULTS: logFile
| bench_queries_region_twitter_1000_1000_10_1510425176.log |
| bench_queries_region_twitter_1000_1000_11_1510425176.log |
| bench_queries_region_twitter_1000_1000_1_1510425176.log  |
| bench_queries_region_twitter_1000_1000_12_1510425176.log |
| bench_queries_region_twitter_1000_1000_13_1510425176.log |
| bench_queries_region_twitter_1000_1000_14_1510425176.log |
| bench_queries_region_twitter_1000_1000_2_1510425176.log  |
| bench_queries_region_twitter_1000_1000_3_1510425176.log  |
| bench_queries_region_twitter_1000_1000_4_1510425176.log  |
| bench_queries_region_twitter_1000_1000_5_1510425176.log  |
| bench_queries_region_twitter_1000_1000_6_1510425176.log  |
| bench_queries_region_twitter_1000_1000_7_1510425176.log  |
| bench_queries_region_twitter_1000_1000_8_1510425176.log  |
| bench_queries_region_twitter_1000_1000_9_1510425176.log  |
| bench_queries_region_twitter_2000_1000_10_1510425176.log |
| bench_queries_region_twitter_2000_1000_11_1510425176.log |
| bench_queries_region_twitter_2000_1000_1_1510425176.log  |
| bench_queries_region_twitter_2000_1000_12_1510425176.log |
| bench_queries_region_twitter_2000_1000_13_1510425176.log |
| bench_queries_region_twitter_2000_1000_14_1510425176.log |
| bench_queries_region_twitter_2000_1000_2_1510425176.log  |
| bench_queries_region_twitter_2000_1000_3_1510425176.log  |
| bench_queries_region_twitter_2000_1000_4_1510425176.log  |
| bench_queries_region_twitter_2000_1000_5_1510425176.log  |
| bench_queries_region_twitter_2000_1000_6_1510425176.log  |
| bench_queries_region_twitter_2000_1000_7_1510425176.log  |
| bench_queries_region_twitter_2000_1000_8_1510425176.log  |
| bench_queries_region_twitter_2000_1000_9_1510425176.log  |

Create CSV using logFile 

#+NAME: csvFile
#+begin_src sh :results output :exports both :var logFileList=logFile

#echo $logFile | sed 's/bench_queries_region_random_10000_100_\([[:digit:]]\)//g'
for logFile in $logFileList
do
echo $(basename -s .log $logFile ).csv
grep "GeoHashBinary\|BTree" $logFile | grep "query" | sed "s/QueryBench//g" >  $(basename -s .log $logFile ).csv
done

#+end_src

#+RESULTS: csvFile
: bench_queries_region_twitter_1000_1000_10_1510783350.csv
: bench_queries_region_twitter_128000_1000_10_1510783350.csv
: bench_queries_region_twitter_16000_1000_10_1510783350.csv
: bench_queries_region_twitter_2000_1000_10_1510783350.csv
: bench_queries_region_twitter_32000_1000_10_1510783350.csv
: bench_queries_region_twitter_4000_1000_10_1510783350.csv
: bench_queries_region_twitter_64000_1000_10_1510783350.csv
: bench_queries_region_twitter_8000_1000_10_1510783350.csv


Create an director for images
#+begin_src sh :results output :exports both :tangle no
mkdir img
#+end_src

#+RESULTS:

** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:END:      


*** Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile path=(print default-directory)
setwd(path)
library(tidyverse)

                                        # Reads a csv file and add a column identifying the csv by parsin its name
readAdd <- function(input){

return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:11),sep="") ) %>%
         mutate (
             ref = as.factor(
                 gsub("bench_queries_region_twitter_[[:digit:]]+_1000_([[:digit:]]+)_.*","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]

df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

#+end_src

#+RESULTS:
#+begin_example
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_11_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_11_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_11_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_11_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_11_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_1_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_1_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_1_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_1_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_1_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_12_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_12_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_12_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_12_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_12_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_13_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_13_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_13_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_13_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_13_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_14_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_14_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_14_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_14_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_14_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_2_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_2_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_2_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_2_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_2_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_3_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_3_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_3_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_3_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_3_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_4_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_4_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_4_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_4_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_4_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_5_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_5_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_5_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_5_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_5_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_6_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_6_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_6_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_6_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_6_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_7_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_7_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_7_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_7_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_7_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_8_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_9_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_9_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_9_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_9_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_1000_1000_9_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_11_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_11_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_11_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_11_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_11_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_double()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_1_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_1_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_1_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_1_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_1_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_12_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_12_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_12_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_12_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_12_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_13_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_13_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_13_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_13_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_13_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_14_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_14_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_14_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_14_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_14_1510425176.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_double()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_2_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_2_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_2_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_2_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_2_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_double()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_3_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_3_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_3_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_3_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_3_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_double()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_4_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_4_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_4_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_4_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_4_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_5_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_5_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_5_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_5_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_5_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_6_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_6_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_6_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_6_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_6_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_7_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_7_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_7_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_7_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_7_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_8_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
Parsed with column specification:
cols(
  V1 = col_character(),
  V2 = col_character(),
  V3 = col_integer(),
  V4 = col_logical(),
  V5 = col_integer(),
  V6 = col_character(),
  V7 = col_double(),
  V8 = col_character(),
  V9 = col_integer(),
  V10 = col_character(),
  V11 = col_integer()
)
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                      file expected   <int> <chr>      <chr>      <chr>                                                     <chr> actual 1     1  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_9_1510425176.csv' file 2     2  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_9_1510425176.csv' row 3     3  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_9_1510425176.csv' col 4     4  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_9_1510425176.csv' expected 5     5  <NA> 11 columns 12 columns 'bench_queries_region_twitter_2000_1000_9_1510425176.csv'
... ................. ... ............................................................................................. ........ ............................................................................................. ...... ......................... [... truncated]
There were 28 warnings (use warnings() to see them)
#+end_example

Remove useless columns
#+begin_src R :results output :exports both :session 

names(df) <- c("algo" , "V2" , "queryId",  "V4" , "T"  ,"bench" , "ms" , "V8", "refinements" , "V10", "Count", "refLevel")

df <- select(df, -V2, -V4, -V8, -V10)

str(df)
#+end_src

#+RESULTS:
: Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	89600 obs. of  8 variables:
:  $ algo       : chr  "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" ...
:  $ queryId    : int  0 0 0 0 0 0 0 0 0 0 ...
:  $ T          : int  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...
:  $ bench      : chr  "apply_at_region" "apply_at_region" "apply_at_region" "apply_at_region" ...
:  $ ms         : num  0.357 0.355 0.361 0.353 0.352 ...
:  $ refinements: int  69 69 69 69 69 69 69 69 69 69 ...
:  $ Count      : num  924827 924827 924827 924827 924827 ...
:  $ refLevel   : Factor w/ 14 levels "10","11","1",..: 1 1 1 1 1 1 1 1 1 1 ...

Summarize the averages
#+begin_src R :results output :session :exports both
dfplot <- 
    df %>% 
    group_by_at(vars(-ms, -refinements)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms)) %>%
    mutate(refLevel = as.integer(as.character(refLevel))) %>%
    arrange(refLevel) 

dfplot 
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 8,960 x 8
# Groups:   algo, queryId, T, bench, Count [1,822]
    algo queryId     T           bench   Count refLevel   avg_ms       stdv
   <chr>   <int> <int>           <chr>   <dbl>    <int>    <dbl>      <dbl>
 1 BTree       0  1000 apply_at_region  924827        1 17.88266 0.31246492
 2 BTree       0  1000  scan_at_region      NA        1 24.76905 0.01259605
 3 BTree       0  2000 apply_at_region 1855890        1 38.20375 0.16051378
 4 BTree       0  2000  scan_at_region      NA        1 51.31576 0.01120716
 5 BTree       1  1000 apply_at_region  929918        1 17.78989 0.26006714
 6 BTree       1  1000  scan_at_region      NA        1 24.80026 0.02786073
 7 BTree       1  2000 apply_at_region 1866101        1 38.11300 0.03965109
 8 BTree       1  2000  scan_at_region      NA        1 51.47506 0.02106430
 9 BTree       2  1000 apply_at_region  753921        1 17.52672 0.10294663
10 BTree       2  1000  scan_at_region      NA        1 23.90354 0.01953829
# ... with 8,950 more rows
#+end_example


*** PLot time Vs RefLevel T = 1000
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session 

dfplot %>% 
    filter( T == 1000) %>%
#    mutate(queryW = queryId %/% 10) 
#    filter(queryId < 10) %>%
    ggplot(aes(x = refLevel,avg_ms,color=as.factor(queryId))) + 
 #   geom_errorbar(aes(ymin = avg_ms - stdv, ymax = avg_ms + stdv) ) +
    geom_line() +
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
    facet_grid(bench~algo) 
  #  facet_wrap(algo~bench,nrow=2)

#+end_src

#+RESULTS:
[[file:/tmp/babel-17282Hv/figure1728_0R.png]]

Note the the smaller queryIDs have a larger area of selection 
[[file:~/Projects/pmq/data/queriesLHS.org::*Coordinates%20LHS%20To%20avoid%20out-of-bound%20queries%20+%20LAPPLY%20+%20RBIND][Coordinates LHS To avoid out-of-bound queries + LAPPLY + RBIND]]


Group by query width
#+begin_src R :results output :exports both :session 

dfplot %>% 
    filter( T == 1000) %>%
    mutate(queryW = queryId %/% 10) %>%
    group_by(algo,queryW,bench,refLevel) %>%
    summarize(time = mean(avg_ms), sdtdv = sd(avg_ms))

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 448 x 6
# Groups:   algo, queryW, bench [?]
    algo queryW           bench refLevel     time     sdtdv
   <chr>  <dbl>           <chr>    <int>    <dbl>     <dbl>
 1 BTree      0 apply_at_region        1 17.77716 0.4177340
 2 BTree      0 apply_at_region        2 17.61552 0.3825027
 3 BTree      0 apply_at_region        3 14.08434 1.3392955
 4 BTree      0 apply_at_region        4 12.95060 1.6072631
 5 BTree      0 apply_at_region        5 12.00733 0.7700920
 6 BTree      0 apply_at_region        6 11.54523 0.4004386
 7 BTree      0 apply_at_region        7 10.72311 0.6150486
 8 BTree      0 apply_at_region        8 10.54572 0.5945493
 9 BTree      0 apply_at_region        9 10.51401 0.6934013
10 BTree      0 apply_at_region       10 10.65255 0.8656675
# ... with 438 more rows
#+end_example


#+begin_src R :results output graphics :file "./img/Reflevel.png" :exports both :width 600 :height 400 :session 

dfplot %>% 
    filter( T == 1000) %>%
    mutate(queryW = 90 / 2**(queryId %/% 10)) %>%
    group_by(algo,queryW,bench,refLevel) %>%
    summarize(time = mean(avg_ms), stdv = sd(avg_ms)) %>%
    ggplot(aes(x = refLevel,time,color=as.factor(queryW))) + 
 #   geom_errorbar(aes(ymin = time - stdv, ymax = time + stdv) ) +
    geom_line() +
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
    scale_y_continuous(trans = scales::log2_trans()) +
    labs(colour="Query Width\n (Degrees)", y = "times ms (log2 scale)") +
    facet_grid(bench~algo)
  #  facet_wrap(algo~bench,nrow=2)

#+end_src

#+RESULTS:
[[file:./img/Reflevel.png]]


#+begin_src R :results output graphics :file "./img/Reflevel2k.png" :exports both :width 600 :height 400 :session 

dfplot %>% 
    filter( T == 2000) %>%
    mutate(queryW = 90 / 2**(queryId %/% 10)) %>%
    group_by(algo,queryW,bench,refLevel) %>%
    summarize(time = mean(avg_ms), stdv = sd(avg_ms)) %>%
    ggplot(aes(x = refLevel,time,color=as.factor(queryW))) + 
 #   geom_errorbar(aes(ymin = time - stdv, ymax = time + stdv) ) +
    geom_line() +
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
    scale_y_continuous(trans = scales::log2_trans()) +
    labs(colour="Query Width\n (Degrees)", y = "times ms (log2 scale)") +
    facet_grid(bench~algo)
  #  facet_wrap(algo~bench,nrow=2)

#+end_src

#+RESULTS:
[[file:./img/Reflevel2k.png]]

#+begin_src R :results output graphics :file "./img/refLevelAnalysis.pdf" :exports both :width 10 :height 5 :session :var path=(print default-directory)
setwd(path)


algo_labels <- c(BTree = "BTree", GeoHashBinary = "PMQ")

dfplot %>% 
    filter( T == 1000) %>%
    filter( bench == "scan_at_region", algo == "GeoHashBinary") %>% 
    mutate(queryW = 90 / 2**(queryId %/% 10)) %>%
    ggplot( aes(x = refLevel,avg_ms, color=as.factor(queryW), group=(queryId))) + 
    theme_bw() + 
    geom_line() +
    geom_vline(xintercept=10, linetype="dashed") + 
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
    scale_y_continuous(trans = scales::log2_trans(),
                       labels = scales::trans_format("log2", scales::math_format(2^.x))) +
    labs(colour="Query Width (Degrees)", 
         y = "Query execution time in (ms)" ,
         x = "Geohash refinement level") +
    facet_wrap(~queryW,
               scale="free_y",
               labeller = labeller(algo=algo_labels))+
    theme(legend.position = "bottom",
          axis.text = element_text(size = 12),
          axis.title = element_text(size = 14),
          legend.text = element_text( size = 12),
          legend.title = element_text( size = 12),
          strip.text = element_text( size = 12)) +
    guides(color=guide_legend(nrow=1,byrow=TRUE))

#+end_src

#+RESULTS:
[[file:./img/refLevelAnalysis.pdf]]







** Conclusions

+We can justify that we verify in our experiments that Z = 8 gives in general the best performance.+

+Z  = number of quadtree refinements of the query algorithm+

In the case of irregular dataset there is no best threshold for every one. 


** Cache configurations

#+begin_src sh :session cicero :results output :exports both 
ssh cicero
#+end_src

#+begin_src sh :session cicero :results output :exports both 
sudo lshw -class memory
#+end_src

#+RESULTS:
#+begin_example
  ,*-cache:0
       description: L1 cache
       physical id: 3e
       slot: CPU Internal L1
       size: 256KiB
       capacity: 256KiB
       capabilities: synchronous internal write-back
       configuration: level=1
  ,*-cache:1
       description: L2 cache
       physical id: 3f
       slot: CPU Internal L2
       size: 1MiB
       capacity: 1MiB
       capabilities: synchronous internal write-back unified
       configuration: level=2
  ,*-cache:2
       description: L3 cache
       physical id: 40
       slot: CPU Internal L3
       size: 8MiB
       capacity: 8MiB
       capabilities: synchronous internal write-back unified
       configuration: level=3
  ,*-memory
       description: System Memory
       physical id: 42
       slot: System board or motherboard
       size: 32GiB
     ,*-bank:0
          description: DIMM DDR3 Synchronous 1333 MHz (0,8 ns)
          product: KHX1866C10D3/8G
          vendor: Kingston
          physical id: 0
          serial: 4D1CB889
          slot: ChannelA-DIMM0
          size: 8GiB
          width: 64 bits
          clock: 1333MHz (0.8ns)
     ,*-bank:1
          description: DIMM DDR3 Synchronous 1333 MHz (0,8 ns)
          product: KHX1866C10D3/8G
          vendor: Kingston
          physical id: 1
          serial: 681CDAB1
          slot: ChannelA-DIMM1
          size: 8GiB
          width: 64 bits
          clock: 1333MHz (0.8ns)
     ,*-bank:2
          description: DIMM DDR3 Synchronous 1333 MHz (0,8 ns)
          product: KHX1866C10D3/8G
          vendor: Kingston
          physical id: 2
          serial: 681CC6B1
          slot: ChannelB-DIMM0
          size: 8GiB
          width: 64 bits
          clock: 1333MHz (0.8ns)
     ,*-bank:3
          description: DIMM DDR3 Synchronous 1333 MHz (0,8 ns)
          product: KHX1866C10D3/8G
          vendor: Kingston
          physical id: 3
          serial: 6B399525
          slot: ChannelB-DIMM1
          size: 8GiB
          width: 64 bits
          clock: 1333MHz (0.8ns)
#+end_example

lscpu
#+begin_src sh :session cicero :results output :exports both 
lscpu
#+end_src

#+RESULTS:
#+begin_example
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    2
Core(s) per socket:    4
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 60
Model name:            Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz
Stepping:              3
CPU MHz:               844.453
CPU max MHz:           4000,0000
CPU min MHz:           800,0000
BogoMIPS:              7199.13
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              8192K
NUMA node0 CPU(s):     0-7
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts
#+end_example

/proc/cpuinfo
#+begin_src sh :session cicero :results output :exports both 
cat /proc/cpuinfo | head -n 27
#+end_src

#+RESULTS:
#+begin_example
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 60
model name	: Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz
stepping	: 3
microcode	: 0x22
cpu MHz		: 800.015
cache size	: 8192 KB
physical id	: 0
siblings	: 8
core id		: 0
cpu cores	: 4
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts
bugs		:
bogomips	: 7199.13
clflush size	: 64
cache_alignment	: 64
address sizes	: 39 bits physical, 48 bits virtual
power management:
#+end_example

http://techreport.com/review/24879/intel-core-i7-4770k-and-4950hq-haswell-processors-reviewed
http://www.hotchips.org/wp-content/uploads/hc_archives/hc25/HC25.80-Processors2-epub/HC25.27.820-Haswell-Hammarlund-Intel.pdf


Use the LLC cache as parameter for refinement (8192 KB))

Elements size:
key + data = 4 + 16 bytes = 20
 
 src_R{8192 * 1000 / 20 } {{{results(=409600=)}}}
 
** Element counts on small queries

#+begin_src R :results output :exports both :session 
dfplot %>% 
    filter( algo == "GeoHashBinary") %>% 
    filter( bench == "apply_at_region") %>%
    filter( queryId %in% c(51) )
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 14 x 8
# Groups:   algo, queryId, T, bench, Count [1]
            algo queryId     T           bench Count refLevel    avg_ms         stdv
           <chr>   <int> <int>           <chr> <dbl>    <int>     <dbl>        <dbl>
 1 GeoHashBinary      51  1000 apply_at_region  2377        1 0.2179745 0.0055641255
 2 GeoHashBinary      51  1000 apply_at_region  2377        2 0.2202932 0.0063527706
 3 GeoHashBinary      51  1000 apply_at_region  2377        3 0.2303760 0.0076044619
 4 GeoHashBinary      51  1000 apply_at_region  2377        4 0.2221759 0.0042164115
 5 GeoHashBinary      51  1000 apply_at_region  2377        5 0.1658692 0.0055438127
 6 GeoHashBinary      51  1000 apply_at_region  2377        6 0.1544180 0.0059785325
 7 GeoHashBinary      51  1000 apply_at_region  2377        7 0.0697843 0.0035846630
 8 GeoHashBinary      51  1000 apply_at_region  2377        8 0.0540278 0.0028638843
 9 GeoHashBinary      51  1000 apply_at_region  2377        9 0.0380415 0.0005814945
10 GeoHashBinary      51  1000 apply_at_region  2377       10 0.0403815 0.0007826548
11 GeoHashBinary      51  1000 apply_at_region  2377       11 0.0218341 0.0006465239
12 GeoHashBinary      51  1000 apply_at_region  2377       12 0.0328534 0.0006727750
13 GeoHashBinary      51  1000 apply_at_region  2377       13 0.0519562 0.0008018186
14 GeoHashBinary      51  1000 apply_at_region  2377       14 0.0942067 0.0012592498
#+end_example


#+begin_src R :results output :exports both :session 
dfplot %>% 
    filter( algo == "GeoHashBinary") %>% 
    filter( bench == "scan_at_region") %>%
    filter( queryId %in% c(51) )
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 14 x 8
# Groups:   algo, queryId, T, bench, Count [7]
            algo queryId     T          bench Count refLevel    avg_ms         stdv
           <chr>   <int> <int>          <chr> <dbl>    <int>     <dbl>        <dbl>
 1 GeoHashBinary      51  1000 scan_at_region  2377        1 0.2345622 0.0024317560
 2 GeoHashBinary      51  1000 scan_at_region  2377        2 0.2340035 0.0022638553
 3 GeoHashBinary      51  1000 scan_at_region  2377        3 0.2359501 0.0024188380
 4 GeoHashBinary      51  1000 scan_at_region  2377        4 0.2373651 0.0033100812
 5 GeoHashBinary      51  1000 scan_at_region  2377        5 0.1762077 0.0021624186
 6 GeoHashBinary      51  1000 scan_at_region  2377        6 0.1599304 0.0017155236
 7 GeoHashBinary      51  1000 scan_at_region  2377        7 0.0723863 0.0015773309
 8 GeoHashBinary      51  1000 scan_at_region   800        8 0.0584567 0.0009696057
 9 GeoHashBinary      51  1000 scan_at_region   634        9 0.0465737 0.0004474881
10 GeoHashBinary      51  1000 scan_at_region   387       10 0.0467218 0.0005622435
11 GeoHashBinary      51  1000 scan_at_region   386       11 0.0218820 0.0008287360
12 GeoHashBinary      51  1000 scan_at_region   197       12 0.0262591 0.0009895785
13 GeoHashBinary      51  1000 scan_at_region   197       13 0.0331863 0.0010786184
14 GeoHashBinary      51  1000 scan_at_region    47       14 0.0527840 0.0009347405
#+end_example

#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session 

algo_labels <- c(BTree = "BTree", GeoHashBinary = "PMQ")

dfplot %>% 
    filter(queryId %in% c(50:54) ) %>% 
    filter( bench == "scan_at_region", algo == "GeoHashBinary") %>% 
    mutate(queryW = 90 / 2**(queryId %/% 10)) %>%
    ggplot( aes(x = refLevel,avg_ms, color=as.factor(queryId), group=(queryId))) + 
    theme_bw() + 
    geom_line() +
    geom_vline(xintercept=8, linetype="dashed") + 
    scale_x_continuous(breaks=seq(0, 14, by=1)) +
   scale_y_continuous(trans = scales::log2_trans(),
                       labels = scales::trans_format("log2", scales::math_format(2^.x))) +
    labs(colour="Query Width (Degrees)", 
         y = "Query execution time in (ms)" ,
         x = "Geohash refinement level") +
    facet_wrap(~queryW,
               scale="fixed",
               labeller = labeller(algo=algo_labels))+
    theme(legend.position = "bottom",
    #      axis.text = element_text(size = 12),
    #      axis.title = element_text(size = 14),
    #      legend.text = element_text( size = 12),
    #      legend.title = element_text( size = 12),
          strip.text = element_text( size = 12)) +
    guides(color=guide_legend(ncol=10,byrow=TRUE))

#+end_src

#+RESULTS:
[[file:/tmp/babel-1215udE/figure1215q2X.png]]


* Comparing the performance of other ref levels [2017-11-13 seg] 

Old performance of geohash binary
#+begin_src R :results output :exports both :session 
dfSpeedUp2 %>% filter(queryId == 54, T== 1000)
#+end_src

#+RESULTS:
: # A tibble: 2 x 16
: # Groups:   algo_speedup, queryWidth [2]
:   queryId     T queryWidth  FPOS ALLPOS BTree_avg_ms  BTree_stdv GeoHashBinary_avg_ms GeoHashBinary_stdv RTree_avg_ms  RTree_stdv Count TotScans   algo_speedup val_speedup   ord
:     <int> <int>      <dbl> <dbl>  <dbl>        <dbl>       <dbl>                <dbl>              <dbl>        <dbl>       <dbl> <int>    <dbl>          <chr>       <dbl> <int>
: 1      54  1000     2.8125 39312  43343    0.4925632 0.001224988            0.3213411       0.0007721667    0.0989732 0.001842404  6164    45476 speed_up_RTree   0.3080004     4
: 2      54  1000     2.8125 39312  43343    0.4925632 0.001224988            0.3213411       0.0007721667    0.0989732 0.001842404  6164    45476 speed_up_BTree   1.5328360     1


New performance , good news, with the right refinements we are able to improve the performance over RTREE
(ignore the count column, it doesnt apply here)
- with 10, to 12 refinements we are better that rtree
#+begin_src R :results output :exports both :session 
dfplot %>% 
    filter( algo == "GeoHashBinary") %>% 
    filter( bench == "scan_at_region") %>%
    filter( queryId %in% c(54) )
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 14 x 8
# Groups:   algo, queryId, T, bench, Count [8]
            algo queryId     T          bench Count refLevel    avg_ms         stdv
           <chr>   <int> <int>          <chr> <dbl>    <int>     <dbl>        <dbl>
 1 GeoHashBinary      54  1000 scan_at_region  6164        1 5.4961580 0.0051371735
 2 GeoHashBinary      54  1000 scan_at_region  6164        2 5.5202330 0.0102075289
 3 GeoHashBinary      54  1000 scan_at_region  6164        3 5.5092310 0.0065509871
 4 GeoHashBinary      54  1000 scan_at_region  6164        4 5.5266320 0.0264299581
 5 GeoHashBinary      54  1000 scan_at_region  6164        5 2.7627000 0.0045491489
 6 GeoHashBinary      54  1000 scan_at_region  6164        6 0.7000676 0.0013051868
 7 GeoHashBinary      54  1000 scan_at_region  6164        7 0.6421820 0.0013168537
 8 GeoHashBinary      54  1000 scan_at_region  4031        8 0.3568843 0.0004429108
 9 GeoHashBinary      54  1000 scan_at_region  1809        9 0.2403781 0.0011835930
10 GeoHashBinary      54  1000 scan_at_region  1068       10 0.1121849 0.0011592101
11 GeoHashBinary      54  1000 scan_at_region   926       11 0.0701151 0.0013435695
12 GeoHashBinary      54  1000 scan_at_region   837       12 0.0573999 0.0018288265
13 GeoHashBinary      54  1000 scan_at_region   472       13 0.0613457 0.0020029027
14 GeoHashBinary      54  1000 scan_at_region   360       14 0.0801638 0.0015638560
#+end_example


#+begin_src R :results output :exports both :session 

as.numeric(levels(factor(dfSpeedUp2$queryWidth)))
360 / as.numeric(levels(factor(dfSpeedUp2$queryWidth)))

(360*180) / as.numeric(levels(factor(dfSpeedUp2$queryWidth)))**2
#+end_src

#+RESULTS:
: [1]  0.703125  1.406250  2.812500  5.625000 11.250000 22.500000 45.000000
: [8] 90.000000
: [1] 512 256 128  64  32  16   8   4
: [1] 131072  32768   8192   2048    512    128     32      8

I think the ideal would be to select the optimal refinement level for each one of the queries. 



* New experiment with reflevel = 10 to compute the speedups. 

Combine this results of BTREE and PMQ with the RTREE results from 
[[file:~/Projects/pmq-DATA/data/cicero/exp20171016155353/exp.org::#exp20171115231329]]


** 1 - load results from [[file:~/Projects/pmq-DATA/data/cicero/exp20171016155353/exp.org::#df20171115232624]]
#+begin_src R :results output :exports both :session 
levels(factor(dfCount$algo))
dfCount %>% filter( algo == "RTree" ) 
#+end_src

#+RESULTS:
#+begin_example
[1] "BTree"               "GeoHashBinary"       "ImplicitDenseVector"
[4] "RTree"               "RTreeBulk"
# A tibble: 3,200 x 10
   queryId  algo   Count     T EltSize          bench Refine queryWidth   avg_ms       stdv
     <int> <chr>   <int> <int>   <dbl>          <chr>  <int>      <dbl>    <dbl>      <dbl>
 1       0 RTree  924827  1000      16 scan_at_region     NA         90 20.79137 0.05379544
 2       0 RTree  924827  1000      32 scan_at_region     NA         90 20.98994 0.06185160
 3       0 RTree  924827  1000      64 scan_at_region     NA         90 21.08689 0.03205188
 4       0 RTree  924827  1000     128 scan_at_region     NA         90 21.00232 0.02487559
 5       0 RTree  924827  1000     256 scan_at_region     NA         90 20.95482 0.03284485
 6       0 RTree 1855890  2000      16 scan_at_region     NA         90 42.39427 0.05708750
 7       0 RTree 1855890  2000      32 scan_at_region     NA         90 42.33648 0.03501726
 8       0 RTree 1855890  2000      64 scan_at_region     NA         90 42.88428 0.05767346
 9       0 RTree 1855890  2000     128 scan_at_region     NA         90 43.15382 0.05103667
10       0 RTree 1855890  2000     256 scan_at_region     NA         90 43.20637 0.04042428
# ... with 3,190 more rows
#+end_example

** 2 - new results - generate csv files
:PROPERTIES: 
:HEADER-ARGS:sh: :tangle parse.sh :shebang #!/bin/bash
:END:      

List logFiles
#+begin_src sh :results table :exports both
ls -htl *tgz
#+end_src

#+RESULTS:
| -rw-r--r-- | 1 | julio | julio | 198K | nov | 15 | 22:34 | log_1510783350.tgz |
| -rw-r--r-- | 1 | julio | julio | 669K | nov | 11 | 16:51 | log_1510425176.tgz |

#+NAME: logFile2
#+begin_src sh :results table :exports both 
tar xvzf log_1510783350.tgz
#+end_src

#+RESULTS: logFile2
| bench_queries_region_twitter_1000_1000_10_1510783350.log   |
| bench_queries_region_twitter_128000_1000_10_1510783350.log |
| bench_queries_region_twitter_16000_1000_10_1510783350.log  |
| bench_queries_region_twitter_2000_1000_10_1510783350.log   |
| bench_queries_region_twitter_32000_1000_10_1510783350.log  |
| bench_queries_region_twitter_4000_1000_10_1510783350.log   |
| bench_queries_region_twitter_64000_1000_10_1510783350.log  |
| bench_queries_region_twitter_8000_1000_10_1510783350.log   |

Create CSV using logFile 
#+NAME: csvFile2
#+begin_src sh :results output :exports both :var logFileList=logFile2

#echo $logFile | sed 's/bench_queries_region_random_10000_100_\([[:digit:]]\)//g'
for logFile in $logFileList
do
    echo $(basename -s .log $logFile ).csv
    grep "GeoHashBinary\|BTree" $logFile | grep "query" | sed "s/QueryBench//g" >  $(basename -s .log $logFile ).csv
done

#+end_src

#+RESULTS: csvFile2
: bench_queries_region_twitter_1000_1000_10_1510783350.csv
: bench_queries_region_twitter_128000_1000_10_1510783350.csv
: bench_queries_region_twitter_16000_1000_10_1510783350.csv
: bench_queries_region_twitter_2000_1000_10_1510783350.csv
: bench_queries_region_twitter_32000_1000_10_1510783350.csv
: bench_queries_region_twitter_4000_1000_10_1510783350.csv
: bench_queries_region_twitter_64000_1000_10_1510783350.csv
: bench_queries_region_twitter_8000_1000_10_1510783350.csv


** Results
:PROPERTIES: 
:HEADER-ARGS:R: :session *R2* :tangle plotResults.R :shebang #!/usr/bin/env Rscript
:CUSTOM_ID: exp20171116083818
:END:      


*** Load the CSV into R
#+begin_src R :results output :exports both :var f=csvFile2 path=(print default-directory)
setwd(path)
library(tidyverse)
options(tibble.width = Inf)

                                        # Reads a csv file and add a column identifying the csv by parsin its name
readAdd <- function(input){

    
return ( read_delim(input,delim=";",trim_ws = TRUE, col_names = paste("V",c(1:15),sep=""), col_types="ccicicdcicdcdcd" ) %>%
         mutate (
             ref = as.factor(
             gsub("bench_queries_region_twitter_[[:digit:]]+_[[:digit:]]+_([[:digit:]]+)_[[:digit:]]+.csv","\\1",input))))
} 


files = strsplit(f,"\n")[[1]]

df <- files %>%
    map(readAdd) %>%   # use my custom read function
    reduce(rbind)   # used rbind to combine into one dataframe

names(df) <- c("algo" , "V2" , "queryId", "V4", 
                "T", "bench" , "ms" , "V8","Refine","V10","TPOS","V12","FPOS","V14","ALLPOS","RefLevel")

#+end_src

#+RESULTS:
#+begin_example
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_1000_1000_10_1510783350.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                         file expected   <int> <chr>      <chr>      <chr>                                                        <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_128000_1000_10_1510783350.csv'
... ................. ... ................................................................................................ ........ ................................................................................................ ..... [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_16000_1000_10_1510783350.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_2000_1000_10_1510783350.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_32000_1000_10_1510783350.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_4000_1000_10_1510783350.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                        file expected   <int> <chr>      <chr>      <chr>                                                       <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_64000_1000_10_1510783350.csv'
... ................. ... ............................................................................................... ........ ............................................................................................... ...... ....... [... truncated]
Warning: 3200 parsing failures.
row # A tibble: 5 x 5 col     row   col   expected     actual                                                       file expected   <int> <chr>      <chr>      <chr>                                                      <chr> actual 1     1  <NA> 15 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_1510783350.csv' file 2     2  <NA> 15 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_1510783350.csv' row 3     3  <NA> 15 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_1510783350.csv' col 4     4  <NA> 15 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_1510783350.csv' expected 5     5  <NA> 15 columns 12 columns 'bench_queries_region_twitter_8000_1000_10_1510783350.csv'
... ................. ... .............................................................................................. ........ .............................................................................................. ...... ................ [... truncated]
Warning messages:
1: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
2: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
3: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
4: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
5: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
6: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
7: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
8: In rbind(names(probs), probs_f) :
  number of columns of result is not a multiple of vector length (arg 1)
#+end_example

Remove useless columns
#+begin_src R :results output :exports both  
levels(as.factor(df$algo))
df %>% print( n = 20)
#names(df) <- c("algo" , "V2" , "queryId",  "V4" , "T"  ,"bench" , "ms" , "V8", "refinements" , "V10", "Count", "refLevel")

df <- select(df, -V2, -V4, -V8, -V10,-V12,-V14)
df %>% print( n = 20)

str(df)
#+end_src

#+RESULTS:
#+begin_example
[1] "BTree"         "GeoHashBinary"
# A tibble: 25,600 x 16
            algo    V2 queryId    V4     T           bench       ms                          V8 Refine                           V10   TPOS                            V12  FPOS                           V14 ALLPOS RefLevel
           <chr> <chr>   <int> <chr> <int>           <chr>    <dbl>                       <chr>  <int>                         <chr>  <dbl>                          <chr> <dbl>                         <chr>  <dbl>   <fctr>
 1 GeoHashBinary query       0     T  1000 apply_at_region 0.357045 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 2 GeoHashBinary query       0     T  1000 apply_at_region 0.355450 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 3 GeoHashBinary query       0     T  1000 apply_at_region 0.361164 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 4 GeoHashBinary query       0     T  1000 apply_at_region 0.353275 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 5 GeoHashBinary query       0     T  1000 apply_at_region 0.353269 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 6 GeoHashBinary query       0     T  1000 apply_at_region 0.353003 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 7 GeoHashBinary query       0     T  1000 apply_at_region 0.352893 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 8 GeoHashBinary query       0     T  1000 apply_at_region 0.352796 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
 9 GeoHashBinary query       0     T  1000 apply_at_region 0.352557 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
10 GeoHashBinary query       0     T  1000 apply_at_region 0.352253 apply_at_region_refinements     69                         count 924827                           <NA>    NA                          <NA>     NA       10
11 GeoHashBinary query       0     T  1000  scan_at_region 4.339830  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
12 GeoHashBinary query       0     T  1000  scan_at_region 4.333450  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
13 GeoHashBinary query       0     T  1000  scan_at_region 4.344870  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
14 GeoHashBinary query       0     T  1000  scan_at_region 4.337440  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
15 GeoHashBinary query       0     T  1000  scan_at_region 4.338080  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
16 GeoHashBinary query       0     T  1000  scan_at_region 4.323750  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
17 GeoHashBinary query       0     T  1000  scan_at_region 4.322290  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
18 GeoHashBinary query       0     T  1000  scan_at_region 4.323760  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
19 GeoHashBinary query       0     T  1000  scan_at_region 4.325550  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
20 GeoHashBinary query       0     T  1000  scan_at_region 4.326320  scan_at_region_refinements     69 scan_at_region_true_positives    445 scan_at_region_false_positives   798 scan_at_region_true_and_false   1243       10
# ... with 2.558e+04 more rows
# A tibble: 25,600 x 10
            algo queryId     T           bench       ms Refine   TPOS  FPOS ALLPOS RefLevel
           <chr>   <int> <int>           <chr>    <dbl>  <int>  <dbl> <dbl>  <dbl>   <fctr>
 1 GeoHashBinary       0  1000 apply_at_region 0.357045     69 924827    NA     NA       10
 2 GeoHashBinary       0  1000 apply_at_region 0.355450     69 924827    NA     NA       10
 3 GeoHashBinary       0  1000 apply_at_region 0.361164     69 924827    NA     NA       10
 4 GeoHashBinary       0  1000 apply_at_region 0.353275     69 924827    NA     NA       10
 5 GeoHashBinary       0  1000 apply_at_region 0.353269     69 924827    NA     NA       10
 6 GeoHashBinary       0  1000 apply_at_region 0.353003     69 924827    NA     NA       10
 7 GeoHashBinary       0  1000 apply_at_region 0.352893     69 924827    NA     NA       10
 8 GeoHashBinary       0  1000 apply_at_region 0.352796     69 924827    NA     NA       10
 9 GeoHashBinary       0  1000 apply_at_region 0.352557     69 924827    NA     NA       10
10 GeoHashBinary       0  1000 apply_at_region 0.352253     69 924827    NA     NA       10
11 GeoHashBinary       0  1000  scan_at_region 4.339830     69    445   798   1243       10
12 GeoHashBinary       0  1000  scan_at_region 4.333450     69    445   798   1243       10
13 GeoHashBinary       0  1000  scan_at_region 4.344870     69    445   798   1243       10
14 GeoHashBinary       0  1000  scan_at_region 4.337440     69    445   798   1243       10
15 GeoHashBinary       0  1000  scan_at_region 4.338080     69    445   798   1243       10
16 GeoHashBinary       0  1000  scan_at_region 4.323750     69    445   798   1243       10
17 GeoHashBinary       0  1000  scan_at_region 4.322290     69    445   798   1243       10
18 GeoHashBinary       0  1000  scan_at_region 4.323760     69    445   798   1243       10
19 GeoHashBinary       0  1000  scan_at_region 4.325550     69    445   798   1243       10
20 GeoHashBinary       0  1000  scan_at_region 4.326320     69    445   798   1243       10
# ... with 2.558e+04 more rows
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	25600 obs. of  10 variables:
 $ algo    : chr  "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" "GeoHashBinary" ...
 $ queryId : int  0 0 0 0 0 0 0 0 0 0 ...
 $ T       : int  1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ...
 $ bench   : chr  "apply_at_region" "apply_at_region" "apply_at_region" "apply_at_region" ...
 $ ms      : num  0.357 0.355 0.361 0.353 0.353 ...
 $ Refine  : int  69 69 69 69 69 69 69 69 69 69 ...
 $ TPOS    : num  924827 924827 924827 924827 924827 ...
 $ FPOS    : num  NA NA NA NA NA NA NA NA NA NA ...
 $ ALLPOS  : num  NA NA NA NA NA NA NA NA NA NA ...
 $ RefLevel: Factor w/ 1 level "10": 1 1 1 1 1 1 1 1 1 1 ...
#+end_example

Summarize the averages
#+begin_src R :results output :exports both
dfplot <- 
    df %>% 
    group_by_at(vars(-ms, -Refine)) %>%   #group_by all expect ms
    summarize(avg_ms = mean(ms), stdv = sd(ms)) %>%
    mutate(RefLevel = as.integer(as.character(RefLevel)))

dfplot %>% #arrange(T,bench,queryId) 
filter(algo!="BTree", bench == "scan_at_region") %>% 
filter(queryId == 54)

#+end_src

#+RESULTS:
#+begin_example
# A tibble: 8 x 10
# Groups:   algo, queryId, T, bench, TPOS, FPOS, ALLPOS [8]
           algo queryId      T          bench   TPOS    FPOS  ALLPOS RefLevel     avg_ms         stdv
          <chr>   <int>  <int>          <chr>  <dbl>   <dbl>   <dbl>    <int>      <dbl>        <dbl>
1 GeoHashBinary      54   1000 scan_at_region   1068    9812   10880       10  0.1120869 0.0008021441
2 GeoHashBinary      54   2000 scan_at_region   2225   20457   22682       10  0.2274997 0.0025799108
3 GeoHashBinary      54   4000 scan_at_region   4491   42845   47336       10  0.4925631 0.0061154698
4 GeoHashBinary      54   8000 scan_at_region   8801   84878   93679       10  1.0206861 0.0319284523
5 GeoHashBinary      54  16000 scan_at_region  16594  181116  197710       10  2.1953460 0.0235862131
6 GeoHashBinary      54  32000 scan_at_region  35532  358281  393813       10  4.3387190 0.0083988815
7 GeoHashBinary      54  64000 scan_at_region  69805  739051  808856       10  8.8362260 0.0110299826
8 GeoHashBinary      54 128000 scan_at_region 150009 1435480 1585480       10 17.6364500 0.0241291087
#+end_example

#+begin_src R :results output :exports both
sum(df$ms) / 1000 / 60
#+end_src

#+RESULTS:
: [1] 49.78826

 -> NOTE rerunning exp201710165553 with refinement level == 10 
 [[file:~/Projects/pmq-DATA/data/cicero/exp20171016155353/exp.org::*Experiment%20Script]]

**** Save this file
#+begin_src R :results output :exports both 
df %>% filter(bench=="scan_at_region") %>%
write_rds("falsePosAnalysisRef10.rds","gz")
#+end_src

#+RESULTS:

#+begin_src R :results output :exports both 
dfFalsePos <- read_rds("falsePosAnalysisRef10.rds")
dfFalsePos
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 12,800 x 10
            algo queryId     T          bench      ms Refine  TPOS  FPOS ALLPOS RefLevel
           <chr>   <int> <int>          <chr>   <dbl>  <int> <dbl> <dbl>  <dbl>   <fctr>
 1 GeoHashBinary       0  1000 scan_at_region 4.33983     69   445   798   1243       10
 2 GeoHashBinary       0  1000 scan_at_region 4.33345     69   445   798   1243       10
 3 GeoHashBinary       0  1000 scan_at_region 4.34487     69   445   798   1243       10
 4 GeoHashBinary       0  1000 scan_at_region 4.33744     69   445   798   1243       10
 5 GeoHashBinary       0  1000 scan_at_region 4.33808     69   445   798   1243       10
 6 GeoHashBinary       0  1000 scan_at_region 4.32375     69   445   798   1243       10
 7 GeoHashBinary       0  1000 scan_at_region 4.32229     69   445   798   1243       10
 8 GeoHashBinary       0  1000 scan_at_region 4.32376     69   445   798   1243       10
 9 GeoHashBinary       0  1000 scan_at_region 4.32555     69   445   798   1243       10
10 GeoHashBinary       0  1000 scan_at_region 4.32632     69   445   798   1243       10
# ... with 12,790 more rows
#+end_example

