# -*- org-export-babel-evaluate: nil; -*-
#+TITLE: Experiment Diary
#+LANGUAGE: en 
#+STARTUP: indent
#+STARTUP: logdrawer hideblocks
#+SEQ_TODO: TODO INPROGRESS(i) | DONE DEFERRED(@) CANCELED(@)
#+TAGS: @JULIO(J)
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n)
#+CATEGORY: exp
#+OPTIONS: ^:{}
#+PROPERTY: header-args :cache no :eval no-export

* TODO Description 
- Describe what are you doing

- A template to execute experiment _locally_

* Experiment Script
** Initial Setup 

#+begin_src sh :results value :exports both
expId=$(basename $(pwd))
echo $expId
#+end_src

#+NAME: expId
#+RESULTS:
: exp1483472768

Set up git branch
#+begin_src sh :results output :exports both
git checkout master
#+end_src

Create EXP branch
#+begin_src sh :results output :exports both :var expId=expId
git checkout -b $expId
#+end_src

#+RESULTS:
: M	LabBook.org
: M	include/GeoHash.cpp

Commit branch
#+begin_src sh :results output :exports both :var expId=expId
git status .
git add exp.org
git commit -m "Initial commit for $expId"
#+end_src

#+RESULTS:
#+begin_example
On branch exp1483472768
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	modified:   exp.org

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	exec_1483475450.log
	exec_1483475645.log
	exec_1483475656.log
	exp.org~
	run.sh
	run.sh~

no changes added to commit (use "git add" and/or "git commit -a")
[exp1483472768 ddeef18] Initial commit for exp1483472768
 1 file changed, 32 insertions(+), 2 deletions(-)
#+end_example

#+begin_src sh :results output :exports both :var expId=expId
git la -3 
#+end_src

#+RESULTS:
: * 13cc243 (HEAD -> exp1483472768) Initial commit for
: * 89c5385 (bitbucket/master, master) Code cleanup (formatting style). Add .gitignore file.
: | *   a7f2592 (refs/stash) WIP on master: 208d730 optimize GeoHash binary search (2)
: | |\  
: |/ /  

** Export run script 

Use C-u C-c C-v t to tangle this script 
#+begin_src sh :results output :exports both :tangle run.sh :shebang #!/bin/bash :eval never :var expId=expId
set -e
# Any subsequent(*) commands which fail will cause the shell script to exit immediately
echo $(hostname) 

##########################################################
### SETUP THIS VARIABLES

BUILDIR=~/Projects/twitterVis/build-release
PMABUILD_DIR=~/Projects/hppsimulations/build-release
DATADIR=$(pwd)
TMPDIR=/tmp/$expId

#########################################################

mkdir -p $TMPDIR
#mkdir -p $DATADIR

# generate output name
OUT=exec_$(date +%s).log

cd $BUILDIR 

cmake -DPMA_BUILD_DIR=$PMABUILD_DIR -DCMAKE_BUILD_TYPE="Release" ..
make

# EXECUTE BENCHMARK
./benchmarks/bench_insert_and_scan -n $((10**6)) -r 123 -x 3 -b 100 > $TMPDIR/$OUT

cd $DATADIR

cp $TMPDIR/$OUT .

git checkout $expId

#get machine configuration
echo "" > info.org
~/Projects/twitterVis/scripts/g5k_get_info.sh info.org 

git add info.org $OUT 
git commit -m "Finish execution of $OUT"
#git push origin $expId
#+end_src 


** Execute on local Machine

#+begin_src sh :results output :exports both 
tmux new -d -s benchmarks './run.sh &> run_$(date +%s)'
tmux ls
#+end_src


* TODO Analisys
** Generate csv files
#+begin_src sh :results output :exports both
pwd
ls *log
#+end_src

#+RESULTS:
: /home/julio/Projects/twitterVis/data/idphix/exp1476928803
: idphix_tweet_b100_1476931833.log

#+begin_src sh :results none :exports both 
grep "PMABatch\|TimDense\|StdDense\|PostgreSQL\|SQLite ;" idphix_tweet*.log | sed "s/InsertionBench//g" > tweet.csv
#+end_src

#+begin_src sh :results output :exports both
mkdir img
#+end_src

#+RESULTS:

** Twitter distribution
:PROPERTIES: 
:HEADER-ARGS:R: :session twitter
:END:      

#+begin_src R :results output :exports both :var f="tweet.csv"
library(plyr)
df = read.csv(f,header=FALSE,strip.white=TRUE,sep=";")
names(df) = c("algo","bench","k","time")
head(df)
#+end_src

#+RESULTS:
:       algo          bench k     time NA NA
: 1 PMABatch         Insert 0 0.122308 ms NA
: 2 PMABatch   ModifiedKeys 0 0.004868 ms NA
: 3 PMABatch QuadtreeUpdate 0 0.276548 ms NA
: 4 PMABatch         Insert 1 0.031147 ms NA
: 5 PMABatch   ModifiedKeys 1 0.005784 ms NA
: 6 PMABatch QuadtreeUpdate 1 0.325558 ms NA


*** Overview of results                                                :plot:

Plot an overview of every benchmark , doing average of times. 

#+begin_src R :results output graphics :file "./img/twitter_overview.png" :exports both :width 800 :height 600
library(ggplot2)
summary_avg = ddply(df ,c("algo","k","bench"),summarise,"time"=mean(time))
ggplot(summary_avg, aes(x=k,y=time, color=factor(algo))) + geom_line() + 
facet_wrap(~bench, scales="free",labeller=label_both)
#+end_src

#+RESULTS:
[[file:./img/twitter_overview.png]]

*** Overall Insertion performance

For DB benchmarks:
- SQLite :: time = Analyze + Insert

For Vectors:
- time = Insert + ModifiedKeys + QuadtreeUpdate 

#+begin_src R :results none :exports both
insTime = ddply( summary_avg  ,c("algo","k"),summarise,"time"=sum(time)) 
#+end_src

#+begin_src R :results output graphics :file "./img/twitter_overallInsertion.png" :exports both :width 600 :height 400
ggplot(insTime, aes(x=k,y=time, color=factor(algo))) + geom_line() 
#+end_src

#+RESULTS:
[[file:./img/twitter_overallInsertion.png]]

Notes about the graph above:
- SQLite seem to have a near constant insertion time. For large
  datasets it has an insertion time much better than the Vector based
  solutions. (note however that its query time increases very fast,
  not shown here, see previous experiment on 10^4 elements)

**** SpeeUp on last iteration
 
#+begin_src R :results output :exports both
insTime[insTime$k == 9999,]
#+end_src

#+RESULTS:
:           algo    k       time
: 10000 PMABatch 9999   0.768605
: 20000   SQLite 9999  37.544700
: 30000 StdDense 9999 368.825970
: 40000 TimDense 9999 311.011450

**** Amortized time

Compute the avgTime
#+begin_src R :results output :exports both
avgTime = cbind(insTime, avgTime=c(lapply(split(insTime, insTime$algo), function(x) cumsum(x$time)/(x$k+1)), recursive=T))
#+end_src

#+RESULTS:

***** Melting the data (time / avgTime)
#+begin_src R :results output graphics :file "./img/twitter_avgInsertion.png" :exports both :width 800 :height 600 
library(reshape2)
melted_times = melt(avgTime, id.vars = c("algo","k"),measure.vars = c("time", "avgTime"))

ggplot(melted_times, aes(x=k,y=value)) +
geom_line(aes(colour=algo,linetype=variable)) +
scale_linetype_manual(values=c("solid","dashed"),labels=c("Real time","Amortized time") ) +
labs(colour="Algorithms",linetype="")
#+end_src

#+RESULTS:
[[file:./img/twitter_avgInsertion.png]]

***** Plot only amortized (average) time.
#+begin_src R :results output graphics :file "./img/twitter_avgInsertion2.png" :exports both :width 600 :height 400 
ggplot(avgTime, aes(x=k,y=avgTime)) +
geom_line(aes(colour=algo)) 
#+end_src

#+RESULTS:
[[file:./img/twitter_avgInsertion2.png]]

#+begin_src R :results output :session twitter :exports both
summary(avgTime[avgTime$algo=="PMABatch",])
#+end_src

#+RESULTS:
:        algo             k             time             avgTime      
:  PMABatch:10000   Min.   :   0   Min.   :  0.3155   Min.   :0.3817  
:  SQLite  :    0   1st Qu.:2500   1st Qu.:  0.5819   1st Qu.:0.8792  
:  StdDense:    0   Median :5000   Median :  0.6926   Median :0.9106  
:  TimDense:    0   Mean   :5000   Mean   :  0.9364   Mean   :0.8951  
:                   3rd Qu.:7499   3rd Qu.:  0.9526   3rd Qu.:0.9565  
:                   Max.   :9999   Max.   :291.9910   Max.   :1.0517

Final mean amortized time of each algorithm: 
#+begin_src R :results output :session twitter :exports both
subset(avgTime, k==9999)
#+end_src

#+RESULTS:
:                   algo    k       time     avgTime
: PMABatch10000 PMABatch 9999   0.768605   0.9363866
: SQLite10000     SQLite 9999  37.544700  22.3111618
: StdDense10000 StdDense 9999 368.825970 191.8165845
: TimDense10000 TimDense 9999 311.011450 165.2370824

***** Comparison Time X avgTime
#+begin_src R :results output graphics :file "./img/pma_batch_times.png" :exports both :width 600 :height 400 
ggplot(subset(melted_times,algo=="PMABatch"), aes(x=k,y=value)) +
geom_line() + 
facet_wrap(~variable, scales="free",labeller=label_both)
#+end_src

#+RESULTS:
[[file:./img/pma_batch_times.png]]

